{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# go to upper diretory\n",
    "sys.path.append(os.path.abspath('./../../../'))\n",
    "import csv\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import noisereduce as nr\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from Audio_Sentiment_Analysis.utils.Configuration import Configuration\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUDIO_DIR = f\"{os.path.abspath('./../../../')}/IEMOCAP_Dataset\"\n",
    "EXTRACTED_FEATURES_FILE = 'extracted_features_iemocap.csv'\n",
    "PREPROCESSED_EXTRACTED_FEATURES_FILE = 'preprocessed_extracted_features_iemocap.csv'\n",
    "PREPROCESSED_ALL_EXTRACTED_FEATURES_FILE = 'all_preprocessed_extracted_features_iemocap.csv'\n",
    "ALL_EXTRACTED_FEATURES_FILE = 'all_extracted_features_iemocap.csv'\n",
    "RAW_AUDIO_FILES = 'raw_audio_files.csv'\n",
    "AUDIO_SPECTROGRAMS_DIR = './../data/spectrograms/'\n",
    "AUDIO_MFCCS_DIR = './../data/mfccs/'\n",
    "AUDIO_MEL_SPECTROGRAMS_DIR = './../data/mel_spectrograms/'\n",
    "AUDIO_SPECTROGRAM_IMAGES_DIR = './../data/spectrograms_images/'\n",
    "AUDIO_MEL_SPECTROGRAM_IMAGES_DIR = './../data/mel_spectrograms_images/'\n",
    "AUDIO_MFCCS_IMAGES_DIR = './../data/mfccs_images/'\n",
    "AUDIO_ALL_DATA_DIR = './../data/all_data/'\n",
    "\n",
    "CONFIG_FILE = f\"{os.path.abspath('./../../../')}/Audio_Sentiment_Analysis/iemocap/config.json\"\n",
    "config = Configuration.load_json(CONFIG_FILE)\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the 43 features selected from the eNTERFACE05 dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance',\n",
    "    'spikes_mel_spect', '25thpercentile_mel_spect', 'median_mel_spect', '75thpercentile_mel_spect', 'mean_mel_spect', 'min_mel_spect', 'max_mel_spect', 'var_mel_spect', 'std_mel_spect', 'sum_mel_spect',\n",
    "    'spikes_chroma_stft', '25thpercentile_chroma_stft', 'median_chroma_stft', '75thpercentile_chroma_stft', 'mean_chroma_stft', 'var_chroma_stft', 'std_chroma_stft', 'sum_chroma_stft',\n",
    "    'spikes_rms', '25thpercentile_rms', 'median_rms', '75thpercentile_rms', 'mean_rms', 'max_rms', 'var_rms', 'std_rms', 'sum_rms',\n",
    "    'spikes_spec_cent', '25thpercentile_spec_cent', 'median_spec_cent', '75thpercentile_spec_cent', 'mean_spec_cent', 'min_spec_cent', 'max_spec_cent', 'var_spec_cent', 'std_spec_cent', 'sum_spec_cent',\n",
    "    'spikes_spec_bw', '25thpercentile_spec_bw', 'median_spec_bw', '75thpercentile_spec_bw', 'mean_spec_bw', 'min_spec_bw', 'max_spec_bw', 'var_spec_bw', 'std_spec_bw', 'sum_spec_bw',\n",
    "    'spikes_spec_rolloff', '25thpercentile_spec_rolloff', 'median_spec_rolloff', '75thpercentile_spec_rolloff', 'mean_spec_rolloff', 'min_spec_rolloff', 'max_spec_rolloff', 'var_spec_rolloff', 'std_spec_rolloff', 'sum_spec_rolloff',\n",
    "    'spikes_spec_cont', '25thpercentile_spec_cont', 'median_spec_cont', '75thpercentile_spec_cont', 'mean_spec_cont', 'min_spec_cont', 'max_spec_cont', 'var_spec_cont', 'std_spec_cont', 'sum_spec_cont',\n",
    "    'spikes_tonnetz', '25thpercentile_tonnetz', 'median_tonnetz', '75thpercentile_tonnetz', 'mean_tonnetz', 'min_tonnetz', 'max_tonnetz', 'var_tonnetz', 'std_tonnetz', 'sum_tonnetz',\n",
    "    'spikes_zcr', '25thpercentile_zcr', 'median_zcr', '75thpercentile_zcr', 'mean_zcr', 'min_zcr', 'max_zcr', 'var_zcr', 'std_zcr', 'sum_zcr'\n",
    "]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    all_headers.extend([f'kurtosis_mfcc{i}', f'skew_mfcc{i}', f'spikes_mfcc{i}', f'25thpercentile_mfcc{i}', f'median_mfcc{i}',\n",
    "                       f'75thpercentile_mfcc{i}', f'mean_mfcc{i}', f'min_mfcc{i}', f'max_mfcc{i}', f'var_mfcc{i}', f'std_mfcc{i}', f'sum_mfcc{i}'])\n",
    "\n",
    "headers = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance',\n",
    "    'max_tonnetz', 'var_spec_bw', 'thpercentile75_spec_bw', 'mean_spec_cont', 'thpercentile75_spec_rolloff',\n",
    "    'var_mel_spect', 'spikes_zcr', 'std_mfcc1', 'kurtosis_mfcc1', 'kurtosis_mfcc2', 'thpercentile75_mfcc2',\n",
    "    'kurtosis_mfcc3', 'var_mfcc3', 'thpercentile75_mfcc4', 'var_mfcc4', 'max_mfcc4', 'kurtosis_mfcc4',\n",
    "    'median_mfcc4', 'median_mfcc5', 'std_mfcc6', 'max_mfcc6', 'max_mfcc7', 'skew_mfcc7', 'skew_mfcc9',\n",
    "    'max_mfcc9', 'kurtosis_mfcc9', 'var_mfcc10', 'skew_mfcc11', 'var_mfcc12', 'thpercentile75_mfcc13',\n",
    "    'skew_mfcc13', 'max_mfcc14', 'thpercentile75_mfcc18', 'median_mfcc20', 'skew_mfcc20'\n",
    "]\n",
    "\n",
    "headers_raw = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance'\n",
    "]\n",
    "\n",
    "emotion_label = {\n",
    "    'ang': \"angry\", 'hap': \"happy\", 'sad': \"sad\", 'neu': \"neutral\",\n",
    "    'fru': \"frustrated\", 'exc': \"excited\", 'fea': \"fearful\",\n",
    "    'sur': \"surprised\", 'dis': \"disgusted\", 'xxx': \"other\", 'oth': \"other\"\n",
    "}\n",
    "\n",
    "emotion_number = {\n",
    "    'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4,\n",
    "    'exc': 5, 'fea': 6, 'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 9\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spikes(data):\n",
    "    if len(data.shape) != 1:\n",
    "        data = np.concatenate(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    threshold = mean + std * 2 / 100\n",
    "    num_spikes = 0\n",
    "    for value in data:\n",
    "        if value >= threshold:\n",
    "            num_spikes += 1\n",
    "\n",
    "    return num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(y, sr):\n",
    "    y = nr.reduce_noise(y=y, sr=sr, n_fft=2048, hop_length=512, prop_decrease=.75, time_constant_s=1)\n",
    "    y, _ = librosa.effects.trim(y, top_db=30)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(audio_file, emotion, valence, activation, dominance, writer, preprocess=False):\n",
    "    file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "    acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "    gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    duration = librosa.get_duration(y, sr)\n",
    "\n",
    "    if preprocess:\n",
    "        y = preprocess_audio(y, sr)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.n_mels)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_cont = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "\n",
    "    features_str = f'{file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {valence} {activation} {dominance}\\\n",
    "        {spikes(mel_spect)} {np.nanpercentile(mel_spect, 0.25)} {np.median(mel_spect)} {np.nanpercentile(mel_spect, 0.75)} {np.mean(mel_spect)} {np.min(mel_spect)} {np.max(mel_spect)} {np.var(mel_spect)} {np.std(mel_spect)} {np.sum(mel_spect)}\\\n",
    "        {spikes(chroma_stft)} {np.nanpercentile(chroma_stft, 0.25)} {np.median(chroma_stft)} {np.nanpercentile(chroma_stft, 0.75)} {np.mean(chroma_stft)} {np.var(chroma_stft)} {np.std(chroma_stft)} {np.sum(chroma_stft)}\\\n",
    "        {spikes(rms)} {np.nanpercentile(rms, 0.25)} {np.median(rms)} {np.nanpercentile(rms, 0.75)} {np.mean(rms)} {np.max(rms)} {np.var(rms)} {np.std(rms)} {np.sum(rms)}\\\n",
    "        {spikes(spec_cent)} {np.nanpercentile(spec_cent, 0.25)} {np.median(spec_cent)} {np.nanpercentile(spec_cent, 0.75)} {np.mean(spec_cent)} {np.min(spec_cent)} {np.max(spec_cent)} {np.var(spec_cent)} {np.std(spec_cent)} {np.sum(spec_cent)}\\\n",
    "        {spikes(spec_bw)} {np.nanpercentile(spec_bw, 0.25)} {np.median(spec_bw)} {np.nanpercentile(spec_bw, 0.75)} {np.mean(spec_bw)} {np.min(spec_bw)} {np.max(spec_bw)} {np.var(spec_bw)} {np.std(spec_bw)} {np.sum(spec_bw)}\\\n",
    "        {spikes(spec_rolloff)} {np.nanpercentile(spec_rolloff, 0.25)} {np.median(spec_rolloff)} {np.nanpercentile(spec_rolloff, 0.75)} {np.mean(spec_rolloff)} {np.min(spec_rolloff)} {np.max(spec_rolloff)} {np.var(spec_rolloff)} {np.std(spec_rolloff)} {np.sum(spec_rolloff)}\\\n",
    "        {spikes(spec_cont)} {np.nanpercentile(spec_cont, 0.25)} {np.median(spec_cont)} {np.nanpercentile(spec_cont, 0.75)} {np.mean(spec_cont)} {np.min(spec_cont)} {np.max(spec_cont)} {np.var(spec_cont)} {np.std(spec_cont)} {np.sum(spec_cont)}\\\n",
    "        {spikes(tonnetz)} {np.nanpercentile(tonnetz, 0.25)} {np.median(tonnetz)} {np.nanpercentile(tonnetz, 0.75)} {np.mean(tonnetz)} {np.min(tonnetz)} {np.max(tonnetz)} {np.var(tonnetz)} {np.std(tonnetz)} {np.sum(tonnetz)}\\\n",
    "        {spikes(zcr)} {np.nanpercentile(zcr, 0.25)} {np.median(zcr)} {np.nanpercentile(zcr, 0.75)} {np.mean(zcr)} {np.min(zcr)} {np.max(zcr)} {np.var(zcr)} {np.std(zcr)} {np.sum(zcr)}'\n",
    "\n",
    "    for e in mfcc:\n",
    "        features_str += f' {kurtosis(e)} {skew(e)} {spikes(e)} {np.nanpercentile(e, 0.25)} {np.median(e)} {np.nanpercentile(e, 0.75)} {np.mean(e)} {np.min(e)} {np.max(e)} {np.var(e)} {np.std(e)} {np.sum(e)}'\n",
    "\n",
    "    writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file, emotion, valence, activation, dominance, writer, preprocess=False): \n",
    "    file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "    acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "    gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "    \n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "    bef_prep_duration = librosa.get_duration(y, sr)\n",
    "    aft_prep_duration = bef_prep_duration\n",
    "\n",
    "    if preprocess:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "        aft_prep_duration = librosa.get_duration(y, sr)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "\n",
    "    max_tonnetz = np.max(librosa.feature.tonnetz(y, sr=sr))\n",
    "    var_spec_bw = np.var(spec_bw)\n",
    "    thpercentile75_spec_bw = np.nanpercentile(spec_bw, 0.75)\n",
    "    mean_spec_cont = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "    thpercentile75_spec_rolloff = np.nanpercentile(librosa.feature.spectral_rolloff(y, sr=sr), 0.75)\n",
    "    var_mel_spect = np.var(librosa.feature.melspectrogram(y, sr=sr, n_mels=127))\n",
    "    spikes_zcr = spikes(librosa.feature.zero_crossing_rate(y))\n",
    "\n",
    "    std_mfcc1 = np.std(mfcc[0])\n",
    "    kurtosis_mfcc1 = kurtosis(mfcc[0])\n",
    "    kurtosis_mfcc2 = kurtosis(mfcc[1])\n",
    "    thpercentile75_mfcc2 = np.nanpercentile(mfcc[1], 0.75)\n",
    "    kurtosis_mfcc3 = kurtosis(mfcc[2])\n",
    "    var_mfcc3 = np.var(mfcc[2])\n",
    "    thpercentile75_mfcc4 = np.nanpercentile(mfcc[3], 0.75)\n",
    "    var_mfcc4 = np.var(mfcc[3])\n",
    "    max_mfcc4 = np.max(mfcc[3])\n",
    "    kurtosis_mfcc4 = kurtosis(mfcc[3])\n",
    "    median_mfcc4 = np.median(mfcc[3])\n",
    "    median_mfcc5 = np.median(mfcc[4])\n",
    "    std_mfcc6 = np.std(mfcc[5])\n",
    "    max_mfcc6 = np.max(mfcc[5])\n",
    "    max_mfcc7 = np.max(mfcc[6])\n",
    "    skew_mfcc7 = skew(mfcc[6])\n",
    "    skew_mfcc9 = skew(mfcc[8])\n",
    "    max_mfcc9 = np.max(mfcc[8])\n",
    "    kurtosis_mfcc9 = kurtosis(mfcc[8])\n",
    "    var_mfcc10 = np.var(mfcc[9])\n",
    "    skew_mfcc11 = skew(mfcc[10])\n",
    "    var_mfcc12 = np.var(mfcc[11])\n",
    "    thpercentile75_mfcc13 = np.nanpercentile(mfcc[12], 0.75)\n",
    "    skew_mfcc13 = skew(mfcc[12])\n",
    "    max_mfcc14 = np.max(mfcc[13])\n",
    "    thpercentile75_mfcc18 = np.nanpercentile(mfcc[17], 0.75)\n",
    "    median_mfcc20 = np.median(mfcc[19])\n",
    "    skew_mfcc20 = skew(mfcc[19])\n",
    "\n",
    "    features_str = f'{file} {acting} {gender} {bef_prep_duration} {aft_prep_duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {valence} {activation} {dominance}\\\n",
    "        {max_tonnetz} {var_spec_bw} {thpercentile75_spec_bw} {mean_spec_cont} {thpercentile75_spec_rolloff}\\\n",
    "        {var_mel_spect} {spikes_zcr} {std_mfcc1} {kurtosis_mfcc1} {kurtosis_mfcc2} {thpercentile75_mfcc2}\\\n",
    "        {kurtosis_mfcc3} {var_mfcc3} {thpercentile75_mfcc4} {var_mfcc4} {max_mfcc4} {kurtosis_mfcc4}\\\n",
    "        {median_mfcc4} {median_mfcc5} {std_mfcc6} {max_mfcc6} {max_mfcc7} {skew_mfcc7} {skew_mfcc9}\\\n",
    "        {max_mfcc9} {kurtosis_mfcc9} {var_mfcc10} {skew_mfcc11} {var_mfcc12} {thpercentile75_mfcc13}\\\n",
    "        {skew_mfcc13} {max_mfcc14} {thpercentile75_mfcc18} {median_mfcc20} {skew_mfcc20}'\n",
    "\n",
    "    writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(audio_dir, headers, proc_feat_dataset=EXTRACTED_FEATURES_FILE, all_features=False, preprocess=False):\n",
    "    # Create a CSV for storing all processed features and write the header\n",
    "    file = open(proc_feat_dataset, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    print(\"Processing audio files from all dialogs:\")\n",
    "    for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "        emoEvalFile = open(file_path)\n",
    "\n",
    "        for line in emoEvalFile:\n",
    "            if line[0] == '[':\n",
    "                args = line.split()\n",
    "                if args[4] in {'ang', 'neu', 'exc', 'hap', 'sad'}:\n",
    "                    # on windows\n",
    "                    audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "                        \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "                    # on linux/macOS\n",
    "                    # audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "                    if all_features:\n",
    "                        extract_all_features(\n",
    "                            audio_file, args[4], args[5][1:-1], args[6][:-1], args[7][:-1], writer, preprocess)\n",
    "                    else:\n",
    "                        extract_features(\n",
    "                            audio_file, args[4], args[5][1:-1], args[6][:-1], args[7][:-1], writer, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [12:20<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [13:59<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, headers, PREPROCESSED_EXTRACTED_FEATURES_FILE, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [18:07<00:00,  7.20s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, all_headers, ALL_EXTRACTED_FEATURES_FILE, all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [19:41<00:00,  7.82s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, all_headers, PREPROCESSED_ALL_EXTRACTED_FEATURES_FILE, all_features=True, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_number = {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'exc': 1}\n",
    "emotion_label = {'ang': \"angry\", 'hap': \"happy\", 'sad': \"sad\", 'neu': \"neutral\", 'exc': \"happy\"}\n",
    "\n",
    "def extract_raw_data(audio_dir, headers, raw_audio_dataset):\n",
    "   file = open(raw_audio_dataset, 'w', newline='')\n",
    "   writer = csv.writer(file)\n",
    "   writer.writerow(headers)\n",
    "\n",
    "   print(\"Processing audio files from all dialogs:\")\n",
    "   for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "      emoEvalFile = open(file_path)\n",
    "\n",
    "      for line in emoEvalFile:\n",
    "         if line[0] == '[':\n",
    "            args = line.split()\n",
    "            duration = float(args[2][:-1]) - float(args[0][1:])\n",
    "            # on windows\n",
    "            emotion, valence, activation, dominance = args[4], args[5][1:-\n",
    "                                                                       1], args[6][:-1], args[7][:-1]\n",
    "            if emotion in emotion_label.keys():\n",
    "                audio_file = f'{args[3]}-{emotion_number[emotion]}'\n",
    "                acting = \"improvised\" if audio_file.split(\n",
    "                    \"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "                gender = \"Male\" if audio_file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "                features_str = f'{audio_file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "                    {valence} {activation} {dominance}'\n",
    "\n",
    "                writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:00<00:00, 2251.87it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_raw_data(AUDIO_DIR, headers_raw, RAW_AUDIO_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_number = {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'exc': 1}\n",
    "\n",
    "def extract_spectograms():\n",
    "  for file_path in tqdm(glob.glob(AUDIO_DIR+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "    for line in open(file_path):\n",
    "      if line[0] == '[':\n",
    "        args = line.split()\n",
    "        emotion = args[4]\n",
    "        if emotion not in emotion_number.keys():\n",
    "          continue\n",
    "        # on windows\n",
    "        audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "            \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "        # on linux/macOS\n",
    "        # audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "        filename = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = preprocess_audio(y, sr)\n",
    "        # Obtain 6 seconds of the audio waveform\n",
    "        duration_seconds = 6\n",
    "        y = librosa.util.fix_length(y, int(sr * duration_seconds))\n",
    "\n",
    "        # ------------Spectrogram------------\n",
    "        spec = np.abs(librosa.stft(y, n_fft=2048))\n",
    "        # with open(f'{AUDIO_SPECTROGRAMS_DIR}{filename}-{emotion_number[emotion]}-spec.pickle', 'wb') as f:\n",
    "        #   pickle.dump(spec, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "        # # ----------Mel-Spectrogram----------\n",
    "        # mel_spec = librosa.feature.melspectrogram(S=spec, n_mels=256)\n",
    "        # with open(f'{AUDIO_MEL_SPECTROGRAMS_DIR}{filename}-{emotion_number[emotion]}-mel.pickle', 'wb') as f:\n",
    "        #   pickle.dump(mel_spec, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # # ---------------MFCCs---------------\n",
    "        # mfcc = librosa.feature.mfcc(S=mel_spec, n_mfcc=40)\n",
    "        # with open(f'{AUDIO_MFCCS_DIR}{filename}-{emotion_number[emotion]}-mfcc.pickle', 'wb') as f:\n",
    "        #   pickle.dump(mfcc, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # ----------Spectrogram Image----------\n",
    "        # fig = plt.figure(dpi=100)\n",
    "        # ax = fig.add_subplot()\n",
    "        # ax.axes.get_xaxis().set_visible(False)\n",
    "        # ax.axes.get_yaxis().set_visible(False)\n",
    "        # ax.set_frame_on(False)\n",
    "        # spec = librosa.amplitude_to_db(np.abs(librosa.stft(y,  n_fft=2048, hop_length=512)), ref=np.max)\n",
    "        # librosa.display.specshow(spec, sr=sr, hop_length=512, ax=ax, cmap=\"viridis_r\")\n",
    "        # fig.savefig(f'{AUDIO_SPECTROGRAM_IMAGES_DIR}{filename}-{emotion_number[emotion]}-spec_img.png',\n",
    "        #             bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        # fig.clf()\n",
    "        # plt.close(fig)\n",
    "\n",
    "        # -------Mel-Spectrogram Image-------\n",
    "        # fig = plt.figure(dpi=100)\n",
    "        # ax = fig.add_subplot()\n",
    "        # ax.axes.get_xaxis().set_visible(False)\n",
    "        # ax.axes.get_yaxis().set_visible(False)\n",
    "        # ax.set_frame_on(False)\n",
    "        # mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256, n_fft=2048, hop_length=512)\n",
    "        # librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), cmap=\"viridis_r\", ax=ax)\n",
    "        # fig.savefig(f'{AUDIO_MEL_SPECTROGRAM_IMAGES_DIR}{filename}-{emotion_number[emotion]}-mel_img.png',\n",
    "        #             bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        # fig.clf()\n",
    "        # plt.close(fig)\n",
    "\n",
    "        # # ----------MFCCs Image----------\n",
    "        # fig = plt.figure(dpi=100)\n",
    "        # ax = fig.add_subplot()\n",
    "        # ax.axes.get_xaxis().set_visible(False)\n",
    "        # ax.axes.get_yaxis().set_visible(False)\n",
    "        # ax.set_frame_on(False)\n",
    "        # spec = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\n",
    "        # mfcc = librosa.feature.mfcc(y=y, sr=sr, S=spec, n_mfcc=40)\n",
    "        # librosa.display.specshow(mfcc, sr=sr, ax=ax, cmap=\"viridis_r\")\n",
    "        # fig.savefig(f'{AUDIO_MFCCS_IMAGES_DIR}{filename}-{emotion_number[emotion]}-mfcc_img.png',\n",
    "        #             bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        # fig.clf()\n",
    "        # plt.close(fig)\n",
    "\n",
    "        # --Spectrogram,MFCC,Mel-Spectrogram--\n",
    "        # spec = np.abs(librosa.stft(y, n_fft=255, hop_length=755))\n",
    "        # mfcc = librosa.feature.mfcc(y=y, sr=sr, S=spec, n_mfcc=128)\n",
    "        # mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, S=spec, n_mels=128)\n",
    "        # all = np.stack((spec, mfcc, mel_spec), axis=-1)\n",
    "        # with open(f'{AUDIO_ALL_DATA_DIR}{filename}-{emotion_number[emotion]}-all.pickle', 'wb') as f:\n",
    "        #   pickle.dump(all, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1025, 188)\n",
      "(256, 188)\n",
      "(40, 188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_spectograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_number = {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'exc': 1}\n",
    "y = None\n",
    "\n",
    "for file_path in glob.glob(AUDIO_DIR+'/Session[0-9]*/dialog/EmoEvaluation/*.txt'):\n",
    "    for line in open(file_path):\n",
    "        if line[0] == '[':\n",
    "            args = line.split()\n",
    "            emotion = args[4]\n",
    "\n",
    "            if emotion not in emotion_number.keys():\n",
    "                continue\n",
    "\n",
    "            audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "                \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "            filename = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "\n",
    "            y, sr = librosa.load(audio_file, sr=16000)\n",
    "            y = preprocess_audio(y, sr)\n",
    "            break\n",
    "    if y.any():\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(16, 12))\n",
    "\n",
    "grid = plt.GridSpec(1, 3, wspace=0.2, hspace=0.2)\n",
    "\n",
    "ax = plt.subplot(grid[0, 0])\n",
    "spec = np.abs(librosa.stft(y, n_fft=2048))\n",
    "librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), sr=sr, ax=ax, cmap=\"viridis_r\", y_axis='linear', x_axis='time')\n",
    "plt.title('Spectrogram')\n",
    "\n",
    "ax = plt.subplot(grid[0, 1])\n",
    "mel_spec = librosa.feature.melspectrogram(S=spec, n_mels=256)\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), cmap=\"viridis_r\", ax=ax)\n",
    "plt.title('Mel Spectrogram')\n",
    "\n",
    "ax = plt.subplot(grid[0, 2])\n",
    "mfcc = librosa.feature.mfcc(S=mel_spec, n_mfcc=40)\n",
    "librosa.display.specshow(mfcc, sr=sr, ax=ax, cmap=\"viridis_r\", x_axis='time')\n",
    "plt.title('MFCCs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "133e7c8b355aba2fdf124ff52e5351672d24544aa772c704b7c0070650922ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

\chapter{Discussion and Conclusions}
\label{chapter:conc}

\section{Discussion}


\section{Contributions and Developed Tools}

This study has made several contributions towards the improvement of accuracy and efficiency in emotion recognition systems.

Firstly, this study contributes to the \ac{sota} research in \ac{ser} by providing a comprehensive and up-to-date review of recent advances in the field. Our review includes both traditional approaches, such as the use of hand-crafted features and machine learning techniques, as well as more recent \ac{dl}-based approaches. We also cover recent developments in multimodal \ac{ser}, which combine audio with other modalities such as visual or physiological signals, and discuss the challenges and opportunities of these approaches. Our review also includes a detailed analysis of recent benchmark datasets, which highlights the need for larger and more diverse datasets, as well as the importance of accounting for the variability of emotions across cultures and contexts.

Furthermore, by exploring the traditional approach for \ac{ser}, we were able to identify a small set of audio features that can be used by any model. Additionally, from our \ac{dl} approach, we were able to demonstrate the effectiveness of transfer learning techniques.

The data stratification study of the \ac{iemo} dataset, allowed us to create a set of conditions for obtaining better quality data that is suitable for more diverse environments. This study not only helped us in our research but can also be used as a guideline for future research.

Moreover, we have developed two Python classes that automate the process of feature extraction and classification of emotions \cite{Mario_Silva_Speech_Emotion_Recognition_2023}. The classes allow users to choose between the \ac{xgb} and ResNet50 models trained on either the entire \ac{iemo} dataset or the limited data resulting from the data stratification study. The classes can return the predicted emotion or probabilities of each emotion.

Lastly, we have developed a \ac{ser} pipeline that can analyze an audio stream in real-time or offline, using the Silero \ac{vad} model to detect voiced speech segments and the developed \ac{ser} classification python class to label the segments. This pipeline can be easily integrated into streaming services, such as video conference systems, and can enhance their experience by providing real-time emotional feedback to participants.

In conclusion, this study has contributed to the field of \ac{ser} by providing a extensive overview of the field, by exploring the traditional and \ac{dl} approaches, and by developing tools that can improve the accuracy and efficiency of emotion recognition systems. The data stratification study of the \ac{iemo} dataset has allowed us to identify better quality data suitable for diverse environments, and our Python classes can automate the whole process of a \ac{ser} pipeline, from processing the speech, in online or offline time, to the classification of emotions.

\section{Conclusion}

This dissertation aimed to explore and develop \ac{ser} models using various approaches and techniques. To achieve this goal, state-of-the-art research on emotion recognition was reviewed, and different methods were analyzed for speech emotion recognition. A comprehensive methodology was proposed for developing reliable \ac{ser} models, which included requirements gathering, dataset selection, audio feature engineering, and model implementation.

The requirements-gathering phase involved identifying the research objectives, selecting the appropriate dataset, and defining the evaluation criteria for the models. The dataset was selected after analyzing different publicly available datasets and stratifying the data according to various factors such as gender, clip durations, and annotated labels, ensuring that the models were more reliable and accurate.

Various acoustic features such as prosodic, spectral, and voice-quality features were extracted from the speech signals during the audio feature engineering phase. Feature selection techniques such as correlation-based feature and backwards selection were used to identify the most relevant features for the models.

Different models were implemented and evaluated, including traditional machine learning models, and \ac{dl} models. Transfer learning and ensemble methods were investigated to improve the performance of the models further. The models were evaluated using various performance metrics such as accuracy, precision, recall, F1 score, and \ac{mcc}.

In summary, this dissertation proposes a comprehensive methodology for developing reliable \ac{ser} models and demonstrates that the proposed methodology significantly improves the performance of the models. The findings of this study can be used to improve the accuracy and reliability of emotion recognition systems and contribute to the development of emotionally intelligent systems. The future of speech emotion recognition research and development is promising, with many exciting opportunities for further exploration and innovation in this area.


\section{Future Work}

In order to advance our work on \ac{ser}, several areas of future research could be explored. One promising direction is the incorporation of additional features, such as physiological signals or facial expressions, to create more robust and accurate models. The development of multimodal models that can better handle noisy and non-stationary acoustic environments is also a potential area for improvement. This could involve weighing each input source differently depending on certain factors to improve classification accuracy. For example, the video brightness could be reduced if it is too low, to reduce its classification weight.

Another area for future work is the development of contextually aware models. These models could take into account situational factors, as well as the user's history and preferences, to create more personalized emotion recognition services.

While this study has made progress towards improving the accuracy and efficiency of \ac{ser} systems, there are still areas that require further investigation. One such area is the audio preprocessing techniques, which have been shown to play a critical role in the success of \ac{ser} models. Therefore, an in-depth study of the audio preprocessing techniques, and their effects on the classification accuracy, should be conducted using the available datasets and the developed models. This would allow for a better understanding of the optimal audio preprocessing pipeline for \ac{ser} systems.

Another area that requires further investigation is the fine-tuning of the developed models, particularly the deep learning model. This requires more computationally expensive operations, such as increasing the number of trainable parameters or adjusting the hyperparameters of the model. However, fine-tuning can potentially lead to even better performance and accuracy.

Finally, integrating emotion recognition technology into a range of real-world applications, such as virtual assistants, smart homes, monitoring systems, and call centers, is a promising avenue for future research that can help validate the developed \ac{ser} tools. The design of emotionally intelligent and responsive interactive interfaces could also enhance the end user experience. Continued research and development in these areas could lead to significant advances in \ac{ser} technology and its applications.
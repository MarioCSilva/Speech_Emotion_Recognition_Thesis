{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Audio_Sentiment_Analysis/iemocap/data"
      ],
      "metadata": {
        "id": "gBKdo1yXoJAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4bb387a-d476-4983-e5b2-b9ffbe3ac8fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Audio_Sentiment_Analysis/iemocap/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "843rNBLvkSJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3edc85-a7f1-432d-a1ca-ab5bfd1decd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autokeras\n",
            "  Downloading autokeras-1.0.19-py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.3.5)\n",
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (21.3)\n",
            "Collecting keras-tuner>=1.1.0\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 42.1 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.1.0->autokeras) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.1.0->autokeras) (2.23.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.1.0->autokeras) (2.8.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.1.0->autokeras) (5.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (0.26.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (14.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (4.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.14.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (0.5.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.46.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->autokeras) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.8.0->autokeras) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.1.0->autokeras) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.1.0->autokeras) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner>=1.1.0->autokeras) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.1.0->autokeras) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.1.0->autokeras) (3.2.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.1.0->autokeras) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner>=1.1.0->autokeras) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner>=1.1.0->autokeras) (0.7.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner, autokeras\n",
            "Successfully installed autokeras-1.0.19 keras-tuner-1.1.2 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "# go to upper diretory\n",
        "sys.path.append(os.path.abspath('./../../../'))\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier as RandomForest, RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold\n",
        "from Audio_Sentiment_Analysis.utils.Configuration import Configuration\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanAbsoluteError, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "AUDIO_DIR = f\"{os.path.abspath('./../../../')}/IEMOCAP_Dataset\"\n",
        "EXTRACTED_FEATURES_FILE = 'extracted_features_iemocap.csv'\n",
        "CONFIG_FILE = f\"{os.path.abspath('./../../../')}/Audio_Sentiment_Analysis/iemocap/config.json\"\n",
        "config = Configuration.load_json(CONFIG_FILE)\n",
        "!pip install autokeras\n",
        "import autokeras as ak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSg2cYuckSJo"
      },
      "source": [
        "## Read the extracted features from the CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLl1X99mkSJp",
        "outputId": "c3d6a358-4393-4ecd-af7c-6a71e81ee445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Audio Files: 10039\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Interaction  Gender  Duration Emotion  Emotion_Id  \\\n",
              "File                                                                       \n",
              "Ses01F_impro01_F012     improvised  Female     2.750   angry           0   \n",
              "Ses01F_impro04_F028     improvised  Female     2.010   angry           0   \n",
              "Ses01F_impro04_F029     improvised  Female     3.160   angry           0   \n",
              "Ses01F_impro04_F030     improvised  Female     3.185   angry           0   \n",
              "Ses01F_impro04_F031     improvised  Female     4.400   angry           0   \n",
              "...                            ...     ...       ...     ...         ...   \n",
              "Ses05M_script03_2_M013    scripted    Male     9.340   other          10   \n",
              "Ses05M_script03_2_M014    scripted    Male     4.150   other          10   \n",
              "Ses05M_script03_2_M018    scripted    Male     2.170   other          10   \n",
              "Ses05M_script03_2_M019    scripted    Male     1.180   other          10   \n",
              "Ses05M_script03_2_M021    scripted    Male     2.400   other          10   \n",
              "\n",
              "                        Valence  Activation  Dominance  std_chroma_stft  \\\n",
              "File                                                                      \n",
              "Ses01F_impro01_F012         2.0         3.5        3.5         0.296270   \n",
              "Ses01F_impro04_F028         2.0         3.5        3.5         0.319851   \n",
              "Ses01F_impro04_F029         1.5         4.0        4.0         0.298296   \n",
              "Ses01F_impro04_F030         1.5         3.5        4.0         0.316109   \n",
              "Ses01F_impro04_F031         1.5         3.0        3.5         0.314387   \n",
              "...                         ...         ...        ...              ...   \n",
              "Ses05M_script03_2_M013      2.0         3.5        4.0         0.314951   \n",
              "Ses05M_script03_2_M014      1.5         3.5        4.0         0.315053   \n",
              "Ses05M_script03_2_M018      3.5         3.0        3.0         0.302163   \n",
              "Ses05M_script03_2_M019      2.5         3.0        3.5         0.310805   \n",
              "Ses05M_script03_2_M021      2.0         3.5        4.0         0.320674   \n",
              "\n",
              "                        mean_zcr  ...  max_mfcc6  min_mfcc7   var_mfcc9  \\\n",
              "File                              ...                                     \n",
              "Ses01F_impro01_F012     0.082856  ...  39.327560 -40.651329   42.375973   \n",
              "Ses01F_impro04_F028     0.086560  ...  62.818893 -40.228039   84.092949   \n",
              "Ses01F_impro04_F029     0.085880  ...  29.858166 -33.455795  164.847565   \n",
              "Ses01F_impro04_F030     0.106300  ...  48.537384 -38.774422  178.178299   \n",
              "Ses01F_impro04_F031     0.065918  ...  44.125771 -49.090309  100.364433   \n",
              "...                          ...  ...        ...        ...         ...   \n",
              "Ses05M_script03_2_M013  0.063627  ...  58.600712 -46.007534  113.818436   \n",
              "Ses05M_script03_2_M014  0.067145  ...  73.493271 -54.896301  162.575638   \n",
              "Ses05M_script03_2_M018  0.062219  ...  51.708046 -34.321487  181.688660   \n",
              "Ses05M_script03_2_M019  0.116048  ...  58.984211 -54.916523   73.387558   \n",
              "Ses05M_script03_2_M021  0.120333  ...  68.866486 -47.562469  194.224579   \n",
              "\n",
              "                        max_mfcc9  max_mfcc10  max_mfcc13  var_mfcc14  \\\n",
              "File                                                                    \n",
              "Ses01F_impro01_F012     19.492970   19.512123   18.821735   47.666279   \n",
              "Ses01F_impro04_F028     24.274593   11.059961   36.497154   29.247034   \n",
              "Ses01F_impro04_F029     34.833263   22.882631   33.635147  118.356186   \n",
              "Ses01F_impro04_F030     24.138340   11.319570   35.046803  183.465393   \n",
              "Ses01F_impro04_F031     35.686844   29.594337   11.990172   68.253944   \n",
              "...                           ...         ...         ...         ...   \n",
              "Ses05M_script03_2_M013  32.636822   28.463696   25.335880   78.160225   \n",
              "Ses05M_script03_2_M014  31.630028   12.225033   33.560890   87.977707   \n",
              "Ses05M_script03_2_M018  23.869041   14.644627   14.957552   41.870499   \n",
              "Ses05M_script03_2_M019  23.126339   13.553119   24.739117   51.918839   \n",
              "Ses05M_script03_2_M021  23.398148   23.116982   20.446377   71.395477   \n",
              "\n",
              "                        var_mfcc15  min_mfcc17  min_mfcc19  \n",
              "File                                                        \n",
              "Ses01F_impro01_F012      51.472340  -18.892769  -27.943581  \n",
              "Ses01F_impro04_F028      61.287384  -17.456673  -15.254041  \n",
              "Ses01F_impro04_F029     128.460770  -32.842518  -12.516586  \n",
              "Ses01F_impro04_F030     119.128494  -13.127378  -24.713459  \n",
              "Ses01F_impro04_F031      51.992161  -20.573139  -20.019981  \n",
              "...                            ...         ...         ...  \n",
              "Ses05M_script03_2_M013   66.925186  -27.014687  -21.044426  \n",
              "Ses05M_script03_2_M014   42.288971  -24.003210  -19.897762  \n",
              "Ses05M_script03_2_M018   43.103497  -20.033688  -19.955128  \n",
              "Ses05M_script03_2_M019   60.310535  -22.075874  -19.231363  \n",
              "Ses05M_script03_2_M021   28.724272  -28.672276  -21.132488  \n",
              "\n",
              "[10039 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d41a48db-040f-445a-b3a9-abd533dc7d8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Interaction</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion_Id</th>\n",
              "      <th>Valence</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Dominance</th>\n",
              "      <th>std_chroma_stft</th>\n",
              "      <th>mean_zcr</th>\n",
              "      <th>...</th>\n",
              "      <th>max_mfcc6</th>\n",
              "      <th>min_mfcc7</th>\n",
              "      <th>var_mfcc9</th>\n",
              "      <th>max_mfcc9</th>\n",
              "      <th>max_mfcc10</th>\n",
              "      <th>max_mfcc13</th>\n",
              "      <th>var_mfcc14</th>\n",
              "      <th>var_mfcc15</th>\n",
              "      <th>min_mfcc17</th>\n",
              "      <th>min_mfcc19</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>File</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Ses01F_impro01_F012</th>\n",
              "      <td>improvised</td>\n",
              "      <td>Female</td>\n",
              "      <td>2.750</td>\n",
              "      <td>angry</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.296270</td>\n",
              "      <td>0.082856</td>\n",
              "      <td>...</td>\n",
              "      <td>39.327560</td>\n",
              "      <td>-40.651329</td>\n",
              "      <td>42.375973</td>\n",
              "      <td>19.492970</td>\n",
              "      <td>19.512123</td>\n",
              "      <td>18.821735</td>\n",
              "      <td>47.666279</td>\n",
              "      <td>51.472340</td>\n",
              "      <td>-18.892769</td>\n",
              "      <td>-27.943581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses01F_impro04_F028</th>\n",
              "      <td>improvised</td>\n",
              "      <td>Female</td>\n",
              "      <td>2.010</td>\n",
              "      <td>angry</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.319851</td>\n",
              "      <td>0.086560</td>\n",
              "      <td>...</td>\n",
              "      <td>62.818893</td>\n",
              "      <td>-40.228039</td>\n",
              "      <td>84.092949</td>\n",
              "      <td>24.274593</td>\n",
              "      <td>11.059961</td>\n",
              "      <td>36.497154</td>\n",
              "      <td>29.247034</td>\n",
              "      <td>61.287384</td>\n",
              "      <td>-17.456673</td>\n",
              "      <td>-15.254041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses01F_impro04_F029</th>\n",
              "      <td>improvised</td>\n",
              "      <td>Female</td>\n",
              "      <td>3.160</td>\n",
              "      <td>angry</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.298296</td>\n",
              "      <td>0.085880</td>\n",
              "      <td>...</td>\n",
              "      <td>29.858166</td>\n",
              "      <td>-33.455795</td>\n",
              "      <td>164.847565</td>\n",
              "      <td>34.833263</td>\n",
              "      <td>22.882631</td>\n",
              "      <td>33.635147</td>\n",
              "      <td>118.356186</td>\n",
              "      <td>128.460770</td>\n",
              "      <td>-32.842518</td>\n",
              "      <td>-12.516586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses01F_impro04_F030</th>\n",
              "      <td>improvised</td>\n",
              "      <td>Female</td>\n",
              "      <td>3.185</td>\n",
              "      <td>angry</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.316109</td>\n",
              "      <td>0.106300</td>\n",
              "      <td>...</td>\n",
              "      <td>48.537384</td>\n",
              "      <td>-38.774422</td>\n",
              "      <td>178.178299</td>\n",
              "      <td>24.138340</td>\n",
              "      <td>11.319570</td>\n",
              "      <td>35.046803</td>\n",
              "      <td>183.465393</td>\n",
              "      <td>119.128494</td>\n",
              "      <td>-13.127378</td>\n",
              "      <td>-24.713459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses01F_impro04_F031</th>\n",
              "      <td>improvised</td>\n",
              "      <td>Female</td>\n",
              "      <td>4.400</td>\n",
              "      <td>angry</td>\n",
              "      <td>0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.314387</td>\n",
              "      <td>0.065918</td>\n",
              "      <td>...</td>\n",
              "      <td>44.125771</td>\n",
              "      <td>-49.090309</td>\n",
              "      <td>100.364433</td>\n",
              "      <td>35.686844</td>\n",
              "      <td>29.594337</td>\n",
              "      <td>11.990172</td>\n",
              "      <td>68.253944</td>\n",
              "      <td>51.992161</td>\n",
              "      <td>-20.573139</td>\n",
              "      <td>-20.019981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses05M_script03_2_M013</th>\n",
              "      <td>scripted</td>\n",
              "      <td>Male</td>\n",
              "      <td>9.340</td>\n",
              "      <td>other</td>\n",
              "      <td>10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.314951</td>\n",
              "      <td>0.063627</td>\n",
              "      <td>...</td>\n",
              "      <td>58.600712</td>\n",
              "      <td>-46.007534</td>\n",
              "      <td>113.818436</td>\n",
              "      <td>32.636822</td>\n",
              "      <td>28.463696</td>\n",
              "      <td>25.335880</td>\n",
              "      <td>78.160225</td>\n",
              "      <td>66.925186</td>\n",
              "      <td>-27.014687</td>\n",
              "      <td>-21.044426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses05M_script03_2_M014</th>\n",
              "      <td>scripted</td>\n",
              "      <td>Male</td>\n",
              "      <td>4.150</td>\n",
              "      <td>other</td>\n",
              "      <td>10</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.315053</td>\n",
              "      <td>0.067145</td>\n",
              "      <td>...</td>\n",
              "      <td>73.493271</td>\n",
              "      <td>-54.896301</td>\n",
              "      <td>162.575638</td>\n",
              "      <td>31.630028</td>\n",
              "      <td>12.225033</td>\n",
              "      <td>33.560890</td>\n",
              "      <td>87.977707</td>\n",
              "      <td>42.288971</td>\n",
              "      <td>-24.003210</td>\n",
              "      <td>-19.897762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses05M_script03_2_M018</th>\n",
              "      <td>scripted</td>\n",
              "      <td>Male</td>\n",
              "      <td>2.170</td>\n",
              "      <td>other</td>\n",
              "      <td>10</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.302163</td>\n",
              "      <td>0.062219</td>\n",
              "      <td>...</td>\n",
              "      <td>51.708046</td>\n",
              "      <td>-34.321487</td>\n",
              "      <td>181.688660</td>\n",
              "      <td>23.869041</td>\n",
              "      <td>14.644627</td>\n",
              "      <td>14.957552</td>\n",
              "      <td>41.870499</td>\n",
              "      <td>43.103497</td>\n",
              "      <td>-20.033688</td>\n",
              "      <td>-19.955128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses05M_script03_2_M019</th>\n",
              "      <td>scripted</td>\n",
              "      <td>Male</td>\n",
              "      <td>1.180</td>\n",
              "      <td>other</td>\n",
              "      <td>10</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.310805</td>\n",
              "      <td>0.116048</td>\n",
              "      <td>...</td>\n",
              "      <td>58.984211</td>\n",
              "      <td>-54.916523</td>\n",
              "      <td>73.387558</td>\n",
              "      <td>23.126339</td>\n",
              "      <td>13.553119</td>\n",
              "      <td>24.739117</td>\n",
              "      <td>51.918839</td>\n",
              "      <td>60.310535</td>\n",
              "      <td>-22.075874</td>\n",
              "      <td>-19.231363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ses05M_script03_2_M021</th>\n",
              "      <td>scripted</td>\n",
              "      <td>Male</td>\n",
              "      <td>2.400</td>\n",
              "      <td>other</td>\n",
              "      <td>10</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.320674</td>\n",
              "      <td>0.120333</td>\n",
              "      <td>...</td>\n",
              "      <td>68.866486</td>\n",
              "      <td>-47.562469</td>\n",
              "      <td>194.224579</td>\n",
              "      <td>23.398148</td>\n",
              "      <td>23.116982</td>\n",
              "      <td>20.446377</td>\n",
              "      <td>71.395477</td>\n",
              "      <td>28.724272</td>\n",
              "      <td>-28.672276</td>\n",
              "      <td>-21.132488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10039 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d41a48db-040f-445a-b3a9-abd533dc7d8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d41a48db-040f-445a-b3a9-abd533dc7d8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d41a48db-040f-445a-b3a9-abd533dc7d8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv(EXTRACTED_FEATURES_FILE)\n",
        "print(f\"Number of Audio Files: {df.shape[0]}\")\n",
        "df = df.sort_values(['Emotion_Id', 'Gender'], ascending = (True, True))\n",
        "df = df.set_index('File')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJvGhP-xkSJr"
      },
      "source": [
        "## Data used in SOA models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwMOp3UekSJr",
        "outputId": "2b29646d-2553-419e-96d0-f21586fc66f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(loc, value, pi)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Emotion\n",
              "                     count\n",
              "Emotion Emotion_Id        \n",
              "angry   0             1103\n",
              "happy   1             1636\n",
              "neutral 3             1708\n",
              "sad     2             1084"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb75a3c8-539b-4531-817f-ae68280773ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion_Id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>angry</th>\n",
              "      <th>0</th>\n",
              "      <td>1103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happy</th>\n",
              "      <th>1</th>\n",
              "      <td>1636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <th>3</th>\n",
              "      <td>1708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <th>2</th>\n",
              "      <td>1084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb75a3c8-539b-4531-817f-ae68280773ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb75a3c8-539b-4531-817f-ae68280773ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb75a3c8-539b-4531-817f-ae68280773ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_soa = df[df['Emotion'].isin({'angry', 'neutral', 'sad', 'happy', 'excited'})]\n",
        "df_soa.loc[df_soa['Emotion'] == 'excited', 'Emotion'] = 'happy'\n",
        "df_soa.loc[df_soa['Emotion_Id'] == 5, 'Emotion_Id'] = 1\n",
        "df_soa.groupby(['Emotion', 'Emotion_Id']).agg({'Emotion': ['count']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln7PNIbbkSJs"
      },
      "source": [
        "### Maybe improvised data is more adequate to the problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqVD_PpZkSJt",
        "outputId": "e3120cf0-27c7-4dae-c72c-f577e496ae9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Emotion\n",
              "          count\n",
              "Emotion        \n",
              "angry       289\n",
              "happy       947\n",
              "neutral    1099\n",
              "sad         608"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5653fb42-5eb8-4475-92c5-4075aca05cbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Emotion</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>angry</th>\n",
              "      <td>289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happy</th>\n",
              "      <td>947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>1099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5653fb42-5eb8-4475-92c5-4075aca05cbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5653fb42-5eb8-4475-92c5-4075aca05cbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5653fb42-5eb8-4475-92c5-4075aca05cbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_soa[df_soa['Interaction'] == 'improvised'].groupby(['Emotion']).agg({'Emotion': ['count']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmKa3F8KkSJu"
      },
      "source": [
        "### The duration of the audio can have importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31t9IeOmkSJv",
        "outputId": "1b619eb1-5825-4bab-ea49-7546b3d3acab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Emotion\n",
              "          count\n",
              "Emotion        \n",
              "angry       813\n",
              "happy      1170\n",
              "neutral    1105\n",
              "sad         843"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8b7da95-ee4b-49a8-9337-e89f7005ad53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Emotion</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>angry</th>\n",
              "      <td>813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>happy</th>\n",
              "      <td>1170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>1105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8b7da95-ee4b-49a8-9337-e89f7005ad53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8b7da95-ee4b-49a8-9337-e89f7005ad53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8b7da95-ee4b-49a8-9337-e89f7005ad53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_soa[df_soa['Duration'] >= 2.5].groupby(['Emotion']).agg({'Emotion': ['count']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD-nKqBzkSJw"
      },
      "outputs": [],
      "source": [
        "def plot_cm_predictions(model, input_data, labels, cv=5, draw_corr_matrix=True, verbose=1, n_jobs=4):\n",
        "    y_pred = cross_val_predict(model, input_data, labels, cv=cv, verbose=verbose, n_jobs=n_jobs)\n",
        "\n",
        "    print(\"accuracy: \", metrics.accuracy_score(labels, y_pred))\n",
        "    print(\"f1 score macro: \", metrics.f1_score(labels, y_pred, average='macro') )\n",
        "    print(\"f1 score micro: \", metrics.f1_score(labels, y_pred, average='micro') )\n",
        "    print(\"precision score: \", metrics.precision_score(labels, y_pred, average='macro') )\n",
        "    print(\"recall score: \", metrics.recall_score(labels, y_pred, average='macro') )\n",
        "    print(\"hamming loss: \", metrics.hamming_loss(labels, y_pred))\n",
        "    print(\"matthews corrcoef: \", metrics.matthews_corrcoef(labels, y_pred) )\n",
        "    print(\"zero one loss: \", metrics.zero_one_loss(labels, y_pred))\n",
        "    print(\"mean absolute error: \", metrics.mean_absolute_error(labels, y_pred))\n",
        "\n",
        "    print(metrics.classification_report(labels, y_pred))\n",
        "\n",
        "    if draw_corr_matrix:\n",
        "        cm = metrics.confusion_matrix(labels, y_pred)\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "        plt.title(\"Confusion Matrix Predicted Labels\")\n",
        "        plt.xlabel(\"Emotions Labels\")\n",
        "        plt.ylabel(\"Emotions Labels\")\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbM2lOxSkSJx"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0hy9WDbkSJx"
      },
      "outputs": [],
      "source": [
        "data = df_soa.iloc[:,8:]\n",
        "regression_labels = df_soa.iloc[:,5:8].values\n",
        "categorical_labels = np.ravel(df_soa.iloc[:,4:5].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWPBD1PkSJy"
      },
      "source": [
        "### Simple Random Forests\n",
        "#### Categorical Problem (anger, happiness (+ excited), neutral, sadness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjIxNUmRkSJy",
        "outputId": "4d9e4de0-e015-41e8-ae3e-b2540440e7cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:   30.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.4952088229976496\n",
            "f1 score macro:  0.5128402898669654\n",
            "f1 score micro:  0.4952088229976496\n",
            "precision score:  0.5235160408175284\n",
            "recall score:  0.5050289689027115\n",
            "hamming loss:  0.5047911770023504\n",
            "matthews corrcoef:  0.31341736401352394\n",
            "zero one loss:  0.5047911770023503\n",
            "mean absolute error:  0.7868378231784487\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.54      0.58      1103\n",
            "           1       0.41      0.45      0.42      1636\n",
            "           2       0.60      0.57      0.58      1084\n",
            "           3       0.45      0.47      0.46      1708\n",
            "\n",
            "    accuracy                           0.50      5531\n",
            "   macro avg       0.52      0.51      0.51      5531\n",
            "weighted avg       0.50      0.50      0.50      5531\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_cm_predictions(RandomForest(random_state=1, max_features=None), data.values, categorical_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wde1RYkSJz"
      },
      "source": [
        "#### Regression Problem (Valence, Activation, Dominance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RERiPbRQkSJz",
        "outputId": "8c1de7b0-49d4-4221-869b-a3e25dbf91e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done   2 out of   5 | elapsed:   21.7s remaining:   32.5s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 0.663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Done   5 out of   5 | elapsed:   22.2s finished\n"
          ]
        }
      ],
      "source": [
        "model = RandomForestRegressor(random_state=1, max_features=None)\n",
        "pred_values = cross_val_predict(model, data.values, regression_labels, cv=5, verbose=1, n_jobs=8)\n",
        "print('MAE: %.3f' % metrics.mean_absolute_error(pred_values, regression_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IebHZrj3kSJz"
      },
      "source": [
        "### Convolution Neural Networks\n",
        "#### Categorical Problem (anger, happiness (+ excited), neutral, sadness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPxlcsQBkSJ0",
        "outputId": "89dc23f8-ccda-427d-eccd-922e25e6e017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "____________________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   Trainable  \n",
            "============================================================================\n",
            " conv1d (Conv1D)             (None, 20, 256)           1536      Y          \n",
            "                                                                            \n",
            " batch_normalization (BatchN  (None, 20, 256)          1024      Y          \n",
            " ormalization)                                                              \n",
            "                                                                            \n",
            " activation (Activation)     (None, 20, 256)           0         Y          \n",
            "                                                                            \n",
            " conv1d_1 (Conv1D)           (None, 16, 128)           163968    Y          \n",
            "                                                                            \n",
            " activation_1 (Activation)   (None, 16, 128)           0         Y          \n",
            "                                                                            \n",
            " dropout (Dropout)           (None, 16, 128)           0         Y          \n",
            "                                                                            \n",
            " max_pooling1d (MaxPooling1D  (None, 14, 128)          0         Y          \n",
            " )                                                                          \n",
            "                                                                            \n",
            " conv1d_2 (Conv1D)           (None, 10, 128)           82048     Y          \n",
            "                                                                            \n",
            " activation_2 (Activation)   (None, 10, 128)           0         Y          \n",
            "                                                                            \n",
            " batch_normalization_1 (Batc  (None, 10, 128)          512       Y          \n",
            " hNormalization)                                                            \n",
            "                                                                            \n",
            " dropout_1 (Dropout)         (None, 10, 128)           0         Y          \n",
            "                                                                            \n",
            " flatten (Flatten)           (None, 1280)              0         Y          \n",
            "                                                                            \n",
            " dense (Dense)               (None, 3)                 3843      Y          \n",
            "                                                                            \n",
            " activation_3 (Activation)   (None, 3)                 0         Y          \n",
            "                                                                            \n",
            "============================================================================\n",
            "Total params: 252,931\n",
            "Trainable params: 252,163\n",
            "Non-trainable params: 768\n",
            "____________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model_0(input_shape=(24, 1), loss=MeanSquaredError(), optimizer=Adam(learning_rate=1e-5, epsilon=1e-6)):\n",
        "    if loss.__class__ == SparseCategoricalCrossentropy().__class__:\n",
        "        n_labels = 4\n",
        "        metrics = ['accuracy']\n",
        "        activation = 'softmax'\n",
        "    else:\n",
        "        n_labels = 3\n",
        "        metrics = ['mae']\n",
        "        activation = 'relu'\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(256, (5), input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv1D(128, (5)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling1D(n_labels, strides=1))\n",
        "\n",
        "    model.add(Conv1D(128, (5)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(n_labels))\n",
        "\n",
        "    model.add(Activation(activation))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "    return model\n",
        "build_model_0(input_shape=(data.iloc[0].shape[0],1)).summary(show_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0zhxiRekSJ1"
      },
      "outputs": [],
      "source": [
        "def categorical_cross_validation(callbacks, no_epochs, batch_size, loss, optimizer, verbosity, num_folds):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "  fold_no = 1\n",
        "  \n",
        "  X = df_soa.iloc[:,8:]\n",
        "  y = df_soa.iloc[:,4:5]\n",
        "\n",
        "  for train, test in kfold.split(X, y):\n",
        "    model = build_model_0(input_shape=(data.shape[1], 1), loss=loss, optimizer=optimizer)\n",
        "    \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=no_epochs,\n",
        "                verbose=verbosity,\n",
        "                callbacks=callbacks,\n",
        "                workers=4)\n",
        "\n",
        "    scores = model.evaluate(X_test, y_test, verbose=verbosity)\n",
        "\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dy_fALukSJ2",
        "outputId": "a2f56cc0-eb60-408a-caa2-7288bd636b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 1.111961007118225; accuracy of 49.232158064842224%\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 1.1252243518829346; accuracy of 48.73417615890503%\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 1.1336780786514282; accuracy of 46.473780274391174%\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 1.1383367776870728; accuracy of 46.383363008499146%\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 1.1080365180969238; accuracy of 48.19168150424957%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "> Fold 1 - Loss: 1.111961007118225 - Accuracy: 49.232158064842224%\n",
            "> Fold 2 - Loss: 1.1252243518829346 - Accuracy: 48.73417615890503%\n",
            "> Fold 3 - Loss: 1.1336780786514282 - Accuracy: 46.473780274391174%\n",
            "> Fold 4 - Loss: 1.1383367776870728 - Accuracy: 46.383363008499146%\n",
            "> Fold 5 - Loss: 1.1080365180969238 - Accuracy: 48.19168150424957%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 47.80303180217743 (+- 1.1698589134156916)\n",
            "> Loss: 1.123447346687317\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=10)\n",
        "no_epochs = 500 # try 300 or 700\n",
        "batch_size = 16 # try 16\n",
        "learning_rate=1e-5 # try 1e-4\n",
        "loss = SparseCategoricalCrossentropy()\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "verbosity = 0\n",
        "num_folds = 5\n",
        "\n",
        "categorical_cross_validation(callback, no_epochs, batch_size, loss, optimizer, verbosity, num_folds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgZb_YF3kSJ2"
      },
      "source": [
        "#### Regression Problem (Valence, Activation, Dominance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBmTJHdlkSJ2"
      },
      "source": [
        "Note: Here Stratified K-Fold cannot be used because it has a continuous multi output label, so the distribution of emotions per fold is not going to be balanced.\n",
        "\n",
        "Could use the categorical emotion to generate the folds and then use the regression labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv0CjQVXkSJ3"
      },
      "outputs": [],
      "source": [
        "def regression_cross_validation(callbacks, no_epochs, batch_size, loss, optimizer, verbosity, num_folds):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
        "  mae_per_fold = []\n",
        "  loss_per_fold = []\n",
        "  fold_no = 1\n",
        "  \n",
        "  X = df_soa.iloc[:,8:]\n",
        "  y = df_soa.iloc[:,5:8]\n",
        "\n",
        "  for train, test in kfold.split(X, y):\n",
        "    model = build_model_0(input_shape=(data.shape[1], 1), loss=loss, optimizer=optimizer)\n",
        "    \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=no_epochs,\n",
        "                verbose=verbosity,\n",
        "                callbacks=callbacks,\n",
        "                workers=4)\n",
        "\n",
        "    scores = model.evaluate(X_test, y_test, verbose=verbosity)\n",
        "\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    mae_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(mae_per_fold)):\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - MAE: {mae_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> MAE: {np.mean(mae_per_fold)} (+- {np.std(mae_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJpPpYOikSJ3",
        "outputId": "4168b929-8262-4a24-a7a4-057d3a629679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.5870349407196045; mae of 58.70349407196045%\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 0.615779459476471; mae of 61.577945947647095%\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 0.5887221693992615; mae of 58.87221693992615%\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 0.5906510353088379; mae of 59.06510353088379%\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 0.6135765910148621; mae of 61.35765314102173%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "> Fold 1 - Loss: 0.5870349407196045 - MAE: 58.70349407196045%\n",
            "> Fold 2 - Loss: 0.615779459476471 - MAE: 61.577945947647095%\n",
            "> Fold 3 - Loss: 0.5887221693992615 - MAE: 58.87221693992615%\n",
            "> Fold 4 - Loss: 0.5906510353088379 - MAE: 59.06510353088379%\n",
            "> Fold 5 - Loss: 0.6135765910148621 - MAE: 61.35765314102173%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> MAE: 59.91528272628784 (+- 1.2746845917867797)\n",
            "> Loss: 0.5991528391838074\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "callback = EarlyStopping(monitor='loss', patience=10)\n",
        "no_epochs = 500 # try 300 or 700\n",
        "batch_size = 32 # try 16\n",
        "learning_rate=1e-5 # try 1e-5\n",
        "loss = MeanAbsoluteError()\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "verbosity = 0\n",
        "num_folds = 5\n",
        "\n",
        "regression_cross_validation(callback, no_epochs, batch_size, loss, optimizer, verbosity, num_folds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoKeras"
      ],
      "metadata": {
        "id": "Tm3wT0d8DiNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_validation2(no_epochs, batch_size, loss, optimizer, verbosity, num_folds):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "  fold_no = 1\n",
        "  \n",
        "  X = df_soa.iloc[:,8:]\n",
        "  y = df_soa.iloc[:,4:5]\n",
        "\n",
        "  for train, test in kfold.split(X, y):\n",
        "    model = ak.StructuredDataClassifier(overwrite=True, max_trials=100, seed=1, directory=f\"best_regression_keras_model_fold_{fold_no}\")\n",
        "    \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=no_epochs,\n",
        "                verbose=verbosity,\n",
        "                callbacks=[EarlyStopping()],\n",
        "                workers=8)\n",
        "  \n",
        "    print(model.export_model().summary())\n",
        "\n",
        "\n",
        "    scores = model.evaluate(X_test, y_test, verbose=verbosity)\n",
        "\n",
        "    print(f'Score for fold {fold_no}: Loss of {scores[0]}; Accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')\n",
        "\n",
        "# classifier that tries 100 different keras classifier models\n",
        "no_epochs = 500 # try 300 or 700\n",
        "batch_size = 16 # try 16\n",
        "learning_rate=1e-5 # try 1e-4\n",
        "loss = SparseCategoricalCrossentropy()\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "verbosity = 1\n",
        "num_folds = 5\n",
        "\n",
        "categorical_cross_validation2(no_epochs, batch_size, loss, optimizer, verbosity, num_folds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J3rMB1uDhmF",
        "outputId": "ad140d3e-4910-472a-c13a-8c9fe97a07a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 84 Complete [00h 00m 03s]\n",
            "val_accuracy: 0.13287514448165894\n",
            "\n",
            "Best val_accuracy So Far: 0.743413507938385\n",
            "Total elapsed time: 00h 05m 42s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.6184 - accuracy: 0.2493\n",
            "Epoch 2/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.6064 - accuracy: 0.2522\n",
            "Epoch 3/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5952 - accuracy: 0.2592\n",
            "Epoch 4/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.5787 - accuracy: 0.2615\n",
            "Epoch 5/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5554 - accuracy: 0.2712\n",
            "Epoch 6/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5569 - accuracy: 0.2748\n",
            "Epoch 7/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5372 - accuracy: 0.2843\n",
            "Epoch 8/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5279 - accuracy: 0.2800\n",
            "Epoch 9/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5341 - accuracy: 0.2793\n",
            "Epoch 10/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5075 - accuracy: 0.2913\n",
            "Epoch 11/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.5006 - accuracy: 0.2879\n",
            "Epoch 12/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4928 - accuracy: 0.2893\n",
            "Epoch 13/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4752 - accuracy: 0.3051\n",
            "Epoch 14/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4772 - accuracy: 0.2976\n",
            "Epoch 15/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4607 - accuracy: 0.3186\n",
            "Epoch 16/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4573 - accuracy: 0.3134\n",
            "Epoch 17/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4506 - accuracy: 0.3189\n",
            "Epoch 18/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4433 - accuracy: 0.3175\n",
            "Epoch 19/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4432 - accuracy: 0.3146\n",
            "Epoch 20/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4249 - accuracy: 0.3254\n",
            "Epoch 21/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4232 - accuracy: 0.3256\n",
            "Epoch 22/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4219 - accuracy: 0.3243\n",
            "Epoch 23/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.4109 - accuracy: 0.3227\n",
            "Epoch 24/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4008 - accuracy: 0.3266\n",
            "Epoch 25/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4167 - accuracy: 0.3288\n",
            "Epoch 26/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.4002 - accuracy: 0.3284\n",
            "Epoch 27/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3920 - accuracy: 0.3270\n",
            "Epoch 28/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3977 - accuracy: 0.3397\n",
            "Epoch 29/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3854 - accuracy: 0.3369\n",
            "Epoch 30/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3888 - accuracy: 0.3356\n",
            "Epoch 31/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3787 - accuracy: 0.3453\n",
            "Epoch 32/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3678 - accuracy: 0.3424\n",
            "Epoch 33/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3699 - accuracy: 0.3478\n",
            "Epoch 34/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3639 - accuracy: 0.3534\n",
            "Epoch 35/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3595 - accuracy: 0.3503\n",
            "Epoch 36/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3537 - accuracy: 0.3586\n",
            "Epoch 37/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3528 - accuracy: 0.3566\n",
            "Epoch 38/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3525 - accuracy: 0.3496\n",
            "Epoch 39/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3423 - accuracy: 0.3553\n",
            "Epoch 40/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3368 - accuracy: 0.3598\n",
            "Epoch 41/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3271 - accuracy: 0.3724\n",
            "Epoch 42/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3390 - accuracy: 0.3675\n",
            "Epoch 43/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3294 - accuracy: 0.3661\n",
            "Epoch 44/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3291 - accuracy: 0.3729\n",
            "Epoch 45/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3207 - accuracy: 0.3727\n",
            "Epoch 46/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3252 - accuracy: 0.3679\n",
            "Epoch 47/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3109 - accuracy: 0.3702\n",
            "Epoch 48/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3168 - accuracy: 0.3641\n",
            "Epoch 49/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3112 - accuracy: 0.3747\n",
            "Epoch 50/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3199 - accuracy: 0.3765\n",
            "Epoch 51/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2986 - accuracy: 0.3783\n",
            "Epoch 52/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3020 - accuracy: 0.3763\n",
            "Epoch 53/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.3055 - accuracy: 0.3751\n",
            "Epoch 54/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.3052 - accuracy: 0.3763\n",
            "Epoch 55/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.2942 - accuracy: 0.3853\n",
            "Epoch 56/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.3021 - accuracy: 0.3779\n",
            "Epoch 57/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.2989 - accuracy: 0.3831\n",
            "Epoch 58/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2907 - accuracy: 0.3894\n",
            "Epoch 59/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2905 - accuracy: 0.3833\n",
            "Epoch 60/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2815 - accuracy: 0.3855\n",
            "Epoch 61/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2832 - accuracy: 0.3912\n",
            "Epoch 62/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2825 - accuracy: 0.3919\n",
            "Epoch 63/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2731 - accuracy: 0.3941\n",
            "Epoch 64/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2800 - accuracy: 0.3840\n",
            "Epoch 65/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2897 - accuracy: 0.3892\n",
            "Epoch 66/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2782 - accuracy: 0.3880\n",
            "Epoch 67/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2702 - accuracy: 0.3948\n",
            "Epoch 68/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2758 - accuracy: 0.3894\n",
            "Epoch 69/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2709 - accuracy: 0.4070\n",
            "Epoch 70/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2725 - accuracy: 0.3858\n",
            "Epoch 71/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2626 - accuracy: 0.4041\n",
            "Epoch 72/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2625 - accuracy: 0.3937\n",
            "Epoch 73/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2667 - accuracy: 0.3971\n",
            "Epoch 74/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2663 - accuracy: 0.3901\n",
            "Epoch 75/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2703 - accuracy: 0.3923\n",
            "Epoch 76/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2651 - accuracy: 0.3948\n",
            "Epoch 77/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2602 - accuracy: 0.3998\n",
            "Epoch 78/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2579 - accuracy: 0.4009\n",
            "Epoch 79/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2604 - accuracy: 0.3964\n",
            "Epoch 80/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2578 - accuracy: 0.3982\n",
            "Epoch 81/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2541 - accuracy: 0.4007\n",
            "Epoch 82/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2433 - accuracy: 0.4079\n",
            "Epoch 83/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2439 - accuracy: 0.4160\n",
            "Epoch 84/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2561 - accuracy: 0.3934\n",
            "Epoch 85/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2553 - accuracy: 0.3946\n",
            "Epoch 86/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2485 - accuracy: 0.4027\n",
            "Epoch 87/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2464 - accuracy: 0.4038\n",
            "Epoch 88/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2455 - accuracy: 0.4151\n",
            "Epoch 89/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2552 - accuracy: 0.3934\n",
            "Epoch 90/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2382 - accuracy: 0.4140\n",
            "Epoch 91/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2425 - accuracy: 0.4084\n",
            "Epoch 92/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2421 - accuracy: 0.4077\n",
            "Epoch 93/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2395 - accuracy: 0.4113\n",
            "Epoch 94/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2377 - accuracy: 0.4160\n",
            "Epoch 95/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2326 - accuracy: 0.4138\n",
            "Epoch 96/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2348 - accuracy: 0.4142\n",
            "Epoch 97/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2290 - accuracy: 0.4131\n",
            "Epoch 98/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2317 - accuracy: 0.4203\n",
            "Epoch 99/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2321 - accuracy: 0.4174\n",
            "Epoch 100/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2341 - accuracy: 0.4149\n",
            "Epoch 101/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2311 - accuracy: 0.4219\n",
            "Epoch 102/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2261 - accuracy: 0.4188\n",
            "Epoch 103/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2408 - accuracy: 0.4054\n",
            "Epoch 104/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2276 - accuracy: 0.4217\n",
            "Epoch 105/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2251 - accuracy: 0.4249\n",
            "Epoch 106/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2260 - accuracy: 0.4210\n",
            "Epoch 107/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2312 - accuracy: 0.4206\n",
            "Epoch 108/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2130 - accuracy: 0.4375\n",
            "Epoch 109/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2156 - accuracy: 0.4208\n",
            "Epoch 110/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2247 - accuracy: 0.4154\n",
            "Epoch 111/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2121 - accuracy: 0.4285\n",
            "Epoch 112/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2205 - accuracy: 0.4142\n",
            "Epoch 113/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2211 - accuracy: 0.4221\n",
            "Epoch 114/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2182 - accuracy: 0.4231\n",
            "Epoch 115/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2244 - accuracy: 0.4118\n",
            "Epoch 116/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2198 - accuracy: 0.4221\n",
            "Epoch 117/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2149 - accuracy: 0.4172\n",
            "Epoch 118/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2138 - accuracy: 0.4224\n",
            "Epoch 119/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2035 - accuracy: 0.4357\n",
            "Epoch 120/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2058 - accuracy: 0.4258\n",
            "Epoch 121/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2138 - accuracy: 0.4212\n",
            "Epoch 122/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2099 - accuracy: 0.4260\n",
            "Epoch 123/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2092 - accuracy: 0.4321\n",
            "Epoch 124/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2059 - accuracy: 0.4316\n",
            "Epoch 125/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2027 - accuracy: 0.4285\n",
            "Epoch 126/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2132 - accuracy: 0.4357\n",
            "Epoch 127/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1999 - accuracy: 0.4269\n",
            "Epoch 128/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2048 - accuracy: 0.4353\n",
            "Epoch 129/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2038 - accuracy: 0.4169\n",
            "Epoch 130/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2005 - accuracy: 0.4292\n",
            "Epoch 131/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2052 - accuracy: 0.4371\n",
            "Epoch 132/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2027 - accuracy: 0.4339\n",
            "Epoch 133/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1976 - accuracy: 0.4373\n",
            "Epoch 134/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.2019 - accuracy: 0.4384\n",
            "Epoch 135/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2036 - accuracy: 0.4346\n",
            "Epoch 136/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1948 - accuracy: 0.4357\n",
            "Epoch 137/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.2009 - accuracy: 0.4359\n",
            "Epoch 138/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1974 - accuracy: 0.4262\n",
            "Epoch 139/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1920 - accuracy: 0.4441\n",
            "Epoch 140/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1972 - accuracy: 0.4323\n",
            "Epoch 141/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1959 - accuracy: 0.4386\n",
            "Epoch 142/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1921 - accuracy: 0.4479\n",
            "Epoch 143/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1954 - accuracy: 0.4398\n",
            "Epoch 144/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1884 - accuracy: 0.4447\n",
            "Epoch 145/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1993 - accuracy: 0.4217\n",
            "Epoch 146/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1892 - accuracy: 0.4377\n",
            "Epoch 147/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1878 - accuracy: 0.4380\n",
            "Epoch 148/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1906 - accuracy: 0.4416\n",
            "Epoch 149/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1892 - accuracy: 0.4400\n",
            "Epoch 150/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1927 - accuracy: 0.4359\n",
            "Epoch 151/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1881 - accuracy: 0.4524\n",
            "Epoch 152/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1887 - accuracy: 0.4319\n",
            "Epoch 153/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1916 - accuracy: 0.4398\n",
            "Epoch 154/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1933 - accuracy: 0.4411\n",
            "Epoch 155/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1909 - accuracy: 0.4362\n",
            "Epoch 156/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1851 - accuracy: 0.4423\n",
            "Epoch 157/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1863 - accuracy: 0.4418\n",
            "Epoch 158/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1870 - accuracy: 0.4348\n",
            "Epoch 159/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1875 - accuracy: 0.4409\n",
            "Epoch 160/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1809 - accuracy: 0.4362\n",
            "Epoch 161/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1781 - accuracy: 0.4527\n",
            "Epoch 162/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1803 - accuracy: 0.4441\n",
            "Epoch 163/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1825 - accuracy: 0.4434\n",
            "Epoch 164/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1887 - accuracy: 0.4456\n",
            "Epoch 165/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1778 - accuracy: 0.4393\n",
            "Epoch 166/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1771 - accuracy: 0.4405\n",
            "Epoch 167/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1794 - accuracy: 0.4409\n",
            "Epoch 168/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1792 - accuracy: 0.4337\n",
            "Epoch 169/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1788 - accuracy: 0.4443\n",
            "Epoch 170/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1808 - accuracy: 0.4461\n",
            "Epoch 171/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1819 - accuracy: 0.4502\n",
            "Epoch 172/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1703 - accuracy: 0.4524\n",
            "Epoch 173/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1733 - accuracy: 0.4522\n",
            "Epoch 174/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1766 - accuracy: 0.4518\n",
            "Epoch 175/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1795 - accuracy: 0.4418\n",
            "Epoch 176/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1732 - accuracy: 0.4583\n",
            "Epoch 177/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1796 - accuracy: 0.4445\n",
            "Epoch 178/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1830 - accuracy: 0.4395\n",
            "Epoch 179/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1774 - accuracy: 0.4411\n",
            "Epoch 180/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1671 - accuracy: 0.4601\n",
            "Epoch 181/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1742 - accuracy: 0.4545\n",
            "Epoch 182/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1713 - accuracy: 0.4488\n",
            "Epoch 183/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1690 - accuracy: 0.4642\n",
            "Epoch 184/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1813 - accuracy: 0.4391\n",
            "Epoch 185/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1742 - accuracy: 0.4493\n",
            "Epoch 186/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1744 - accuracy: 0.4549\n",
            "Epoch 187/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1728 - accuracy: 0.4536\n",
            "Epoch 188/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1705 - accuracy: 0.4558\n",
            "Epoch 189/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1689 - accuracy: 0.4533\n",
            "Epoch 190/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1712 - accuracy: 0.4524\n",
            "Epoch 191/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1722 - accuracy: 0.4438\n",
            "Epoch 192/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1662 - accuracy: 0.4560\n",
            "Epoch 193/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1670 - accuracy: 0.4518\n",
            "Epoch 194/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1666 - accuracy: 0.4504\n",
            "Epoch 195/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1584 - accuracy: 0.4536\n",
            "Epoch 196/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1630 - accuracy: 0.4558\n",
            "Epoch 197/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1686 - accuracy: 0.4450\n",
            "Epoch 198/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1696 - accuracy: 0.4633\n",
            "Epoch 199/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1650 - accuracy: 0.4522\n",
            "Epoch 200/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1629 - accuracy: 0.4565\n",
            "Epoch 201/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1634 - accuracy: 0.4486\n",
            "Epoch 202/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1587 - accuracy: 0.4633\n",
            "Epoch 203/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1661 - accuracy: 0.4554\n",
            "Epoch 204/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1618 - accuracy: 0.4545\n",
            "Epoch 205/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1562 - accuracy: 0.4737\n",
            "Epoch 206/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1608 - accuracy: 0.4612\n",
            "Epoch 207/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1561 - accuracy: 0.4610\n",
            "Epoch 208/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1507 - accuracy: 0.4642\n",
            "Epoch 209/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1592 - accuracy: 0.4547\n",
            "Epoch 210/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1646 - accuracy: 0.4472\n",
            "Epoch 211/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1581 - accuracy: 0.4610\n",
            "Epoch 212/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1526 - accuracy: 0.4615\n",
            "Epoch 213/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1554 - accuracy: 0.4576\n",
            "Epoch 214/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1594 - accuracy: 0.4635\n",
            "Epoch 215/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1591 - accuracy: 0.4583\n",
            "Epoch 216/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1639 - accuracy: 0.4599\n",
            "Epoch 217/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1493 - accuracy: 0.4626\n",
            "Epoch 218/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1532 - accuracy: 0.4597\n",
            "Epoch 219/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1571 - accuracy: 0.4569\n",
            "Epoch 220/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1454 - accuracy: 0.4685\n",
            "Epoch 221/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1608 - accuracy: 0.4551\n",
            "Epoch 222/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1533 - accuracy: 0.4597\n",
            "Epoch 223/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1514 - accuracy: 0.4633\n",
            "Epoch 224/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1635 - accuracy: 0.4536\n",
            "Epoch 225/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1561 - accuracy: 0.4572\n",
            "Epoch 226/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1496 - accuracy: 0.4624\n",
            "Epoch 227/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1521 - accuracy: 0.4628\n",
            "Epoch 228/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1476 - accuracy: 0.4637\n",
            "Epoch 229/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1530 - accuracy: 0.4606\n",
            "Epoch 230/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1547 - accuracy: 0.4642\n",
            "Epoch 231/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1437 - accuracy: 0.4631\n",
            "Epoch 232/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1502 - accuracy: 0.4554\n",
            "Epoch 233/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1578 - accuracy: 0.4567\n",
            "Epoch 234/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1394 - accuracy: 0.4746\n",
            "Epoch 235/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1496 - accuracy: 0.4606\n",
            "Epoch 236/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1430 - accuracy: 0.4748\n",
            "Epoch 237/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1436 - accuracy: 0.4680\n",
            "Epoch 238/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1500 - accuracy: 0.4631\n",
            "Epoch 239/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1406 - accuracy: 0.4741\n",
            "Epoch 240/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1435 - accuracy: 0.4673\n",
            "Epoch 241/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1538 - accuracy: 0.4592\n",
            "Epoch 242/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1416 - accuracy: 0.4685\n",
            "Epoch 243/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1455 - accuracy: 0.4678\n",
            "Epoch 244/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1537 - accuracy: 0.4549\n",
            "Epoch 245/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1391 - accuracy: 0.4755\n",
            "Epoch 246/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1430 - accuracy: 0.4669\n",
            "Epoch 247/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1453 - accuracy: 0.4712\n",
            "Epoch 248/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1429 - accuracy: 0.4671\n",
            "Epoch 249/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1487 - accuracy: 0.4529\n",
            "Epoch 250/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1443 - accuracy: 0.4617\n",
            "Epoch 251/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1464 - accuracy: 0.4615\n",
            "Epoch 252/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1494 - accuracy: 0.4569\n",
            "Epoch 253/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1410 - accuracy: 0.4703\n",
            "Epoch 254/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1435 - accuracy: 0.4714\n",
            "Epoch 255/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1405 - accuracy: 0.4610\n",
            "Epoch 256/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1394 - accuracy: 0.4721\n",
            "Epoch 257/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1370 - accuracy: 0.4739\n",
            "Epoch 258/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1446 - accuracy: 0.4673\n",
            "Epoch 259/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1376 - accuracy: 0.4716\n",
            "Epoch 260/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1472 - accuracy: 0.4612\n",
            "Epoch 261/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1350 - accuracy: 0.4689\n",
            "Epoch 262/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1402 - accuracy: 0.4678\n",
            "Epoch 263/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1412 - accuracy: 0.4664\n",
            "Epoch 264/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1295 - accuracy: 0.4798\n",
            "Epoch 265/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1400 - accuracy: 0.4628\n",
            "Epoch 266/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1443 - accuracy: 0.4705\n",
            "Epoch 267/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1387 - accuracy: 0.4757\n",
            "Epoch 268/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1413 - accuracy: 0.4701\n",
            "Epoch 269/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1338 - accuracy: 0.4768\n",
            "Epoch 270/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1369 - accuracy: 0.4698\n",
            "Epoch 271/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1303 - accuracy: 0.4802\n",
            "Epoch 272/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1320 - accuracy: 0.4716\n",
            "Epoch 273/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1366 - accuracy: 0.4696\n",
            "Epoch 274/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1376 - accuracy: 0.4696\n",
            "Epoch 275/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1325 - accuracy: 0.4728\n",
            "Epoch 276/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1336 - accuracy: 0.4737\n",
            "Epoch 277/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1337 - accuracy: 0.4725\n",
            "Epoch 278/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1332 - accuracy: 0.4766\n",
            "Epoch 279/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1337 - accuracy: 0.4685\n",
            "Epoch 280/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1402 - accuracy: 0.4653\n",
            "Epoch 281/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1388 - accuracy: 0.4646\n",
            "Epoch 282/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1345 - accuracy: 0.4644\n",
            "Epoch 283/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1375 - accuracy: 0.4644\n",
            "Epoch 284/500\n",
            "277/277 [==============================] - 0s 2ms/step - loss: 1.1423 - accuracy: 0.4662\n",
            "Epoch 285/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1371 - accuracy: 0.4664\n",
            "Epoch 286/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1407 - accuracy: 0.4667\n",
            "Epoch 287/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1290 - accuracy: 0.4782\n",
            "Epoch 288/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1341 - accuracy: 0.4759\n",
            "Epoch 289/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1327 - accuracy: 0.4676\n",
            "Epoch 290/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1329 - accuracy: 0.4698\n",
            "Epoch 291/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1311 - accuracy: 0.4716\n",
            "Epoch 292/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1312 - accuracy: 0.4802\n",
            "Epoch 293/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1353 - accuracy: 0.4775\n",
            "Epoch 294/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1281 - accuracy: 0.4777\n",
            "Epoch 295/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1265 - accuracy: 0.4750\n",
            "Epoch 296/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1357 - accuracy: 0.4716\n",
            "Epoch 297/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1344 - accuracy: 0.4678\n",
            "Epoch 298/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1280 - accuracy: 0.4755\n",
            "Epoch 299/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1335 - accuracy: 0.4671\n",
            "Epoch 300/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1228 - accuracy: 0.4714\n",
            "Epoch 301/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1274 - accuracy: 0.4805\n",
            "Epoch 302/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1279 - accuracy: 0.4732\n",
            "Epoch 303/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1289 - accuracy: 0.4728\n",
            "Epoch 304/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1341 - accuracy: 0.4814\n",
            "Epoch 305/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1318 - accuracy: 0.4730\n",
            "Epoch 306/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1214 - accuracy: 0.4768\n",
            "Epoch 307/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1312 - accuracy: 0.4689\n",
            "Epoch 308/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1356 - accuracy: 0.4728\n",
            "Epoch 309/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1260 - accuracy: 0.4773\n",
            "Epoch 310/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1284 - accuracy: 0.4798\n",
            "Epoch 311/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1238 - accuracy: 0.4782\n",
            "Epoch 312/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1264 - accuracy: 0.4694\n",
            "Epoch 313/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1292 - accuracy: 0.4716\n",
            "Epoch 314/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1203 - accuracy: 0.4748\n",
            "Epoch 315/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1308 - accuracy: 0.4798\n",
            "Epoch 316/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1201 - accuracy: 0.4899\n",
            "Epoch 317/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1201 - accuracy: 0.4814\n",
            "Epoch 318/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1329 - accuracy: 0.4728\n",
            "Epoch 319/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1322 - accuracy: 0.4701\n",
            "Epoch 320/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1277 - accuracy: 0.4721\n",
            "Epoch 321/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1229 - accuracy: 0.4791\n",
            "Epoch 322/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1269 - accuracy: 0.4777\n",
            "Epoch 323/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1247 - accuracy: 0.4784\n",
            "Epoch 324/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1196 - accuracy: 0.4807\n",
            "Epoch 325/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1323 - accuracy: 0.4680\n",
            "Epoch 326/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1281 - accuracy: 0.4798\n",
            "Epoch 327/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1235 - accuracy: 0.4832\n",
            "Epoch 328/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1234 - accuracy: 0.4847\n",
            "Epoch 329/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1216 - accuracy: 0.4653\n",
            "Epoch 330/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1157 - accuracy: 0.4847\n",
            "Epoch 331/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1204 - accuracy: 0.4838\n",
            "Epoch 332/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1211 - accuracy: 0.4811\n",
            "Epoch 333/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1154 - accuracy: 0.4850\n",
            "Epoch 334/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1170 - accuracy: 0.4829\n",
            "Epoch 335/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1218 - accuracy: 0.4730\n",
            "Epoch 336/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1244 - accuracy: 0.4705\n",
            "Epoch 337/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1241 - accuracy: 0.4753\n",
            "Epoch 338/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1296 - accuracy: 0.4660\n",
            "Epoch 339/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1240 - accuracy: 0.4703\n",
            "Epoch 340/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1252 - accuracy: 0.4696\n",
            "Epoch 341/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1221 - accuracy: 0.4782\n",
            "Epoch 342/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1184 - accuracy: 0.4888\n",
            "Epoch 343/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.1259 - accuracy: 0.4710\n",
            "Epoch 344/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.1161 - accuracy: 0.4816\n",
            "Epoch 345/500\n",
            "277/277 [==============================] - 1s 3ms/step - loss: 1.1178 - accuracy: 0.4775\n",
            "Epoch 346/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1202 - accuracy: 0.4782\n",
            "Epoch 347/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1166 - accuracy: 0.4931\n",
            "Epoch 348/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1154 - accuracy: 0.4811\n",
            "Epoch 349/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1132 - accuracy: 0.4802\n",
            "Epoch 350/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1263 - accuracy: 0.4789\n",
            "Epoch 351/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1201 - accuracy: 0.4768\n",
            "Epoch 352/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1160 - accuracy: 0.4730\n",
            "Epoch 353/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1167 - accuracy: 0.4854\n",
            "Epoch 354/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1191 - accuracy: 0.4744\n",
            "Epoch 355/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1202 - accuracy: 0.4807\n",
            "Epoch 356/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1172 - accuracy: 0.4750\n",
            "Epoch 357/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1224 - accuracy: 0.4734\n",
            "Epoch 358/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1216 - accuracy: 0.4811\n",
            "Epoch 359/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1123 - accuracy: 0.4818\n",
            "Epoch 360/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1231 - accuracy: 0.4739\n",
            "Epoch 361/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1132 - accuracy: 0.4823\n",
            "Epoch 362/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1182 - accuracy: 0.4884\n",
            "Epoch 363/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1203 - accuracy: 0.4793\n",
            "Epoch 364/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1156 - accuracy: 0.4841\n",
            "Epoch 365/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1148 - accuracy: 0.4818\n",
            "Epoch 366/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1152 - accuracy: 0.4820\n",
            "Epoch 367/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1200 - accuracy: 0.4744\n",
            "Epoch 368/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1119 - accuracy: 0.4850\n",
            "Epoch 369/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1098 - accuracy: 0.4877\n",
            "Epoch 370/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1155 - accuracy: 0.4841\n",
            "Epoch 371/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1126 - accuracy: 0.4841\n",
            "Epoch 372/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1144 - accuracy: 0.4832\n",
            "Epoch 373/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1176 - accuracy: 0.4798\n",
            "Epoch 374/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1184 - accuracy: 0.4872\n",
            "Epoch 375/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1169 - accuracy: 0.4768\n",
            "Epoch 376/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1164 - accuracy: 0.4845\n",
            "Epoch 377/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1122 - accuracy: 0.4782\n",
            "Epoch 378/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1079 - accuracy: 0.4816\n",
            "Epoch 379/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1115 - accuracy: 0.4899\n",
            "Epoch 380/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1135 - accuracy: 0.4856\n",
            "Epoch 381/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1123 - accuracy: 0.4834\n",
            "Epoch 382/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1151 - accuracy: 0.4841\n",
            "Epoch 383/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1133 - accuracy: 0.4838\n",
            "Epoch 384/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1179 - accuracy: 0.4827\n",
            "Epoch 385/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1111 - accuracy: 0.4863\n",
            "Epoch 386/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1139 - accuracy: 0.4863\n",
            "Epoch 387/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1153 - accuracy: 0.4786\n",
            "Epoch 388/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1028 - accuracy: 0.4929\n",
            "Epoch 389/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1034 - accuracy: 0.4938\n",
            "Epoch 390/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1161 - accuracy: 0.4816\n",
            "Epoch 391/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1097 - accuracy: 0.4843\n",
            "Epoch 392/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1099 - accuracy: 0.4856\n",
            "Epoch 393/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1152 - accuracy: 0.4809\n",
            "Epoch 394/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1114 - accuracy: 0.4768\n",
            "Epoch 395/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1091 - accuracy: 0.4829\n",
            "Epoch 396/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1112 - accuracy: 0.4906\n",
            "Epoch 397/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1013 - accuracy: 0.4841\n",
            "Epoch 398/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1153 - accuracy: 0.4728\n",
            "Epoch 399/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1149 - accuracy: 0.4786\n",
            "Epoch 400/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1035 - accuracy: 0.4933\n",
            "Epoch 401/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1089 - accuracy: 0.4983\n",
            "Epoch 402/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1098 - accuracy: 0.4863\n",
            "Epoch 403/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1122 - accuracy: 0.4838\n",
            "Epoch 404/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1008 - accuracy: 0.4823\n",
            "Epoch 405/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1084 - accuracy: 0.4847\n",
            "Epoch 406/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1112 - accuracy: 0.4836\n",
            "Epoch 407/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1031 - accuracy: 0.4911\n",
            "Epoch 408/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1023 - accuracy: 0.4829\n",
            "Epoch 409/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1041 - accuracy: 0.4927\n",
            "Epoch 410/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0990 - accuracy: 0.4960\n",
            "Epoch 411/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1081 - accuracy: 0.4890\n",
            "Epoch 412/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1064 - accuracy: 0.4872\n",
            "Epoch 413/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1122 - accuracy: 0.4795\n",
            "Epoch 414/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1101 - accuracy: 0.4868\n",
            "Epoch 415/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1058 - accuracy: 0.4976\n",
            "Epoch 416/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1079 - accuracy: 0.4881\n",
            "Epoch 417/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1083 - accuracy: 0.4757\n",
            "Epoch 418/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1097 - accuracy: 0.4861\n",
            "Epoch 419/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1066 - accuracy: 0.4940\n",
            "Epoch 420/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1051 - accuracy: 0.4895\n",
            "Epoch 421/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1051 - accuracy: 0.4841\n",
            "Epoch 422/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0993 - accuracy: 0.4915\n",
            "Epoch 423/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1162 - accuracy: 0.4786\n",
            "Epoch 424/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1038 - accuracy: 0.4890\n",
            "Epoch 425/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1045 - accuracy: 0.4927\n",
            "Epoch 426/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1083 - accuracy: 0.4938\n",
            "Epoch 427/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1019 - accuracy: 0.4911\n",
            "Epoch 428/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1045 - accuracy: 0.4825\n",
            "Epoch 429/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0990 - accuracy: 0.4936\n",
            "Epoch 430/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1053 - accuracy: 0.4899\n",
            "Epoch 431/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1073 - accuracy: 0.4888\n",
            "Epoch 432/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1034 - accuracy: 0.4927\n",
            "Epoch 433/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0988 - accuracy: 0.4940\n",
            "Epoch 434/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0941 - accuracy: 0.4940\n",
            "Epoch 435/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1061 - accuracy: 0.4938\n",
            "Epoch 436/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0994 - accuracy: 0.4875\n",
            "Epoch 437/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0960 - accuracy: 0.4870\n",
            "Epoch 438/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1054 - accuracy: 0.4886\n",
            "Epoch 439/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0984 - accuracy: 0.4918\n",
            "Epoch 440/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0977 - accuracy: 0.5003\n",
            "Epoch 441/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1103 - accuracy: 0.4814\n",
            "Epoch 442/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1041 - accuracy: 0.4884\n",
            "Epoch 443/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0988 - accuracy: 0.4897\n",
            "Epoch 444/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1013 - accuracy: 0.4936\n",
            "Epoch 445/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1004 - accuracy: 0.4927\n",
            "Epoch 446/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0972 - accuracy: 0.4976\n",
            "Epoch 447/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1015 - accuracy: 0.4852\n",
            "Epoch 448/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0964 - accuracy: 0.4967\n",
            "Epoch 449/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0989 - accuracy: 0.4933\n",
            "Epoch 450/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1091 - accuracy: 0.4899\n",
            "Epoch 451/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0964 - accuracy: 0.4983\n",
            "Epoch 452/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1018 - accuracy: 0.4940\n",
            "Epoch 453/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1078 - accuracy: 0.4829\n",
            "Epoch 454/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0977 - accuracy: 0.4985\n",
            "Epoch 455/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0960 - accuracy: 0.4958\n",
            "Epoch 456/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1011 - accuracy: 0.4963\n",
            "Epoch 457/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1103 - accuracy: 0.4791\n",
            "Epoch 458/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1061 - accuracy: 0.4834\n",
            "Epoch 459/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0934 - accuracy: 0.5062\n",
            "Epoch 460/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0890 - accuracy: 0.4951\n",
            "Epoch 461/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0988 - accuracy: 0.4890\n",
            "Epoch 462/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0936 - accuracy: 0.4945\n",
            "Epoch 463/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1012 - accuracy: 0.4863\n",
            "Epoch 464/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0975 - accuracy: 0.4963\n",
            "Epoch 465/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0995 - accuracy: 0.4929\n",
            "Epoch 466/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0958 - accuracy: 0.4933\n",
            "Epoch 467/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0970 - accuracy: 0.4958\n",
            "Epoch 468/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0901 - accuracy: 0.4954\n",
            "Epoch 469/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0965 - accuracy: 0.5012\n",
            "Epoch 470/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0984 - accuracy: 0.4974\n",
            "Epoch 471/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1027 - accuracy: 0.4927\n",
            "Epoch 472/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1073 - accuracy: 0.4893\n",
            "Epoch 473/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.1009 - accuracy: 0.4985\n",
            "Epoch 474/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0988 - accuracy: 0.4949\n",
            "Epoch 475/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0926 - accuracy: 0.4954\n",
            "Epoch 476/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0955 - accuracy: 0.4981\n",
            "Epoch 477/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0923 - accuracy: 0.4983\n",
            "Epoch 478/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0909 - accuracy: 0.4936\n",
            "Epoch 479/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0954 - accuracy: 0.4904\n",
            "Epoch 480/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0965 - accuracy: 0.4938\n",
            "Epoch 481/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0976 - accuracy: 0.4924\n",
            "Epoch 482/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0951 - accuracy: 0.4940\n",
            "Epoch 483/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0913 - accuracy: 0.4963\n",
            "Epoch 484/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0891 - accuracy: 0.4994\n",
            "Epoch 485/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0974 - accuracy: 0.4940\n",
            "Epoch 486/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0913 - accuracy: 0.4949\n",
            "Epoch 487/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0984 - accuracy: 0.4888\n",
            "Epoch 488/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0941 - accuracy: 0.4958\n",
            "Epoch 489/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0956 - accuracy: 0.4979\n",
            "Epoch 490/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0957 - accuracy: 0.4908\n",
            "Epoch 491/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0881 - accuracy: 0.4994\n",
            "Epoch 492/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0977 - accuracy: 0.4983\n",
            "Epoch 493/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0925 - accuracy: 0.4938\n",
            "Epoch 494/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0944 - accuracy: 0.5010\n",
            "Epoch 495/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0974 - accuracy: 0.4999\n",
            "Epoch 496/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0944 - accuracy: 0.4908\n",
            "Epoch 497/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0964 - accuracy: 0.4997\n",
            "Epoch 498/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0912 - accuracy: 0.4951\n",
            "Epoch 499/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0989 - accuracy: 0.4947\n",
            "Epoch 500/500\n",
            "277/277 [==============================] - 1s 2ms/step - loss: 1.0918 - accuracy: 0.5003\n",
            "INFO:tensorflow:Assets written to: best_regression_keras_model_fold_5/structured_data_classifier/best_model/assets\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 24)]              0         \n",
            "                                                                 \n",
            " multi_category_encoding (Mu  (None, 24)               0         \n",
            " ltiCategoryEncoding)                                            \n",
            "                                                                 \n",
            " normalization (Normalizatio  (None, 24)               49        \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                400       \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 16)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            " classification_head_1 (Soft  (None, 4)                0         \n",
            " max)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 517\n",
            "Trainable params: 468\n",
            "Non-trainable params: 49\n",
            "_________________________________________________________________\n",
            "None\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 1.0476 - accuracy: 0.5371\n",
            "Score for fold 5: Loss of 1.0476468801498413; Accuracy of 53.707051277160645%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "> Fold 1 - Loss: 1.0572731494903564 - Accuracy: 52.30352282524109%\n",
            "> Fold 2 - Loss: 1.0707553625106812 - Accuracy: 55.33453822135925%\n",
            "> Fold 3 - Loss: 1.545008897781372 - Accuracy: 28.119349479675293%\n",
            "> Fold 4 - Loss: 1.5401153564453125 - Accuracy: 26.582279801368713%\n",
            "> Fold 5 - Loss: 1.0476468801498413 - Accuracy: 53.707051277160645%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 43.209348320961 (+- 12.993024897819918)\n",
            "> Loss: 1.2521599292755128\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_validation3(batch_size, verbosity, num_folds):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
        "  acc_per_fold = []\n",
        "  loss_per_fold = []\n",
        "  fold_no = 1\n",
        "  \n",
        "  X = df_soa.iloc[:,8:]\n",
        "  y = df_soa.iloc[:,4:5]\n",
        "\n",
        "  for train, test in kfold.split(X, y):\n",
        "    model = ak.StructuredDataClassifier(\n",
        "        overwrite=True,\n",
        "        tuner=\"bayesian\",\n",
        "        max_trials=100,\n",
        "        seed=1,\n",
        "        directory=f\"best_regression_keras_model_3_fold_{fold_no}\"\n",
        "    )\n",
        "    \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(X_test, y_test),\n",
        "                verbose=verbosity,\n",
        "                callbacks=[EarlyStopping()],\n",
        "                workers=8)\n",
        "  \n",
        "    print(model.export_model().summary())\n",
        "\n",
        "\n",
        "    scores = model.evaluate(X_test, y_test, verbose=verbosity)\n",
        "\n",
        "    print(f'Score for fold {fold_no}: Loss of {scores[0]}; Accuracy of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(acc_per_fold)):\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "  print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "  print('------------------------------------------------------------------------')\n",
        "\n",
        "batch_size = 16 # try 32, 64\n",
        "verbosity = 0\n",
        "num_folds = 5\n",
        "\n",
        "categorical_cross_validation3(batch_size, verbosity, num_folds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UNzbzkHy9p36",
        "outputId": "80a5c65d-9fad-4496-f888-a794cfc3f65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "INFO:tensorflow:Assets written to: best_regression_keras_model_3_fold_1/structured_data_classifier/best_model/assets\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 24)]              0         \n",
            "                                                                 \n",
            " multi_category_encoding (Mu  (None, 24)               0         \n",
            " ltiCategoryEncoding)                                            \n",
            "                                                                 \n",
            " normalization (Normalizatio  (None, 24)               49        \n",
            " n)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25600     \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 4100      \n",
            "                                                                 \n",
            " classification_head_1 (Soft  (None, 4)                0         \n",
            " max)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,749\n",
            "Trainable params: 29,700\n",
            "Non-trainable params: 49\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-eb8f05f599dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mnum_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mcategorical_cross_validation3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-eb8f05f599dd>\u001b[0m in \u001b[0;36mcategorical_cross_validation3\u001b[0;34m(batch_size, verbosity, num_folds)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Score for fold {fold_no}: Loss of {scores[0]}; Accuracy of {scores[1]*100}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autokeras/tasks/structured_data.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         return utils.evaluate_with_adaptive_batch_size(\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36mevaluate_with_adaptive_batch_size\u001b[0;34m(model, batch_size, verbose, **fit_kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ),\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36mrun_with_adaptive_batch_size\u001b[0;34m(batch_size, func, **fit_kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceExhaustedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         lambda x, validation_data, **kwargs: model.evaluate(\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         ),\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node 'model/multi_category_encoding/string_lookup_2/None_Lookup/LookupTableFindV2' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 577, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 606, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 556, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-35-eb8f05f599dd>\", line 57, in <module>\n      categorical_cross_validation3(batch_size, verbosity, num_folds)\n    File \"<ipython-input-35-eb8f05f599dd>\", line 35, in categorical_cross_validation3\n      scores = model.evaluate(X_test, y_test, verbose=verbosity)\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/tasks/structured_data.py\", line 187, in evaluate\n      return super().evaluate(x=x, y=y, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/auto_model.py\", line 493, in evaluate\n      model=model, batch_size=batch_size, x=dataset, verbose=verbose, **kwargs\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\", line 73, in evaluate_with_adaptive_batch_size\n      **fit_kwargs,\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\", line 101, in run_with_adaptive_batch_size\n      history = func(x=x, validation_data=validation_data, **fit_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/utils/utils.py\", line 71, in <lambda>\n      x, verbose=verbose, **kwargs\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1716, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1525, in test_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1514, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1507, in run_step\n      outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1471, in test_step\n      y_pred = self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/keras_layers.py\", line 99, in call\n      for input_node, encoding_layer in zip(split_inputs, self.encoding_layers):\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/keras_layers.py\", line 100, in call\n      if encoding_layer is None:\n    File \"/usr/local/lib/python3.7/dist-packages/autokeras/keras_layers.py\", line 108, in call\n      output_nodes.append(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/preprocessing/index_lookup.py\", line 628, in call\n      lookups = self._lookup_dense(inputs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/preprocessing/index_lookup.py\", line 657, in _lookup_dense\n      lookups = self.lookup_table.lookup(inputs)\nNode: 'model/multi_category_encoding/string_lookup_2/None_Lookup/LookupTableFindV2'\nTable not initialized.\n\t [[{{node model/multi_category_encoding/string_lookup_2/None_Lookup/LookupTableFindV2}}]] [Op:__inference_test_function_17395825]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Copy of 2_models_study.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
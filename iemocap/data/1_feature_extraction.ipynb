{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# go to upper diretory\n",
    "sys.path.append(os.path.abspath('./../../../'))\n",
    "import csv\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from Audio_Sentiment_Analysis.utils.Configuration import Configuration\n",
    "\n",
    "AUDIO_DIR = f\"{os.path.abspath('./../../../')}/IEMOCAP_Dataset\"\n",
    "EXTRACTED_FEATURES_FILE = 'extracted_features_iemocap.csv'\n",
    "RAW_AUDIO_FILES = 'raw_audio_files.csv'\n",
    "\n",
    "CONFIG_FILE = f\"{os.path.abspath('./../../../')}/Audio_Sentiment_Analysis/iemocap/config.json\"\n",
    "config = Configuration.load_json(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the 24 features selected from the eNTERFACE05 dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = {'var_mfcc15', 'min_mfcc17', 'std_chroma_stft', 'mean_zcr',\n",
    "    'min_zcr', 'var_mfcc4', 'spikes_spec_cent', 'var_spec_bw', 'max_mfcc6',\n",
    "    'min_mfcc19', 'max_mfcc13', 'max_mfcc5', 'var_mfcc9', 'min_mfcc7', 'var_mfcc1',\n",
    "    'max_mfcc1', 'max_mfcc9', 'var_mel_spect', 'mean_spec_cont', 'var_mfcc2', 'max_mfcc10',\n",
    "    'min_spec_cent', 'var_mfcc14', 'var_mfcc3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance',\n",
    "    'std_chroma_stft', 'mean_zcr', 'min_zcr',\n",
    "    'var_mel_spect', 'spikes_spec_cent', 'min_spec_cent',\n",
    "    'var_spec_bw', 'mean_spec_cont', \n",
    "    'var_mfcc1', 'max_mfcc1', 'var_mfcc2', 'var_mfcc3', 'var_mfcc4',\n",
    "    'max_mfcc5', 'max_mfcc6', 'min_mfcc7', 'var_mfcc9', 'max_mfcc9',\n",
    "    'max_mfcc10', 'max_mfcc13', 'var_mfcc14', 'var_mfcc15', \n",
    "    'min_mfcc17', 'min_mfcc19'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_raw = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spikes(data):\n",
    "    if len(data.shape) != 1:\n",
    "        data = np.concatenate(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    threshold = mean + std * 2 / 100\n",
    "    num_spikes = 0\n",
    "    for value in data:\n",
    "        if value >= threshold:\n",
    "            num_spikes += 1\n",
    "\n",
    "    return num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label = {\n",
    "    'ang': \"angry\",\n",
    "    'hap': \"happy\",\n",
    "    'sad': \"sad\",\n",
    "    'neu': \"neutral\",\n",
    "    'fru': \"frustrated\",\n",
    "    'exc': \"excited\",\n",
    "    'fea': \"fearful\",\n",
    "    'sur': \"surprised\",\n",
    "    'dis': \"disgusted\",\n",
    "    'xxx': \"other\",\n",
    "    'oth': \"other\"\n",
    "}\n",
    "\n",
    "emotion_number = {\n",
    "    'ang': 0,\n",
    "    'hap': 1,\n",
    "    'sad': 2,\n",
    "    'neu': 3,\n",
    "    'fru': 4,\n",
    "    'exc': 5,\n",
    "    'fea': 6,\n",
    "    'sur': 7,\n",
    "    'dis': 8,\n",
    "    'oth': 9,\n",
    "    'xxx': 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file, duration, emotion, valence, activation, dominance, writer):\n",
    "    file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "    acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "    gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file, res_type='kaiser_fast')\n",
    "\n",
    "    std_chroma_stft = np.std(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "    mean_zcr = np.mean(zcr)\n",
    "    min_zcr = np.min(zcr)\n",
    "    var_mel_spect = np.var(librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.n_mels))\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spikes_spec_cent = spikes(spec_cent)\n",
    "    min_spec_cent = np.min(spec_cent)\n",
    "    var_spec_bw = np.var(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    mean_spec_cont = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    var_mfcc1 = np.var(mfcc[0])\n",
    "    max_mfcc1 = np.max(mfcc[0])\n",
    "    var_mfcc2 = np.var(mfcc[1])\n",
    "    var_mfcc3 = np.var(mfcc[2])\n",
    "    var_mfcc4 = np.var(mfcc[3])\n",
    "    max_mfcc5 = np.max(mfcc[4])\n",
    "    max_mfcc6 = np.max(mfcc[5])\n",
    "    min_mfcc7 = np.min(mfcc[6])\n",
    "    var_mfcc9 = np.var(mfcc[8])\n",
    "    max_mfcc9 = np.max(mfcc[8])\n",
    "    max_mfcc10 = np.max(mfcc[9])\n",
    "    max_mfcc13 = np.max(mfcc[12])\n",
    "    var_mfcc14 = np.var(mfcc[13])\n",
    "    var_mfcc15 = np.var(mfcc[14])\n",
    "    min_mfcc17 = np.min(mfcc[16])\n",
    "    min_mfcc19 = np.min(mfcc[18])\n",
    "\n",
    "    features_str = f'{file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {valence} {activation} {dominance}\\\n",
    "        {std_chroma_stft} {mean_zcr} {min_zcr} {var_mel_spect} {spikes_spec_cent} {min_spec_cent}\\\n",
    "        {var_spec_bw} {mean_spec_cont} {var_mfcc1} {max_mfcc1} {var_mfcc2} {var_mfcc3} {var_mfcc4}\\\n",
    "        {max_mfcc5} {max_mfcc6} {min_mfcc7} {var_mfcc9} {max_mfcc9} {max_mfcc10} {max_mfcc13}\\\n",
    "        {var_mfcc14} {var_mfcc15} {min_mfcc17} {min_mfcc19}'\n",
    "\n",
    "    writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(audio_dir, headers, proc_feat_dataset):\n",
    "    # Create a CSV for storing all processed features and write the header\n",
    "    file = open(proc_feat_dataset, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    print(\"Processing audio files from all dialogs:\")\n",
    "    for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "        emoEvalFile = open(file_path)\n",
    "\n",
    "        for line in emoEvalFile:\n",
    "            if line[0] == '[':\n",
    "                args = line.split()\n",
    "                duration = float(args[2][:-1]) - float(args[0][1:])\n",
    "                # on windows\n",
    "                audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "                # on linux/macOS\n",
    "                # audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "                extract_features(audio_file, duration, args[4], args[5][1:-1], args[6][:-1], args[7][:-1], writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [09:47<00:00,  3.89s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, headers, EXTRACTED_FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_data(audio_dir, headers, raw_audio_dataset):\n",
    "   file = open(raw_audio_dataset, 'w', newline='')\n",
    "   writer = csv.writer(file)\n",
    "   writer.writerow(headers)\n",
    "\n",
    "   print(\"Processing audio files from all dialogs:\")\n",
    "   for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "      emoEvalFile = open(file_path)\n",
    "\n",
    "      for line in emoEvalFile:\n",
    "         if line[0] == '[':\n",
    "            args = line.split()\n",
    "            duration = float(args[2][:-1]) - float(args[0][1:])\n",
    "            # on windows\n",
    "            #  audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "            #      \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "            # on linux/macOS\n",
    "            audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "            emotion, valence, activation, dominance = args[4], args[5][1:-1], args[6][:-1], args[7][:-1]\n",
    "            file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "            acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "            gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "            \n",
    "            features_str = f'{audio_file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "                {valence} {activation} {dominance}'\n",
    "\n",
    "            writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 151/151 [00:00<00:00, 488.21it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_raw_data(AUDIO_DIR, headers_raw, RAW_AUDIO_FILES)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

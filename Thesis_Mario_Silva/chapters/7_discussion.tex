\chapter{Discussion and Conclusions}
\label{chapter:conc}

In this final chapter, we present an analysis of the results obtained throughout this dissertation research, we address the contributions made to the field of \ac{ser} and highlight their novelty and significance. Furthermore, we provide a summary of the main findings and discuss their implications. Finally, we suggest possible directions for future research to complement and advance our work.

\section{Discussion}

Throughout the previous chapters, we have presented a detailed discussion of the results obtained from our experiments. In this section, we will provide a more concise analysis and examine the broader implications of our research.

One of the main challenges in this field is the lack of consistency in the labeling process of the available datasets. We have addressed this issue by using the \ac{iemo} dataset, which is widely used and validated in the field, and it allowed for a more accurate comparison of the performance of different models.

We evaluated two approaches to emotion recognition, traditional and \ac{dl}-based \ac{ser}, and focused on the effectiveness of feature extraction methods and the performance of the developed models. An audio preprocessing set of operations was developed, which showed efficiency in reducing noise and silence frames at the beginning and end of the audio.

Our study arrived at the selection of an \ac{xgb} classifier based on a small set of hand-crafted audio features and a fine-tuned ResNet50 model that utilizes audio spectrogram images. The \ac{xgb} classifier achieved the second-highest accuracy among the \ac{sota} articles that employed the traditional approach in the \ac{iemo} dataset. On the other hand, the ResNet50 model produced lower but comparable results to the \ac{dl} approaches, indicating that additional fine-tuning work could enhance its performance. The \ac{xgb} classifier offers faster prediction times, making it better suited for real-time classifications. However, the \ac{dl} model demonstrated superior cross-dataset validation results, indicating that it has greater generalization capability and is thus better suited for a broader range of contexts.

The importance of proper data stratification was also highlighted in this dissertation. By devising a set of conditions for the training and testing datasets, potential biases were addressed, and the subjective nature of emotions in the labeled data was accounted for. The analysis of the \ac{iemo} dataset allowed for the identification and mitigation of these issues, leading to better model performance within the \ac{iemo} dataset and cross-dataset validation.

The developed pipeline for emotion recognition is versatile and scalable, enabling the creation of voiced audio segments through the use of a \ac{vad*} tool. The pipeline can be used to apply both traditional and \ac{dl}-based \ac{ser} models for classification purposes. The pipeline was thoroughly tested using the entire raw \ac{iemo} dataset, and the results indicated that the pipeline successfully detected audio segments in similar amounts and predicted similar labels to the manually labeled segments. This confirms the robustness of the pipeline and its capability to detect emotions from speech signals. Researchers can use this pipeline for performing \ac{ser}, evaluating their own developed models on various datasets, or even in real-world scenarios.

\section{Contributions and Developed Tools}

This study has made several contributions toward the improvement of accuracy and efficiency in emotion recognition systems.

Firstly, this study contributes to the \ac{sota} research in \ac{ser} by providing a comprehensive and up-to-date review of recent advances in the field. Our review includes both traditional approaches, such as the use of hand-crafted features and machine learning techniques, as well as more recent \ac{dl}-based approaches. We also cover recent developments in multimodal \ac{ser}, which combine audio with other modalities such as visual or physiological signals, and discuss the challenges and opportunities of these approaches. Our review also includes a detailed analysis of recent benchmark datasets, which highlights the need for larger and more diverse datasets, as well as the importance of accounting for the variability of emotions across cultures and contexts.

Furthermore, by exploring the traditional approach for \ac{ser}, we were able to identify a small set of audio features that can be used by any model. Additionally, from our \ac{dl} approach, we were able to demonstrate the effectiveness of transfer learning techniques.

The data stratification study of the \ac{iemo} dataset, allowed us to create a set of conditions for obtaining better quality data that is suitable for more diverse environments. This study not only helped us in our research but can also be used as a guideline for future research.

Moreover, we have developed two Python classes that automate the process of feature extraction and classification of emotions \cite{Mario_Silva_Speech_Emotion_Recognition_2023}. The classes allow users to choose between the \ac{xgb} and ResNet50 models trained on either the entire \ac{iemo} dataset or the limited data resulting from the data stratification study. The classes can return the predicted emotion or probabilities of each emotion.

Lastly, we have developed a \ac{ser} pipeline, that can analyze an audio stream in real-time or offline, using the Silero \ac{vad} model to detect voiced speech segments, and a python class to label the segments using the different models that we trained. This pipeline can be easily integrated into streaming services, such as video conference systems, and can enhance their experience by providing real-time emotional feedback to participants.

\section{Conclusion}

This dissertation aimed to explore and develop \ac{ser} models using various approaches and techniques. To achieve this goal, state-of-the-art research on emotion recognition was reviewed, and different methods were analyzed for speech emotion recognition. A comprehensive methodology was proposed for developing reliable \ac{ser} models, which included requirements gathering, dataset selection, audio feature engineering, and model implementation.

The requirements-gathering phase involved identifying the research objectives, selecting the appropriate dataset, and defining the evaluation criteria for the models. The dataset was selected after analyzing different publicly available datasets and stratifying the data according to various factors such as gender, clip durations, and annotated labels, ensuring that the models were more reliable and accurate.

Through our experiments, we applied an audio preprocessing strategy that reduces background noises and trims silent frames at the beginning and end of audio signals.

Various acoustic features such as prosodic, spectral, and voice-quality features were extracted from the speech signals during the audio feature engineering phase. Feature selection techniques such as correlation-based feature and backwards selection were used to identify the most relevant features for the models.

Different models were implemented and evaluated, including traditional machine learning models, and \ac{dl} models. Transfer learning and ensemble methods were investigated to improve the performance of the models further. The models were evaluated using various performance metrics such as accuracy, precision, recall, F1 score, and \ac{mcc}.

In conclusion, this study has contributed to the field of \ac{ser} by providing an extensive overview of the field, by exploring the traditional and \ac{dl} approaches. The data stratification study of the \ac{iemo} dataset has allowed us to identify better quality data suitable for diverse environments, the pipeline we developed is able to detect and construct audio segments for emotion classification classes, and our developed tools automate the process of a \ac{ser} pipeline, from processing the speech, in online or offline time, to the classification of emotions.


\section{Future Work}

In order to advance our work on \ac{ser}, several areas of future research could be explored. One promising direction is the incorporation of additional features, such as physiological signals or facial expressions, to create more robust and accurate models. The development of multimodal models that can better handle noisy and non-stationary acoustic environments is also a potential area for improvement. This could involve weighing each input source differently depending on certain factors to improve classification accuracy. For example, the video brightness could be reduced if it is too low, to reduce its classification weight.

Another area for future work is the development of contextually aware models. These models could take into account situational factors, as well as the user's history and preferences, to create more personalized emotion recognition services.

While this study has made progress towards improving the accuracy and efficiency of \ac{ser} systems, there are still areas that require further investigation. One such area is the audio preprocessing techniques, which have been shown to play a critical role in the success of \ac{ser} models. Therefore, an in-depth study of the audio preprocessing techniques, and their effects on classification accuracy, should be conducted using the available datasets and the developed models. This would allow for a better understanding of the optimal audio preprocessing pipeline for \ac{ser} systems.

Another area that requires further investigation is the fine-tuning of the developed models, particularly the deep learning model. This requires more computationally expensive operations, such as increasing the number of trainable parameters or adjusting the hyperparameters of the model. However, fine-tuning can potentially lead to even better performance and accuracy.

Finally, integrating emotion recognition technology into a range of real-world applications, such as virtual assistants, smart homes, monitoring systems, and call centers, is a promising avenue for future research that can help validate the developed \ac{ser} tools. The design of emotionally intelligent and responsive interactive interfaces could also enhance the end-user experience. Continued research and development in these areas could lead to significant advances in \ac{ser} technology and its applications.
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# go to upper diretory\n",
    "sys.path.append(os.path.abspath('./../../../'))\n",
    "import csv\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from Audio_Sentiment_Analysis.utils.Configuration import Configuration\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "AUDIO_DIR = f\"{os.path.abspath('./../../../')}/IEMOCAP_Dataset\"\n",
    "EXTRACTED_FEATURES_FILE = 'extracted_features_iemocap.csv'\n",
    "DENOISED_EXTRACTED_FEATURES_FILE = 'denoised_extracted_features_iemocap.csv'\n",
    "ALL_EXTRACTED_FEATURES_FILE = 'all_extracted_features_iemocap.csv'\n",
    "DENOISED_ALL_EXTRACTED_FEATURES_FILE = 'denoised_all_extracted_features_iemocap.csv'\n",
    "RAW_AUDIO_FILES = 'raw_audio_files.csv'\n",
    "AUDIO_SPECTROGRAMS_DIR = './../data/spectrograms/'\n",
    "AUDIO_MFCCS_DIR = './../data/mfccs/'\n",
    "AUDIO_MEL_SPECTROGRAMS_DIR = './../data/mel_spectrograms/'\n",
    "AUDIO_MEL_SPECTROGRAM_IMAGES_DIR = './../data/mel_spectrogram_images/'\n",
    "AUDIO_MEL_SPECTROGRAM_IMAGES_DIR_2 = './../data/mel_spectrogram_images_2/'\n",
    "\n",
    "CONFIG_FILE = f\"{os.path.abspath('./../../../')}/Audio_Sentiment_Analysis/iemocap/config.json\"\n",
    "config = Configuration.load_json(CONFIG_FILE)\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the 24 features selected from the eNTERFACE05 dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = {\n",
    "    'var_mfcc15', 'min_mfcc17', 'std_chroma_stft', 'mean_zcr',\n",
    "    'min_zcr', 'var_mfcc4', 'spikes_spec_cent', 'var_spec_bw', 'max_mfcc6',\n",
    "    'min_mfcc19', 'max_mfcc13', 'max_mfcc5', 'var_mfcc9', 'min_mfcc7', 'var_mfcc1',\n",
    "    'max_mfcc1', 'max_mfcc9', 'var_mel_spect', 'mean_spec_cont', 'var_mfcc2', 'max_mfcc10',\n",
    "    'min_spec_cent', 'var_mfcc14', 'var_mfcc3'\n",
    "}\n",
    "all_headers = ['File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\\\n",
    "    'Valence', 'Activation', 'Dominance',\\\n",
    "    'spikes_mel_spect', 'mean_mel_spect', 'min_mel_spect', 'max_mel_spect', 'var_mel_spect', 'std_mel_spect', 'sum_mel_spect',\\\n",
    "    'spikes_chroma_stft', 'mean_chroma_stft', 'var_chroma_stft', 'std_chroma_stft', 'sum_chroma_stft',\\\n",
    "    'spikes_rms', 'mean_rms', 'max_rms', 'var_rms', 'std_rms', 'sum_rms',\\\n",
    "    'spikes_spec_cent', 'mean_spec_cent', 'min_spec_cent', 'max_spec_cent', 'var_spec_cent', 'std_spec_cent', 'sum_spec_cent',\\\n",
    "    'spikes_spec_bw', 'mean_spec_bw', 'min_spec_bw', 'max_spec_bw', 'var_spec_bw', 'std_spec_bw', 'sum_spec_bw',\\\n",
    "    'spikes_spec_rolloff', 'mean_spec_rolloff', 'min_spec_rolloff', 'max_spec_rolloff', 'var_spec_rolloff', 'std_spec_rolloff', 'sum_spec_rolloff',\\\n",
    "    'spikes_spec_cont', 'mean_spec_cont', 'min_spec_cont', 'max_spec_cont', 'var_spec_cont', 'std_spec_cont', 'sum_spec_cont',\\\n",
    "    'spikes_zcr', 'mean_zcr', 'min_zcr', 'max_zcr', 'var_zcr', 'std_zcr', 'sum_zcr']\n",
    "for i in range(1, 21):\n",
    "    all_headers.extend([f'spikes_mfcc{i}', f'mean_mfcc{i}', f'min_mfcc{i}', f'max_mfcc{i}', f'var_mfcc{i}', f'std_mfcc{i}', f'sum_mfcc{i}'])\n",
    "headers = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance',\n",
    "    'std_chroma_stft', 'mean_zcr', 'min_zcr',\n",
    "    'var_mel_spect', 'spikes_spec_cent', 'min_spec_cent',\n",
    "    'var_spec_bw', 'mean_spec_cont',\n",
    "    'var_mfcc1', 'max_mfcc1', 'var_mfcc2', 'var_mfcc3', 'var_mfcc4',\n",
    "    'max_mfcc5', 'max_mfcc6', 'min_mfcc7', 'var_mfcc9', 'max_mfcc9',\n",
    "    'max_mfcc10', 'max_mfcc13', 'var_mfcc14', 'var_mfcc15',\n",
    "    'min_mfcc17', 'min_mfcc19'\n",
    "]\n",
    "headers_raw = [\n",
    "    'File', 'Interaction', 'Gender', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'Valence', 'Activation', 'Dominance'\n",
    "]\n",
    "emotion_label = {\n",
    "    'ang': \"angry\", 'hap': \"happy\", 'sad': \"sad\", 'neu': \"neutral\",\n",
    "    'fru': \"frustrated\", 'exc': \"excited\", 'fea': \"fearful\",\n",
    "    'sur': \"surprised\", 'dis': \"disgusted\", 'xxx': \"other\", 'oth': \"other\"\n",
    "}\n",
    "emotion_number = {\n",
    "    'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4,\n",
    "    'exc': 5, 'fea': 6, 'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spikes(data):\n",
    "    if len(data.shape) != 1:\n",
    "        data = np.concatenate(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    threshold = mean + std * 2 / 100\n",
    "    num_spikes = 0\n",
    "    for value in data:\n",
    "        if value >= threshold:\n",
    "            num_spikes += 1\n",
    "\n",
    "    return num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file, duration, emotion, valence, activation, dominance, writer, noisereduce=False):\n",
    "    file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "    acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "    gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    if noisereduce:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    std_chroma_stft = np.std(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "    mean_zcr = np.mean(zcr)\n",
    "    min_zcr = np.min(zcr)\n",
    "    var_mel_spect = np.var(librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=config.n_mels))\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spikes_spec_cent = spikes(spec_cent)\n",
    "    min_spec_cent = np.min(spec_cent)\n",
    "    var_spec_bw = np.var(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
    "    mean_spec_cont = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    var_mfcc1 = np.var(mfcc[0])\n",
    "    max_mfcc1 = np.max(mfcc[0])\n",
    "    var_mfcc2 = np.var(mfcc[1])\n",
    "    var_mfcc3 = np.var(mfcc[2])\n",
    "    var_mfcc4 = np.var(mfcc[3])\n",
    "    max_mfcc5 = np.max(mfcc[4])\n",
    "    max_mfcc6 = np.max(mfcc[5])\n",
    "    min_mfcc7 = np.min(mfcc[6])\n",
    "    var_mfcc9 = np.var(mfcc[8])\n",
    "    max_mfcc9 = np.max(mfcc[8])\n",
    "    max_mfcc10 = np.max(mfcc[9])\n",
    "    max_mfcc13 = np.max(mfcc[12])\n",
    "    var_mfcc14 = np.var(mfcc[13])\n",
    "    var_mfcc15 = np.var(mfcc[14])\n",
    "    min_mfcc17 = np.min(mfcc[16])\n",
    "    min_mfcc19 = np.min(mfcc[18])\n",
    "\n",
    "    features_str = f'{file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {valence} {activation} {dominance}\\\n",
    "        {std_chroma_stft} {mean_zcr} {min_zcr} {var_mel_spect} {spikes_spec_cent} {min_spec_cent}\\\n",
    "        {var_spec_bw} {mean_spec_cont} {var_mfcc1} {max_mfcc1} {var_mfcc2} {var_mfcc3} {var_mfcc4}\\\n",
    "        {max_mfcc5} {max_mfcc6} {min_mfcc7} {var_mfcc9} {max_mfcc9} {max_mfcc10} {max_mfcc13}\\\n",
    "        {var_mfcc14} {var_mfcc15} {min_mfcc17} {min_mfcc19}'\n",
    "\n",
    "    writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(audio_file, duration, emotion, valence, activation, dominance, writer, noisereduce=False):\n",
    "    file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "    acting = \"improvised\" if file.split(\"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "    gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    if noisereduce:\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=config.n_mels)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_cont = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "\n",
    "    features_str = f'{file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {valence} {activation} {dominance}\\\n",
    "        {spikes(mel_spect)} {np.mean(mel_spect)} {np.min(mel_spect)} {np.max(mel_spect)} {np.var(mel_spect)} {np.std(mel_spect)} {np.sum(mel_spect)}\\\n",
    "        {spikes(chroma_stft)} {np.mean(chroma_stft)} {np.var(chroma_stft)} {np.std(chroma_stft)} {np.sum(chroma_stft)}\\\n",
    "        {spikes(rms)} {np.mean(rms)} {np.max(rms)} {np.var(rms)} {np.std(rms)} {np.sum(rms)}\\\n",
    "        {spikes(spec_cent)} {np.mean(spec_cent)} {np.min(spec_cent)} {np.max(spec_cent)} {np.var(spec_cent)} {np.std(spec_cent)} {np.sum(spec_cent)}\\\n",
    "        {spikes(spec_bw)} {np.mean(spec_bw)} {np.min(spec_bw)} {np.max(spec_bw)} {np.var(spec_bw)} {np.std(spec_bw)} {np.sum(spec_bw)}\\\n",
    "        {spikes(spec_rolloff)} {np.mean(spec_rolloff)} {np.min(spec_rolloff)} {np.max(spec_rolloff)} {np.var(spec_rolloff)} {np.std(spec_rolloff)} {np.sum(spec_rolloff)}\\\n",
    "        {spikes(spec_cont)} {np.mean(spec_cont)} {np.min(spec_cont)} {np.max(spec_cont)} {np.var(spec_cont)} {np.std(spec_cont)} {np.sum(spec_cont)}\\\n",
    "        {spikes(zcr)} {np.mean(zcr)} {np.min(zcr)} {np.max(zcr)} {np.var(zcr)} {np.std(zcr)} {np.sum(zcr)}'\n",
    "\n",
    "    for e in mfcc:\n",
    "        features_str += f' {spikes(e)} {np.mean(e)} {np.min(e)} {np.max(e)} {np.var(e)} {np.std(e)} {np.sum(e)}'\n",
    "\n",
    "    writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(audio_dir, headers, proc_feat_dataset=EXTRACTED_FEATURES_FILE, all_features=False, noisereduce=False):\n",
    "    # Create a CSV for storing all processed features and write the header\n",
    "    file = open(proc_feat_dataset, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    print(\"Processing audio files from all dialogs:\")\n",
    "    for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "        emoEvalFile = open(file_path)\n",
    "\n",
    "        for line in emoEvalFile:\n",
    "            if line[0] == '[':\n",
    "                args = line.split()\n",
    "                duration = float(args[2][:-1]) - float(args[0][1:])\n",
    "                # on windows\n",
    "                audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "                    \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "                # on linux/macOS\n",
    "                # audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "                if all_features:\n",
    "                    if args[4] in {'ang', 'neu', 'exc', 'hap', 'sad'}:\n",
    "                        extract_all_features(\n",
    "                            audio_file, duration, args[4], args[5][1:-1], args[6][:-1], args[7][:-1], writer, noisereduce)                    \n",
    "                else:\n",
    "                    extract_features(\n",
    "                        audio_file, duration, args[4], args[5][1:-1], args[6][:-1], args[7][:-1], writer, noisereduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [06:17<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [11:13<00:00,  4.46s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, headers, DENOISED_EXTRACTED_FEATURES_FILE, noisereduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [08:39<00:00,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, all_headers, ALL_EXTRACTED_FEATURES_FILE, all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(AUDIO_DIR, all_headers, DENOISED_ALL_EXTRACTED_FEATURES_FILE, all_features=True, noisereduce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_data(audio_dir, headers, raw_audio_dataset):\n",
    "   file = open(raw_audio_dataset, 'w', newline='')\n",
    "   writer = csv.writer(file)\n",
    "   writer.writerow(headers)\n",
    "\n",
    "   print(\"Processing audio files from all dialogs:\")\n",
    "   for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "      emoEvalFile = open(file_path)\n",
    "\n",
    "      for line in emoEvalFile:\n",
    "         if line[0] == '[':\n",
    "            args = line.split()\n",
    "            duration = float(args[2][:-1]) - float(args[0][1:])\n",
    "            # on windows\n",
    "            #  audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "            #      \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "            # on linux/macOS\n",
    "            audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "            emotion, valence, activation, dominance = args[4], args[5][1:-\n",
    "                                                                       1], args[6][:-1], args[7][:-1]\n",
    "            file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "            acting = \"improvised\" if file.split(\n",
    "                \"_\")[1][:5] == \"impro\" else \"scripted\"\n",
    "            gender = \"Male\" if file.split(\"_\")[-1][0] == 'M' else \"Female\"\n",
    "\n",
    "            features_str = f'{audio_file} {acting} {gender} {duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "                {valence} {activation} {dominance}'\n",
    "\n",
    "            writer.writerow(features_str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all dialogs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 151/151 [00:00<00:00, 488.21it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_raw_data(AUDIO_DIR, headers_raw, RAW_AUDIO_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_number = {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'exc': 1}\n",
    "\n",
    "def extract_spectograms(audio_dir):\n",
    "  for file_path in tqdm(glob.glob(audio_dir+'/Session[0-9]*/dialog/EmoEvaluation/*.txt')):\n",
    "    for line in open(file_path):\n",
    "      if line[0] == '[':\n",
    "        args = line.split()\n",
    "        emotion = args[4]\n",
    "        if emotion not in emotion_number.keys():\n",
    "          continue\n",
    "        # on windows\n",
    "        audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/'+file_path.split(\n",
    "            \"\\\\\")[-1][:-4]+f'/{args[3]}.wav'\n",
    "        # on linux/macOS\n",
    "        # audio_file = f'{AUDIO_DIR}/Session{args[3][4]}/sentences/wav/{file_path.split(\"/\")[-1][:-4]}/{args[3]}.wav'\n",
    "        file = audio_file.split(\".\")[-2].split(\"/\")[-1]\n",
    "        filename = file\n",
    "\n",
    "        # Obtain 6 seconds of the audio waveform\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = nr.reduce_noise(y=y, sr=sr)\n",
    "        # seconds = 6\n",
    "        # input_len = sr * seconds\n",
    "        # y = y[:input_len]\n",
    "        # if (input_len > tf.shape(y)[0]):\n",
    "        #   zero_padding = tf.zeros(\n",
    "        #       [input_len] - tf.shape(y),\n",
    "        #       dtype=tf.float32)\n",
    "        #   y = tf.cast(y, dtype=tf.float32)\n",
    "        #   y = tf.concat([y, zero_padding], 0)\n",
    "        #   y = y.numpy()\n",
    "\n",
    "        # ------------Spectrogram------------\n",
    "        # spec = np.abs(librosa.stft(y, n_fft=512, hop_length=256))\n",
    "        # with open(f'{AUDIO_SPECTROGRAMS_DIR}{filename}-{emotion_number[emotion]}.pickle', 'wb') as f:\n",
    "        #   pickle.dump(spec, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # # ---------------MFCCs---------------\n",
    "        # mfcc = librosa.feature.mfcc(y=y, sr=sr, S=spec, n_mfcc=256)\n",
    "        # with open(f'{AUDIO_MFCCS_DIR}{filename}-{emotion_number[emotion]}-mfcc.pickle', 'wb') as f:\n",
    "        #   pickle.dump(mfcc, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # # ----------Mel-Spectrogram----------\n",
    "        # mel_spec = librosa.feature.melspectrogram(\n",
    "        #     y=y, sr=sr, S=spec, n_mels=256)\n",
    "        # with open(f'{AUDIO_MEL_SPECTROGRAMS_DIR}{filename}-{emotion_number[emotion]}-mel.pickle', 'wb') as f:\n",
    "        #   pickle.dump(mel_spec, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # -------Mel-Spectrogram Image-------\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot()\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), cmap=\"viridis\")\n",
    "        plt.savefig(f'{AUDIO_MEL_SPECTROGRAM_IMAGES_DIR_2}{filename}-{emotion_number[emotion]}.png', dpi=400,\n",
    "                    bbox_inches='tight', pad_inches=0)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [23:18<00:00,  9.26s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_spectograms(AUDIO_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "133e7c8b355aba2fdf124ff52e5351672d24544aa772c704b7c0070650922ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import librosa.display\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../../EMO-DB_Dataset/\"\n",
    "PREPROCESSED_EXTRACTED_FEATURES_FILE = './traditional/preprocessed_extracted_features_emodb.csv'\n",
    "PREPROCESSED_SPECTROGRAM_IMAGES_DIR = './deep_learning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(y, sr):\n",
    "    y = nr.reduce_noise(y=y, sr=sr, n_fft=2048, hop_length=512, prop_decrease=.75, time_constant_s=1)\n",
    "    y, _ = librosa.effects.trim(y, top_db=30)\n",
    "    return y\n",
    "\n",
    "def spikes(data):\n",
    "    if len(data.shape) != 1:\n",
    "        data = np.concatenate(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    threshold = mean + std * 2 / 100\n",
    "    num_spikes = 0\n",
    "    for value in data:\n",
    "        if value >= threshold:\n",
    "            num_spikes += 1\n",
    "    \n",
    "    return num_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label = {\n",
    "    'W': 'anger', 'F': 'happy', 'N': 'neutral', 'T': 'sad'\n",
    "}\n",
    "\n",
    "emotion_number = {\n",
    "    'W': 0, 'F': 1, 'N': 3, 'T': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file, emotion, preprocess=True): \n",
    "    file = audio_file.split(\"\\\\\")[-1][:-4]\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "    bef_prep_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    aft_prep_duration = bef_prep_duration\n",
    "\n",
    "    if preprocess:\n",
    "        y = preprocess_audio(y, sr)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=127)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "\n",
    "    min_zcr = np.min(zcr)\n",
    "    spikes_zcr = spikes(zcr)\n",
    "    var_mel_spect = np.var(mel_spect)\n",
    "    spikes_mel_spect = spikes(mel_spect)\n",
    "    thpercentile25_chroma_stft = np.percentile(chroma_stft, 0.25)\n",
    "    spikes_chroma_stft = spikes(chroma_stft)\n",
    "    mean_spec_bw = np.mean(spec_bw)\n",
    "    max_spec_bw = np.max(spec_bw)\n",
    "    thpercentile25_rms = np.percentile(librosa.feature.rms(y=y), 0.25)\n",
    "\n",
    "    var_mfcc1 = np.var(mfcc[0])\n",
    "    var_mfcc3 = np.var(mfcc[2])\n",
    "    max_mfcc5 = np.max(mfcc[4])\n",
    "    var_mfcc5 = np.var(mfcc[4])\n",
    "    median_mfcc5 = np.median(mfcc[4])\n",
    "    spikes_mfcc6 = spikes(mfcc[5])\n",
    "    thpercentile75_mfcc7 = np.percentile(mfcc[6], 0.75)\n",
    "    max_mfcc7 = np.max(mfcc[6])\n",
    "    var_mfcc8 = np.var(mfcc[7])\n",
    "    sum_mfcc10 = np.sum(mfcc[9])\n",
    "    max_mfcc10 = np.max(mfcc[9])\n",
    "    thpercentile75_mfcc11 = np.percentile(mfcc[10], 0.75)\n",
    "    max_mfcc11 = np.max(mfcc[10])\n",
    "    sum_mfcc12 = np.sum(mfcc[11])\n",
    "    kurtosis_mfcc12 = kurtosis(mfcc[11])\n",
    "    mean_mfcc13 = np.mean(mfcc[12])\n",
    "    mean_mfcc15 = np.mean(mfcc[14])\n",
    "    spikes_mfcc16 = spikes(mfcc[15])\n",
    "    kurtosis_mfcc17 = kurtosis(mfcc[16])\n",
    "    mean_mfcc17 = np.mean(mfcc[16])\n",
    "    kurtosis_mfcc18 = kurtosis(mfcc[17])\n",
    "    spikes_mfcc19 = spikes(mfcc[18])\n",
    "    mean_mfcc19 = np.mean(mfcc[18])\n",
    "    mean_mfcc20 = np.mean(mfcc[19])\n",
    "\n",
    "    features_str = f'{file} {bef_prep_duration} {aft_prep_duration} {emotion_label[emotion]} {emotion_number[emotion]}\\\n",
    "        {min_zcr} {spikes_zcr} {var_mel_spect} {spikes_mel_spect} {thpercentile25_chroma_stft}\\\n",
    "        {spikes_chroma_stft} {mean_spec_bw} {max_spec_bw} {thpercentile25_rms} {var_mfcc1} {var_mfcc3} {max_mfcc5}\\\n",
    "        {var_mfcc5} {median_mfcc5} {spikes_mfcc6} {thpercentile75_mfcc7} {max_mfcc7} {var_mfcc8}\\\n",
    "        {sum_mfcc10} {max_mfcc10} {thpercentile75_mfcc11} {max_mfcc11} {sum_mfcc12} {kurtosis_mfcc12}\\\n",
    "        {mean_mfcc13} {mean_mfcc15} {spikes_mfcc16} {kurtosis_mfcc17} {mean_mfcc17} {kurtosis_mfcc18}\\\n",
    "        {spikes_mfcc19} {mean_mfcc19} {mean_mfcc20}'\n",
    "\n",
    "    return features_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dl_features(audio_file, emotion, preprocess=True): \n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "    if preprocess:\n",
    "        y = preprocess_audio(y, sr)\n",
    "\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot()\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    spec = librosa.amplitude_to_db(np.abs(librosa.stft(y,  n_fft=2048, hop_length=512)), ref=np.max)\n",
    "    librosa.display.specshow(spec, sr=sr, hop_length=512, ax=ax, cmap=\"viridis_r\")\n",
    "    filename = audio_file.split(\"\\\\\")[-1][:-4]\n",
    "    fig.savefig(f'{PREPROCESSED_SPECTROGRAM_IMAGES_DIR}{filename}-{emotion_number[emotion]}-spec_img.png',\n",
    "                bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "    fig.clf()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_headers = [\n",
    "    'File', 'Original_Duration', 'Duration', 'Emotion', 'Emotion_Id',\n",
    "    'min_zcr', 'spikes_zcr', 'var_mel_spect', 'spikes_mel_spect', 'thpercentile25_chroma_stft',\n",
    "    'spikes_chroma_stft', 'mean_spec_bw', 'max_spec_bw', 'thpercentile25_rms', 'var_mfcc1', 'var_mfcc3', 'max_mfcc5',\n",
    "    'var_mfcc5', 'median_mfcc5', 'spikes_mfcc6', 'thpercentile75_mfcc7', 'max_mfcc7', 'var_mfcc8',\n",
    "    'sum_mfcc10', 'max_mfcc10', 'thpercentile75_mfcc11', 'max_mfcc11', 'sum_mfcc12', 'kurtosis_mfcc12',\n",
    "    'mean_mfcc13', 'mean_mfcc15', 'spikes_mfcc16', 'kurtosis_mfcc17', 'mean_mfcc17', 'kurtosis_mfcc18',\n",
    "    'spikes_mfcc19', 'mean_mfcc19', 'mean_mfcc20'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(audio_dir, headers, proc_feat_dataset, preprocess=True):\n",
    "    # Create a CSV for storing all processed features and write the header\n",
    "    file = open(proc_feat_dataset, 'w', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    print(\"Processing audio files from all subjects:\")\n",
    "    for file_path in tqdm(glob.glob(audio_dir+'wav/*.wav')):\n",
    "        # for windows:\n",
    "        emotion = file_path[-6:-5]\n",
    "\n",
    "        if emotion not in emotion_number:\n",
    "            continue\n",
    "\n",
    "        processed_data = extract_features(file_path, emotion, preprocess=preprocess)\n",
    "        writer.writerow(processed_data)\n",
    "\n",
    "        extract_dl_features(file_path, emotion, preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files from all subjects:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [01:09<00:00,  7.65it/s]\n"
     ]
    }
   ],
   "source": [
    "process_data(AUDIO_DIR, final_headers, PREPROCESSED_EXTRACTED_FEATURES_FILE, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f961e7f63012a9c9ab542dea19c85589b3bede689791d2c90a627a6e0641da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

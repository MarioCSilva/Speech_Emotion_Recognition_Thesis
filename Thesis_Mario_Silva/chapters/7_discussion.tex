\chapter{Discussion and Conclusions}
\label{chapter:conc}

In this final chapter, we present an analysis of the results obtained throughout this dissertation research, addressing the contributions made to the field of \ac{ser} and highlighting their novelty and significance. We provide a summary of the main findings and discuss their implications. Finally, we suggest possible directions for future research to complement and advance our work.

\section{Discussion}

Throughout the previous chapters, we have presented a detailed discussion of the results obtained from our experiments. In this section, we will provide a more concise analysis and examine the broader implications of our research.

One of the main challenges in this field is the lack of consistency in the labeling process of available datasets. To address this issue, we utilized the widely used and validated \ac{iemo} dataset, allowing for a more accurate comparison of model performance.

We evaluated two approaches to emotion recognition: traditional and \ac{dl}-based \ac{ser}, focusing on the effectiveness of feature extraction methods and model performance. Our audio preprocessing operations successfully reduced noise and eliminated silent frames, contributing to improved model performance.

Our study led us to select an \ac{xgb} classifier based on hand-crafted audio features and a fine-tuned ResNet50 model using audio spectrogram images. The \ac{xgb} classifier achieved the second-highest accuracy among \ac{sota} articles employing the traditional approach in the \ac{iemo} dataset, with 60.69\% accuracy in 5-fold \ac{cv}. The ResNet50 model produced comparable results to \ac{dl} approaches, achieving 58.24\% accuracy. The \ac{xgb} classifier demonstrated faster prediction times, making it suitable for real-time classifications, while the \ac{dl} model showed superior cross-dataset validation results, indicating greater generalization capability.

The importance of proper data stratification was highlighted, addressing potential biases and accounting for the subjective nature of emotions in labeled data. Data stratification improved model performance within the \ac{iemo} dataset and cross-dataset validation.

The developed \ac{ser} pipeline is versatile and scalable, incorporating a \ac{vad*} tool for creating voiced audio segments and supporting both traditional and \ac{dl}-based \ac{ser} models for classification. The pipeline was tested on the entire raw \ac{iemo} dataset, successfully detecting audio segments and predicting emotion labels. This confirms the pipeline's robustness in detecting emotions from speech signals, enabling its use in various scenarios.

\section{Contributions and Developed Tools}

This study has made several contributions toward improving accuracy and efficiency in emotion recognition systems.

Firstly, we provide a comprehensive and up-to-date review of recent advances in \ac{ser}, covering traditional approaches, \ac{dl}-based methods, and multimodal emotion recognition. We emphasize the need for larger and more diverse datasets and address the challenges of variability in emotions across cultures and contexts.

Furthermore, we identify a small set of audio features applicable to any model, contributing to traditional \ac{ser} approaches. Additionally, we demonstrate the effectiveness of transfer learning techniques in \ac{dl}-based \ac{ser}.

Our data stratification study of the \ac{iemo} dataset establishes conditions for obtaining higher-quality data suitable for diverse environments, providing guidance for future research.

We have also developed two Python classes that automate feature extraction and emotion classification \cite{Mario_Silva_Speech_Emotion_Recognition_2023}. These classes allow users to choose between the \ac{xgb} and ResNet50 models trained on either the entire \ac{iemo} dataset or the stratified data, returning predicted emotions or probabilities.

Finally, we present an \ac{ser} pipeline capable of analyzing audio streams in real-time or offline, utilizing a \ac{vad*} model and Python classes for emotion labeling. This pipeline can be seamlessly integrated into streaming services, enhancing user experience with real-time emotional feedback.

\section{Conclusion}

This dissertation explores and develops \ac{ser} classifiers using various approaches and techniques, making significant contributions to the field.

The research begins with a thorough review of \ac{sota} studies, followed by an analysis of different \ac{ser} approaches. The methodology covers requirements gathering, dataset selection, audio feature engineering, and model implementation, ensuring reliable and accurate models.

By addressing key aspects such as dataset quality, feature engineering, and model selection, we contribute to the \ac{ser} field by providing valuable insights into the application of traditional and \ac{dl} techniques. Our findings emphasize the importance of these factors in achieving accurate and reliable results.

In conclusion, this dissertation advances our understanding of \ac{ser} and provides a framework for developing reliable models. The research findings and methodology contribute to the existing body of knowledge in emotion recognition.

\section{Future Work}

Several areas of future research can advance our work on \ac{ser}. Incorporating additional features, such as physiological signals or facial expressions, can create more robust and accurate models. Developing multimodal models capable of handling noisy and non-stationary acoustic environments is another potential area for improvement. Contextually aware models that consider situational factors and user history can provide more personalized emotion recognition services.

Further investigation into audio preprocessing techniques is necessary, as they play a critical role in the success of \ac{ser} models. Studying the effects of audio preprocessing techniques on classification accuracy using available datasets and developed models would provide insights into optimal preprocessing pipelines.

Fine-tuning the developed models, particularly the \ac{dl} model, can potentially lead to even better performance and accuracy, although it requires computationally expensive operations. Adjusting hyperparameters and increasing the number of trainable parameters can further improve performance.

Integrating emotion recognition technology into real-world applications, such as virtual assistants, smart homes, monitoring systems, and call centers, is promising for future research. Validating the developed \ac{ser} tools through integration into various applications and designing emotionally intelligent and responsive interfaces can enhance the end-user experience. Continued research and development in these areas can lead to significant advances in \ac{ser} technology and its applications.
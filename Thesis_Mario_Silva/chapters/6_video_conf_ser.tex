\chapter{Video Conference \acl{ser} Pipeline}
\label{chapter:ser_conf}

In this section, we present an audio pipeline for a \ac{ser} system on a video conference system. This pipeline includes several stages, each of which plays an essential role in identifying the emotional content from the audio.

\subsection{Preprocessing}

The audio pipeline will consume binary audio data of a participant corresponding of half a second. The first step of the pipeline is to convert the binary data to an array of floats, afterwards it normalizes the audio signal so that the algorithms that follow can process it. With the Librosa tookkit, it  resamples the audio to a sampling rate of 16000 Hz and converts the audio signal to mono by averaging samples across the channels if the signal is stereo. These operations are necessary so that the audio can be accepted and interpreted by the machine learning models that will analyze it.

\subsection{\acl{vad*}}

The second step of the pipeline is to detect the presence of voiced speech.

Say it was chosen the silero vad basicly.

https://thegradient.pub/one-voice-detector-to-rule-them-all/

https://github.com/snakers4/silero-vad

https://github.com/wiseman/py-webrtcvad

The detection of the presence of voiced speech among various unvoiced speech and silence is called endpoint detection, speech detection, or voice activity detection.

The performance of the detection algorithm could affect the accuracy of the system. The goal is to detect silent and noisy frames that are potentially irrelevant in terms of \ac{ser}, this will also decrease the complexity and increase the accuracy of the model. The most widely used methods for voice activity detection are zero-crossing rate, short-time energy, and auto-correlation method.

Zero crossing rate is the rate at which a signal changes its sign from positive to negative or vice versa within a given time frame.

The voiced speech has high energy due to its periodicity, while low energy is observed in the unvoiced speech.

The auto-correlation method provides a measure of similarity between a signal and itself as a function of delay. It is used to find repeating patterns. Because of its periodic nature, voiced signals can be detected using the auto-correlation method.


\subsection{Speech Segmentation}

Say the strategy to this segmentation, minimum more than a second of audio, as the data stratification models used. Then say the strategy after detecting voice in a segment.


Speech segmentation, also known as framing, is the process in which continuous speech signals are partitioned into segments.

As mentioned previously, emotions are usually short-lived, and the speech remains invariant for a brief period. By segmenting this data, it is possible to obtain local features of emotions, hence, frames with a short range of length are suitable for classifiers while maintaining the emotional information in a continuous speech.



\section{Results and Discussion}


Demonstrate the results for a file of the iemocap, the number of segments, and the emotions recognized... Maybe do for all files.


A period pause is the pause at punctuation marks between sentences, like a period, or a question mark. The “Others” category in Table 1 are pauses mainly made for breathing. The mean pause duration for commas ranged from 0.51 to 0.78 s, while the mean pause duration for periods ranged from 1.40 to 1.43 s.


The audio pipeline for SER on a video conference system is designed to process audio data in real-time. The pipeline consists of several steps that aim to extract emotions from audio streams. The first step involves processing the raw audio data by converting it to a specified format, such as np.float32. Next, the audio is normalized, where the sample rate is set to 16000 Hz and the number of channels is reduced to one.

The normalized audio is then passed through a Voice Activity Detection (VAD) model, which detects speech segments with a minimum confidence level of 0.6. The detected segments are then fed to an emotion classifier model to predict the emotion in the segment.

The pipeline is designed to process one second of audio data at a time. If a segment is less than one second, it is buffered until the next segment arrives, and if it is more than six seconds, it is split into multiple segments. The pipeline is capable of detecting emotions in real-time by continuously processing audio segments as they arrive.

By integrating the SER pipeline into a video conference system, we can provide real-time feedback to the participants, enhance their emotional experience, and ultimately improve the quality of remote communication.

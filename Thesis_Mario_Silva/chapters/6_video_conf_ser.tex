\chapter{Video Conference \acl{ser} Pipeline}
\label{chapter:ser_conf}

In this section, we present an audio pipeline for performing \ac{ser} on a video conference system. This pipeline can be used both in online and offline time, and includes several stages, each of which plays an essential role for properly identifying emotional content from the audio signal.

\subsection{Preprocessing}

The first step of the pipeline is to consume the binary audio data of a participant corresponding to one second. Then it converts this binary data to an array of floats, afterwards it normalizes the audio signal. The normalization consists of, when necessary, resampling the audio to a sampling rate of 16000 Hz and converting the signal to mono by averaging samples across the channels, recurring to the Librosa toolkit. These operations are necessary so that the audio can be accepted and interpreted by the machine learning models that will follow and analyze it.

\subsection{\acl{vad*}}

The second step of the pipeline is to detect the presence of voiced speech, in the previous second of audio consumed, using a \ac{vad*} model.

We chose the sileroVAD algorithm as it is fast and TODO....

This algorithm returns a confidence level associated with the detection of voiced speech, which allows us to fine-tune the minimum value of confidence to consider a voiced segment.

\subsection{Speech Segmentation}


Emotions are usually short-lived, and the speech remains invariant for a brief period. Speech segmentation is the process in which the continuous speech signals are partitioned into short-length segments while maintaining the emotional information suitable for being inputted to \ac{ser} models.

Each time the pipeline consumes a second of audio, it stores the segment if there is enough confidence that it detected voice activity, if it does not pass the threshold and it has previously saved any audio segment, it feeds it to a \ac{ser} model to return the recognized emotion.

The strategy to this segmentation is to create voiced segments that have at a minimum one second of duration and at most 6 seconds, as utilized previously in the previous data stratification study.




\section{Results and Discussion}


Demonstrate the results for a file of the iemocap, the number of segments, and the emotions recognized... Maybe do for all files.


A period pause is the pause at punctuation marks between sentences, like a period, or a question mark. The “Others” category in Table 1 are pauses mainly made for breathing. The mean pause duration for commas ranged from 0.51 to 0.78 s, while the mean pause duration for periods ranged from 1.40 to 1.43 s.


The pipeline is capable of detecting emotions in real-time by continuously processing audio segments as they arrive.


The audio pipeline for SER on a video conference system is designed to process audio data in real-time. The pipeline consists of several steps that aim to extract emotions from audio streams. The first step involves processing the raw audio data by converting it to a specified format, such as np.float32. Next, the audio is normalized, where the sample rate is set to 16000 Hz and the number of channels is reduced to one.

The normalized audio is then passed through a Voice Activity Detection (VAD) model, which detects speech segments with a minimum confidence level of 0.6. The detected segments are then fed to an emotion classifier model to predict the emotion in the segment.

The pipeline is designed to process one second of audio data at a time. If a segment is less than one second, it is buffered until the next segment arrives, and if it is more than six seconds, it is split into multiple segments. The pipeline is capable of detecting emotions in real-time by continuously processing audio segments as they arrive.

By integrating the SER pipeline into a video conference system, we can provide real-time feedback to the participants, enhance their emotional experience, and ultimately improve the quality of remote communication.

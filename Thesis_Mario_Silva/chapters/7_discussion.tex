\chapter{Discussion and Conclusions}
\label{chapter:conc}

\section{Contributions/Developed Tools}


The contributions of this study include the development of tools that can be used to improve the accuracy and efficiency of emotion recognition in speech.

The data stratification study of the \ac{iemo} dataset, which was used for the development of models, and also by testing the final trained models on other datasets, allowed us to create a set of conditions on the \ac{iemo} for obtaining better quality data that is suitable for more diverse environments.

Additionally, we have developed two Python classes that automate the process of feature extraction and classification of emotions \cite{Mario_Silva_Speech_Emotion_Recognition_2023}, allowing users to choose between the random forest and ResNet50 models trained on either the entire \ac{iemo} dataset or the limited data resulting from the data stratification study. The classes can return the predicted emotion or probabilities of each emotion. We have also developed a \ac{ser} pipeline that can analyze an audio stream in real-time or offline, using the Silero \ac{vad} model to detect voiced speech segments and the developed \ac{ser} classification python class to label the segments. This pipeline can be easily integrated into streaming services, such as video conference systems, and can enhance their experience by providing real-time emotional feedback to participants.

\section{Future Work}

Moving forward, there are several areas of future work that could advance speech emotion recognition technology. One direction is to explore the incorporation of additional features, such as physiological signals or facial expressions, to create more robust and accurate models. The development of multimodal models that can better handle noisy and non-stationary acoustic environments is also an area for potential improvement. This could involve weighing each input source differently depending on certain factors to improve classification accuracy, for example, when the video brightness is too low reduce its classification weight.

Another promising direction for future work is the development of contextually aware emotion recognition models. These models could take into account situational factors, as well as the user's history and preferences. This could enable the creation of more personalized emotion recognition services that are tailored to individual users

Finally, integrating emotion recognition technology into a range of real-world applications, such as virtual assistants, smart homes, monitoring systems, or call centers, could be a promising avenue for future research. The development of new interfaces and interaction paradigms that are designed to be more emotionally intelligent and responsive to user needs could also enhance the user experience. Overall, continued research and development in these areas could lead to significant advances in speech emotion recognition technology and its applications.

\section{Conclusion}

This dissertation aimed to explore and develop \ac{ser} models using various approaches and techniques. To achieve this goal, state-of-the-art research on emotion recognition was reviewed, and different methods were analyzed for speech emotion recognition. A comprehensive methodology was proposed for developing reliable \ac{ser} models, which included requirements gathering, dataset selection, audio feature engineering, and model implementation.

The requirements-gathering phase involved identifying the research objectives, selecting the appropriate dataset, and defining the evaluation criteria for the models. The dataset was selected after analyzing different publicly available datasets and stratifying the data according to various factors such as gender, clip durations, and annotated labels, ensuring that the models were more reliable and accurate.

Various acoustic features such as prosodic, spectral, and voice-quality features were extracted from the speech signals during the audio feature engineering phase. Feature selection techniques such as correlation-based feature and backwards selection were used to identify the most relevant features for the models.

Different models were implemented and evaluated, including traditional machine learning models, and deep learning models. The use of transfer learning and ensemble methods were also investigated to improve the performance of the models further. The models were evaluated using various performance metrics such as accuracy, precision, recall, F1 score, and \ac{mcc}.

In summary, this dissertation proposes a comprehensive methodology for developing reliable \ac{ser} models and demonstrates that the proposed methodology significantly improves the performance of the models. The findings of this study can be used to improve the accuracy and reliability of emotion recognition systems and contribute to the development of emotionally intelligent systems. The future of speech emotion recognition research and development is promising, with many exciting opportunities for further exploration and innovation in this area.

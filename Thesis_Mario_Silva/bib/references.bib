@article{Yan2021,
  doi = {10.1080/08839514.2021.2000688},
  url = {https://doi.org/10.1080/08839514.2021.2000688},
  year = {2021},
  month = nov,
  publisher = {Informa {UK} Limited},
  volume = {36},
  number = {1},
  author = {Xueming Yan and Haiwei Xue and Shengyi Jiang and Ziang Liu},
  title = {Multimodal Sentiment Analysis Using Multi-tensor Fusion Network with Cross-modal Modeling},
  journal = {Applied Artificial Intelligence}
}


@inproceedings{Muppidi2021,
  doi = {10.1109/icassp39728.2021.9414248},
  url = {https://doi.org/10.1109/icassp39728.2021.9414248},
  year = {2021},
  month = jun,
  publisher = {{IEEE}},
  author = {Aneesh Muppidi and Martin Radfar},
  title = {Speech Emotion Recognition Using Quaternion Convolutional Neural Networks},
  booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics,  Speech and Signal Processing ({ICASSP})}
}

@article{Jahangir2021,
  doi = {10.1007/s11042-020-09874-7},
  url = {https://doi.org/10.1007/s11042-020-09874-7},
  year = {2021},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {80},
  number = {16},
  pages = {23745--23812},
  author = {Rashid Jahangir and Ying Wah Teh and Faiqa Hanif and Ghulam Mujtaba},
  title = {Deep learning approaches for speech emotion recognition: state of the art and research challenges},
  journal = {Multimedia Tools and Applications}
}

@article{Hasnul2021,
  doi = {10.3390/s21155015},
  url = {https://doi.org/10.3390/s21155015},
  year = {2021},
  month = jul,
  publisher = {{MDPI} {AG}},
  volume = {21},
  number = {15},
  pages = {5015},
  author = {Muhammad Anas Hasnul and Nor Azlina Ab. Aziz and Salem Alelyani and Mohamed Mohana and Azlan Abd. Aziz},
  title = {Electrocardiogram-Based Emotion Recognition Systems and Their Applications in Healthcare{\textemdash}A Review},
  journal = {Sensors}
}

@inproceedings{2041ade4b5294db59df9f67e9c854632,
    title = "Real vs. acted emotional speech",
    author = "J. Wilting and E.J. Krahmer and M.G.J. Swerts",
    year = "2006",
    language = "English",
    booktitle = "Proceedings of the International Conference on Spoken Language Processing (Interspeech 2006)",
    publisher = "ISCA",
}

@inproceedings{Scherer2000ACI,
  title={A cross-cultural investigation of emotion inferences from voice and speech: implications for speech technology},
  author={Klaus R. Scherer},
  booktitle={Interspeech},
  year={2000}
}

@article{Milling2022,
  doi = {10.3389/fcomp.2022.837269},
  url = {https://doi.org/10.3389/fcomp.2022.837269},
  year = {2022},
  month = feb,
  publisher = {Frontiers Media {SA}},
  volume = {4},
  author = {Manuel Milling and Alice Baird and Katrin D. Bartl-Pokorny and Shuo Liu and Alyssa M. Alcorn and Jie Shen and Teresa Tavassoli and Eloise Ainger and Elizabeth Pellicano and Maja Pantic and Nicholas Cummins and Bj\"{o}rn W. Schuller},
  title = {Evaluating the Impact of Voice Activity Detection on Speech Emotion Recognition for Autistic Children},
  journal = {Frontiers in Computer Science}
}

@incollection{Schuller2011,
  doi = {10.1007/978-0-85729-994-9_9},
  url = {https://doi.org/10.1007/978-0-85729-994-9_9},
  year = {2011},
  publisher = {Springer London},
  pages = {227--253},
  author = {Bj\"{o}rn Schuller},
  title = {Voice and Speech Analysis in Search of States and Traits},
  booktitle = {Computer Analysis of Human Behavior}
}

@inproceedings{Rajoo2016,
  doi = {10.1109/iscaie.2016.7575033},
  url = {https://doi.org/10.1109/iscaie.2016.7575033},
  year = {2016},
  month = may,
  publisher = {{IEEE}},
  author = {Rajesvary Rajoo and Ching Chee Aun},
  title = {Influences of languages in speech emotion recognition: A comparative study using Malay,  English and Mandarin languages},
  booktitle = {2016 {IEEE} Symposium on Computer Applications {\&}amp$\mathsemicolon$ Industrial Electronics ({ISCAIE})}
}

@inproceedings{Luengo2005,
  doi = {10.21437/interspeech.2005-324},
  url = {https://doi.org/10.21437/interspeech.2005-324},
  year = {2005},
  month = sep,
  publisher = {{ISCA}},
  author = {Iker Luengo and Eva Navas and Inmaculada Hern{\'{a}}ez and Jon S{\'{a}}nchez},
  title = {Automatic emotion recognition using prosodic parameters},
  booktitle = {Interspeech 2005}
}

@inproceedings{Slaney,
  doi = {10.1109/icassp.1998.675432},
  url = {https://doi.org/10.1109/icassp.1998.675432},
  publisher = {{IEEE}},
  author = {M. Slaney and G. McRoberts},
  title = {Baby Ears: a recognition system for affective vocalizations},
  booktitle = {Proceedings of the 1998 {IEEE} International Conference on Acoustics,  Speech and Signal Processing,  {ICASSP} {\textquotesingle}98 (Cat. No.98CH36181)}
}

@inproceedings{Gosztolya2015,
  doi = {10.21437/interspeech.2015-332},
  url = {https://doi.org/10.21437/interspeech.2015-332},
  year = {2015},
  month = sep,
  publisher = {{ISCA}},
  author = {G{\'{a}}bor Gosztolya},
  title = {Conflict intensity estimation from speech using Greedy forward-backward feature selection},
  booktitle = {Interspeech 2015}
}

@article{Rao2012,
  doi = {10.1007/s10772-012-9172-2},
  url = {https://doi.org/10.1007/s10772-012-9172-2},
  year = {2012},
  month = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {16},
  number = {2},
  pages = {143--160},
  author = {K. Sreenivasa Rao and Shashidhar G. Koolagudi and Ramu Reddy Vempada},
  title = {Emotion recognition from speech using global and local prosodic features},
  journal = {International Journal of Speech Technology}
}

@INPROCEEDINGS{1202279,
  author={Schuller, B. and Rigoll, G. and Lang, M.},
  booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).}, 
  title={Hidden Markov model-based speech emotion recognition}, 
  year={2003},
  volume={2},
  number={},
  pages={II-1},
  doi={10.1109/ICASSP.2003.1202279}}

@article{Schuller3D2011,
  doi = {10.1109/t-affc.2011.17},
  url = {https://doi.org/10.1109/t-affc.2011.17},
  year = {2011},
  month = oct,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {2},
  number = {4},
  pages = {192--205},
  author = {Bjorn Schuller},
  title = {Recognizing Affect from Linguistic Information in 3D Continuous Space},
  journal = {{IEEE} Transactions on Affective Computing}
}

@article{ZhihongZeng2009,
  doi = {10.1109/tpami.2008.52},
  url = {https://doi.org/10.1109/tpami.2008.52},
  year = {2009},
  month = jan,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {31},
  number = {1},
  pages = {39--58},
  author = {Zhihong Zeng and M. Pantic and G.I. Roisman and T.S. Huang},
  title = {A Survey of Affect Recognition Methods: Audio,  Visual,  and Spontaneous Expressions},
  journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}

@article{ArputhaRathina2012,
  doi = {10.5121/ijcsea.2012.2410},
  url = {https://doi.org/10.5121/ijcsea.2012.2410},
  year = {2012},
  month = aug,
  publisher = {Academy and Industry Research Collaboration Center ({AIRCC})},
  volume = {2},
  number = {4},
  pages = {99--107},
  author = {X Arputha Rathina},
  title = {Basic Analysis on Prosodic Features in Emotional Speech},
  journal = {International Journal of Computer Science,  Engineering and Applications}
}

@inproceedings{Vogt,
  doi = {10.1109/icme.2005.1521463},
  url = {https://doi.org/10.1109/icme.2005.1521463},
  publisher = {{IEEE}},
  author = {T. Vogt and E. Andre},
  title = {Comparing Feature Sets for Acted and Spontaneous Speech in View of Automatic Emotion Recognition},
  booktitle = {2005 {IEEE} International Conference on Multimedia and Expo}
}

@inproceedings{teachexample,
    author = {Ai, Hua and Litman, Diane and Forbes-Riley, Katherine and Rotaru, Mihai and Tetreault, Joel and Purandare, Amruta},
    year = {2006},
    month = {01},
    pages = {},
    title = {Using system and user performance features to improve emotion detection in spoken tutoring dialogs}
}

@inproceedings{ccexample1,
    author = {Devillers, Laurence and Vidrascu, Laurence},
    year = {2006},
    month = {01},
    pages = {},
    title = {Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs.}
}

@article{ccexample2,
    author = {Burkhardt, Felix and van Ballegooy, Markus and Englert, Roman},
    year = {2005},
    month = {01},
    pages = {},
    title = {An emotion-aware voice portal}
}

@online{ibmwatson,
	title = {IBM Watson},
	url = {https://www.ibm.com/watson},
	urldate = {2023-01-03},
}

@online{bitext2022,
	author = {{Bitext. We help AI understand humans. – chatbots that work}},
	date = {2022-09-16},
	title = {Bitext. We help AI understand humans. - chatbots that work - Synthetic data},
	url = {https://www.bitext.com/},
 	urldate = {2023-01-03},
}

@online{microsoftservice,
	title = {Cognitive Services—APIs for AI Solutions},
	url = {https://azure.microsoft.com/en-us/products/cognitive-services/},
	urldate = {2023-01-03},
}

@online{kristensen2022,
	author = {given-i=S., given=Stig, family=Kristensen},
	date = {2022-12-26},
	title = {iMotions - Powering Human Insights},
	url = {https://imotions.com/},
	urldate = {2023-01-03},
}

@online{emovuservice,
	title = {EmoVu by Eyeris},
	url = {https://www.emovu.com/},
	urldate = {2023-01-03},
}

@online{humanbeaviourservice,
	title = {Human Behaviour AI Technology},
	url = {https://www.nviso.ai/en/technology},
	urldate = {2023-01-03},
}

@online{skybiometry2022,
	date = {2022-01-12},
	title = {Skybiometry | Cloud Based Biometrics API as a Service},
	url = {https://skybiometry.com/},
	urldate = {2023-01-03},
}

@online{audeering2022,
	date = {2022-07-20},
	title = {Technology},
	url = {https://www.audeering.com/technology/},
	urldate = {2023-01-03},
}

@online{goodvibrations,
	title = {Emotion Recognition by Voice by Powerful AI Voice Algorithms - Good Vibrations Company},
	url = {https://goodvibrations.nl/},
	urldate = {2023-01-03},
}

@online{Vokaturi,
	title = {Vokaturi - Eyes on Speech Communication},
	url = {https://vokaturi.com/},
	urldate = {2023-01-03},
}

@inproceedings{ma18b_interspeech,
  author={Xi Ma and Zhiyong Wu and Jia Jia and Mingxing Xu and Helen Meng and Lianhong Cai},
  title={{Emotion Recognition from Variable-Length Speech Segments Using Deep Learning on Spectrograms}},
  year=2018,
  booktitle={Proc. Interspeech 2018},
  pages={3683--3687},
  doi={10.21437/Interspeech.2018-2228}
}

@online{ranaTedTalk,
	annotation = {Timestamp: 10:36},
	author = {given-i=R.E., given={Rana El}, family=Kaliouby},
        publisher = {Ted Talks Conference},
	date = {2015-06-15},
	title = {This app knows how you feel -- from the look on your face},
	url = {https://www.ted.com/talks/rana_el_kaliouby_this_app_knows_how_you_feel_from_the_look_on_your_face},
	urldate = {2023-01-05},
}


@article{Zhao2019,
  doi = {10.1109/access.2019.2928625},
  url = {https://doi.org/10.1109/access.2019.2928625},
  year = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {7},
  pages = {97515--97525},
  author = {Ziping Zhao and Zhongtian Bao and Yiqin Zhao and Zixing Zhang and Nicholas Cummins and Zhao Ren and Bjorn Schuller},
  title = {Exploring Deep Spectrum Representations via Attention-Based Recurrent and Convolutional Neural Networks for Speech Emotion Recognition},
  journal = {{IEEE} Access}
}

@article{Krcadinac2016,
  doi = {10.1109/thms.2015.2504081},
  url = {https://doi.org/10.1109/thms.2015.2504081},
  year = {2016},
  month = jun,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {46},
  number = {3},
  pages = {370--379},
  author = {Uros Krcadinac and Jelena Jovanovic and Vladan Devedzic and Philippe Pasquier},
  title = {Textual Affect Communication and Evocation Using Abstract Generative Visuals},
  journal = {{IEEE} Transactions on Human-Machine Systems}
}

@inproceedings{ccexample3,
    author = {Burkhardt, Felix and Ajmera, Jitendra and Englert, Roman and Stegmann, Joachim and Burleson, Winslow},
    year = {2006},
    month = {01},
    pages = {},
    title = {Detecting anger in automated voice portal dialogs.}
}

@inproceedings{Kanda2005,
  doi = {10.1109/iros.2005.1545035},
  url = {https://doi.org/10.1109/iros.2005.1545035},
  year = {2005},
  publisher = {{IEEE}},
  author = {T. Kanda and K. Iwase and M. Shiomi and H. Ishiguro},
  title = {A tension-moderating mechanism for promoting speech-based human-robot interaction},
  booktitle = {2005 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems}
}

@article{Khalil2019,
  doi = {10.1109/access.2019.2936124},
  url = {https://doi.org/10.1109/access.2019.2936124},
  year = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {7},
  pages = {117327--117345},
  author = {Ruhul Amin Khalil and Edward Jones and Mohammad Inayatullah Babar and Tariqullah Jan and Mohammad Haseeb Zafar and Thamer Alhussain},
  title = {Speech Emotion Recognition Using Deep Learning Techniques: A Review},
  journal = {{IEEE} Access}
}


@article{BALAZS201695,
    title = {Opinion Mining and Information Fusion: A survey},
    journal = {Information Fusion},
    volume = {27},
    pages = {95-110},
    year = {2016},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2015.06.002},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253515000536},
    author = {Jorge A. Balazs and Juan D. Velásquez},
    keywords = {Information Fusion, Survey, Opinion Mining, Sentiment Analysis},
    abstract = {Interest in Opinion Mining has been growing steadily in the last years, mainly because of its great number of applications and the scientific challenge it poses. Accordingly, the resources and techniques to help tackle the problem are many, and most of the latest work fuses them at some stage of the process. However, this combination is usually executed without following any defined guidelines and overlooking the possibility of replicating and improving it, hence the need for a deeper understanding of the fusion process becomes apparent. Information Fusion is the field charged with researching efficient methods for transforming information from different sources into a single coherent representation, and therefore can be used to guide fusion processes in Opinion Mining. In this paper we present a survey on Information Fusion applied to Opinion Mining. We first define Opinion Mining and describe its most fundamental aspects, later explain Information Fusion and finally review several Opinion Mining studies that rely at some point on the fusion of information.}
}

@inproceedings{Lu2020,
  doi = {10.1109/icassp40776.2020.9052937},
  url = {https://doi.org/10.1109/icassp40776.2020.9052937},
  year = {2020},
  month = may,
  publisher = {{IEEE}},
  author = {Zhiyun Lu and Liangliang Cao and Yu Zhang and Chung-Cheng Chiu and James Fan},
  title = {Speech Sentiment Analysis via Pre-Trained Features from End-to-End {ASR} Models},
  booktitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics,  Speech and Signal Processing ({ICASSP})}
}

@article{BHASKAR2015635,
    title = {Hybrid Approach for Emotion Classification of Audio Conversation Based on Text and Speech Mining},
    journal = {Procedia Computer Science},
    volume = {46},
    pages = {635-643},
    year = {2015},
    note = {Proceedings of the International Conference on Information and Communication Technologies, ICICT 2014, 3-5 December 2014 at Bolgatty Palace {\&} Island Resort, Kochi, India},
    issn = {1877-0509},
    doi = {https://doi.org/10.1016/j.procs.2015.02.112},
    url = {https://www.sciencedirect.com/science/article/pii/S1877050915001763},
    author = {Jasmine Bhaskar and K. Sruthi and Prema Nedungadi},
    keywords = {Emotion classification, text mining, speech mining, hybrid approach},
    abstract = {One of the greatest challenges in speech technology is estimating the speaker's emotion. Most of the existing approaches concentrate either on audio or text features. In this work, we propose a novel approach for emotion classification of audio conversation based on both speech and text. The novelty in this approach is in the choice of features and the generation of a single feature vector for classification. Our main intention is to increase the accuracy of emotion classification of speech by considering both audio and text features. In this work we use standard methods such as Natural Language Processing, Support Vector Machines, WordNet Affect and SentiWordNet. The dataset for this work have been taken from Semval -2007 and eNTERFACE’05 EMOTION Database.}
}

@article{Buitelaar2018,
  doi = {10.1109/tmm.2018.2798287},
  url = {https://doi.org/10.1109/tmm.2018.2798287},
  year = {2018},
  month = sep,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {20},
  number = {9},
  pages = {2454--2465},
  author = {Paul Buitelaar and Ian D. Wood and Sapna Negi and Mihael Arcan and John P. McCrae and Andrejs Abele and Cecile Robin and Vladimir Andryushechkin and Housam Ziad and Hesam Sagha and Maximilian Schmitt and Bjorn W. Schuller and J. Fernando Sanchez-Rada and Carlos A. Iglesias and Carlos Navarro and Andreas Giefer and Nicolaus Heise and Vincenzo Masucci and Francesco A. Danza and Ciro Caterino and Pavel Smrz and Michal Hradis and Filip Povolny and Marek Klimes and Pavel Matejka and Giovanni Tummarello},
  title = {{MixedEmotions}: An Open-Source Toolbox for Multimodal Emotion Analysis},
  journal = {{IEEE} Transactions on Multimedia}
}

@article{Issa2020,
  doi = {10.1016/j.bspc.2020.101894},
  url = {https://doi.org/10.1016/j.bspc.2020.101894},
  year = {2020},
  month = may,
  publisher = {Elsevier {BV}},
  volume = {59},
  pages = {101894},
  author = {Dias Issa and M. Fatih Demirci and Adnan Yazici},
  title = {Speech emotion recognition with deep convolutional neural networks},
  journal = {Biomedical Signal Processing and Control}
}

@article{Akay2020,
  doi = {10.1016/j.specom.2019.12.001},
  url = {https://doi.org/10.1016/j.specom.2019.12.001},
  year = {2020},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {116},
  pages = {56--76},
  author = {Mehmet Berkehan Ak{\c{c}}ay and Kaya O{\u{g}}uz},
  title = {Speech emotion recognition: Emotional models,  databases,  features,  preprocessing methods,  supporting modalities,  and classifiers},
  journal = {Speech Communication}
}

@inproceedings{Pohjalainen2016,
  doi = {10.1145/2964284.2967306},
  url = {https://doi.org/10.1145/2964284.2967306},
  year = {2016},
  month = oct,
  publisher = {{ACM}},
  author = {Jouni Pohjalainen and Fabien Fabien Ringeval and Zixing Zhang and Bj\"{o}rn Schuller},
  title = {Spectral and Cepstral Audio Noise Reduction Techniques in Speech Emotion Recognition},
  booktitle = {Proceedings of the 24th {ACM} international conference on Multimedia}
}

@article{PORIA201798,
    title = {A review of affective computing: From unimodal analysis to multimodal fusion},
    journal = {Information Fusion},
    volume = {37},
    pages = {98-125},
    year = {2017},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2017.02.003},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253517300738},
    author = {Soujanya Poria and Erik Cambria and Rajiv Bajpai and Amir Hussain},
    keywords = {Affective computing, Sentiment analysis, Multimodal affect analysis, Multimodal fusion, Audio, visual and text information fusion},
    abstract = {Affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields, ranging from artificial intelligence, natural language processing, to cognitive and social sciences. With the proliferation of videos posted online (e.g., on YouTube, Facebook, Twitter) for product reviews, movie reviews, political views, and more, affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis. This is the primary motivation behind our first of its kind, comprehensive literature review of the diverse field of affective computing. Furthermore, existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks, which this review aims to address. Multimodality is defined by the presence of more than one modality or channel, e.g., visual, audio, text, gestures, and eye gage. In this paper, we focus mainly on the use of audio, visual and text information for multimodal affect analysis, since around 90\% of the relevant literature appears to cover these three modalities. Following an overview of different techniques for unimodal affect analysis, we outline existing methods for fusing information from different modalities. As part of this review, we carry out an extensive study of different categories of state-of-the-art fusion techniques, followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis. A comprehensive overview of these two complementary fields aims to form the building blocks for readers, to better understand this challenging and exciting research field.}
}

@ARTICLE{8085174,
  author={Zhang, Shiqing and Zhang, Shiliang and Huang, Tiejun and Gao, Wen},
  journal={IEEE Transactions on Multimedia}, 
  title={Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching}, 
  year={2018},
  volume={20},
  number={6},
  pages={1576-1590},
  doi={10.1109/TMM.2017.2766843}}

@article{GARCIAORDAS2021102946,
    title = {Sentiment analysis in non-fixed length audios using a Fully Convolutional Neural Network},
    journal = {Biomedical Signal Processing and Control},
    volume = {69},
    pages = {102946},
    year = {2021},
    issn = {1746-8094},
    doi = {https://doi.org/10.1016/j.bspc.2021.102946},
    url = {https://www.sciencedirect.com/science/article/pii/S1746809421005437},
    author = {María Teresa García-Ordás and Héctor Alaiz-Moretón and José Alberto Benítez-Andrades and Isaías García-Rodríguez and Oscar García-Olalla and Carmen Benavides},
    keywords = {Sentiment analysis, Fully convolutional network, Real time, MFCC, Mel spectrograms},
    abstract = {In this work, a sentiment analysis method that is capable of accepting audio of any length, without being fixed a priori, is proposed. Mel spectrogram and Mel Frequency Cepstral Coefficients are used as audio description methods and a Fully Convolutional Neural Network architecture is proposed as a classifier. The results have been validated using three well known datasets: EMODB, RAVDESS and TESS. The results obtained were promising, outperforming the state-of–the-art methods. Also, thanks to the fact that the proposed method admits audios of any size, it allows a sentiment analysis to be made in near real time, which is very interesting for a wide range of fields such as call centers, medical consultations or financial brokers.}
}

@article{Handa2021,
  doi = {10.4018/ijcini.20211001.oa34},
  url = {https://doi.org/10.4018/ijcini.20211001.oa34},
  year = {2021},
  month = oct,
  publisher = {{IGI} Global},
  volume = {15},
  number = {4},
  pages = {1--14},
  author = {Anand Handa and Rashi Agarwal and Narendra Kohli},
  title = {Audio-Visual Emotion Recognition System Using Multi-Modal Features},
  journal = {International Journal of Cognitive Informatics and Natural Intelligence}
}

@misc{Luo2018,
  doi = {10.29007/7mhj},
  url = {https://doi.org/10.29007/7mhj},
  year = {2018},
  month = dec,
  publisher = {{EasyChair}},
  author = {Ziqian Luo and Hua Xu and Feiyang Chen},
  title = {Audio Sentiment Analysis by Heterogeneous Signal Features Learned from Utterance-Based Parallel Neural Network}
}

@article{ravdess,
    doi = {10.1371/journal.pone.0196391},
    author = {Livingstone, Steven R. AND Russo, Frank A.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
    year = {2018},
    month = {05},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0196391},
    pages = {1-35},
    abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
    number = {5},
}

@ARTICLE{8421023,
  author={Chen, Mingyi and He, Xuanji and Yang, Jing and Zhang, Han},
  journal={IEEE Signal Processing Letters}, 
  title={3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition}, 
  year={2018},
  volume={25},
  number={10},
  pages={1440-1444},
  doi={10.1109/LSP.2018.2860246}}

@article{VAD_Emotions_article,
    author = {Mitruț, Oana and Moise, Gabriela and Petrescu, Livia and Moldoveanu, Alin and Leordeanu, Marius and Moldoveanu, Florica},
    year = {2019},
    month = {12},
    pages = {21},
    title = {Emotion Classification Based on Biophysical Signals and Machine Learning Techniques},
    volume = {12},
    journal = {Symmetry},
    doi = {10.3390/sym12010021}
}

@misc{rethinkPalanisamy,
  doi = {10.48550/ARXIV.2007.11154},
  url = {https://arxiv.org/abs/2007.11154},
  author = {Palanisamy,  Kamalesh and Singhania,  Dipika and Yao,  Angela},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Sound (cs.SD),  Audio and Speech Processing (eess.AS),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Electrical engineering,  electronic engineering,  information engineering},
  title = {Rethinking CNN Models for Audio Classification},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{moud,
    title = {Multimodal Opinion and Sentiment (MOUD) Dataset},
    author = {Kosti, M. and Pappas, T. and Potamianos, G.},
    year = {2013},
    url = {http://multicomp.cs.cmu.edu/resources/moud-dataset/},
}

@misc{cmu-mosi,
    title = {CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSI)},
    author = {{Zadeh, A. and Morency, L.-P. and Yannakakis, G. and Poria, S. and Cambria, E. and Howard, N. and Pappas, T. and Morency, L. P.}},
    year = {2017},
    url = {http://multicomp.cs.cmu.edu/resources/moud-dataset/},
}

@misc{cmu-mosei,
    title = {CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)},
    author = {{Zadeh, A. and Poria, S. and Cambria, E. and Howard, N. and Pappas, T. and Morency, L.-P.}},
    year = {2018},
    url = {http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/},
}

@inproceedings{zadeh2018multi,
  title={Multi-attention recurrent network for human communication comprehension},
  author={Zadeh, Amir and Liang, Paul Pu and Poria, Soujanya and Vij, Prateek and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@book{Deng2006,
  doi = {10.1007/978-3-031-02555-6},
  url = {https://doi.org/10.1007/978-3-031-02555-6},
  year = {2006},
  publisher = {Springer International Publishing},
  author = {Li Deng},
  title = {Dynamic Speech Models}
}

@misc{tess,
  doi = {10.5683/SP2/E8H2MF},
  url = {https://borealisdata.ca/citation?persistentId=doi:10.5683/SP2/E8H2MF},
  author = {Pichora-Fuller,  M. Kathleen and Dupuis,  Kate},
  title = {Toronto emotional speech set (TESS)},
  publisher = {Borealis},
  year = {2020}
}

@article{Busso2008,
  doi = {10.1007/s10579-008-9076-6},
  url = {https://doi.org/10.1007/s10579-008-9076-6},
  year = {2008},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {42},
  number = {4},
  pages = {335--359},
  author = {Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Abe Kazemzadeh and Emily Mower and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},
  title = {{IEMOCAP}: interactive emotional dyadic motion capture database},
  journal = {Language Resources and Evaluation}
}

@inproceedings{Martin2006,
  doi = {10.1109/icdew.2006.145},
  url = {https://doi.org/10.1109/icdew.2006.145},
  year = {2006},
  publisher = {{IEEE}},
  author = {O. Martin and I. Kotsia and B. Macq and I. Pitas},
  title = {The {eNTERFACE}{\&}amp$\mathsemicolon$amp$\mathsemicolon${\#}146$\mathsemicolon$05 Audio-Visual Emotion Database},
  booktitle = {22nd International Conference on Data Engineering Workshops ({ICDEW}{\textquotesingle}06)}
}

@inproceedings{Burkhardt2005,
  doi = {10.21437/interspeech.2005-446},
  url = {https://doi.org/10.21437/interspeech.2005-446},
  year = {2005},
  month = sep,
  publisher = {{ISCA}},
  author = {Felix Burkhardt and A. Paeschke and M. Rolfes and Walter F. Sendlmeier and Benjamin Weiss},
  title = {A database of German emotional speech},
  booktitle = {Interspeech 2005}
}

@article{Busso2017,
  doi = {10.1109/taffc.2016.2515617},
  url = {https://doi.org/10.1109/taffc.2016.2515617},
  year = {2017},
  month = jan,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {8},
  number = {1},
  pages = {67--80},
  author = {Carlos Busso and Srinivas Parthasarathy and Alec Burmania and Mohammed AbdelWahab and Najmeh Sadoughi and Emily Mower Provost},
  title = {{MSP}-{IMPROV}: An Acted Corpus of Dyadic Interactions to Study Emotion Perception},
  journal = {{IEEE} Transactions on Affective Computing}
}


@article{Cao2014,
  doi = {10.1109/taffc.2014.2336244},
  url = {https://doi.org/10.1109/taffc.2014.2336244},
  year = {2014},
  month = oct,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {5},
  number = {4},
  pages = {377--390},
  author = {Houwei Cao and David G. Cooper and Michael K. Keutmann and Ruben C. Gur and Ani Nenkova and Ragini Verma},
  title = {{CREMA}-D: Crowd-Sourced Emotional Multimodal Actors Dataset},
  journal = {{IEEE} Transactions on Affective Computing}
}

@article{Lotfian2019,
  doi = {10.1109/taffc.2017.2736999},
  url = {https://doi.org/10.1109/taffc.2017.2736999},
  year = {2019},
  month = oct,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {10},
  number = {4},
  pages = {471--483},
  author = {Reza Lotfian and Carlos Busso},
  title = {Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings},
  journal = {{IEEE} Transactions on Affective Computing}
}

@misc{meld,
  doi = {10.48550/ARXIV.1810.02508},
  url = {https://arxiv.org/abs/1810.02508},
  author = {Poria,  Soujanya and Hazarika,  Devamanyu and Majumder,  Navonil and Naik,  Gautam and Cambria,  Erik and Mihalcea,  Rada},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations},
  publisher = {arXiv},
  year = {2018},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@INPROCEEDINGS{1220939,
  author={Schuller, B. and Rigoll, G. and Lang, M.},
  booktitle={2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)}, 
  title={Hidden Markov model-based speech emotion recognition}, 
  year={2003},
  volume={1},
  number={},
  pages={I-401},
  abstract={In this contribution we introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared throughout the paper. Within the first method a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines and results achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English language is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86\% recognition rate. As a basis of comparison the similar judgment of human deciders classifying the same corpus at 79.8\% recognition rate was analyzed.},
  keywords={},
  doi={10.1109/ICME.2003.1220939},
  ISSN={},
  month={7},
}

@incollection{Shuman2015,
  doi = {10.1016/b978-0-08-097086-8.25007-1},
  url = {https://doi.org/10.1016/b978-0-08-097086-8.25007-1},
  year = {2015},
  publisher = {Elsevier},
  pages = {526--533},
  author = {Vera Shuman and Klaus R. Scherer},
  title = {Emotions,  Psychological Structure of},
  booktitle = {International Encyclopedia of the Social {\&} Behavioral Sciences}
}


@inproceedings{affectiva,
    author = {McDuff, Daniel and Mahmoud, Abdelrahman and Mavadati, Mohammad and Amr, May and Turcot, Jay and Kaliouby, Rana el},
    title = {AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit},
    year = {2016},
    isbn = {9781450340823},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2851581.2890247},
    doi = {10.1145/2851581.2890247},
    abstract = {We present a real-time facial expression recognition toolkit that can automatically code the expressions of multiple people simultaneously. The toolkit is available across major mobile and desktop platforms (Android, iOS, Windows). The system is trained on the world's largest dataset of facial expressions and has been optimized to operate on mobile devices and with very few false detections. The toolkit offers the potential for the design of novel interfaces that respond to users' emotional states based on their facial expressions. We present a demonstration application that provides real-time visualization of the expressions captured by the camera.},
    booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
    pages = {3723–3726},
    numpages = {4},
    keywords = {facial expressions, affective computing, emotion},
    location = {San Jose, California, USA},
    series = {CHI EA '16}
}

@misc{hagerty2021,
	author = {given-i=A., given=Alexa, family=Hagerty and given-i=A., given=Alexandra, family=Albert},
	month = {4},
	title = {{AI is increasingly being used to identify emotions – here’s what’s at stake}},
	year = {2021},
	url = {https://theconversation.com/ai-is-increasingly-being-used-to-identify-emotions-heres-whats-at-stake-158809},
}

@misc{Librosa,
  doi = {10.5281/ZENODO.6759664},
  url = {https://zenodo.org/record/6759664},
  author = {McFee,  Brian and Metsai,  Alexandros and McVicar,  Matt and Balke,  Stefan and Thomé,  Carl and Raffel,  Colin and Zalkow,  Frank and Malek,  Ayoub and {,  Dana} and {Kyungyun Lee} and Nieto,  Oriol and Ellis,  Dan and Mason,  Jack and Battenberg,  Eric and Seyfarth,  Scott and Yamamoto,  Ryuichi and {Viktorandreevichmorozov} and {Keunwoo Choi} and Moore,  Josh and Bittner,  Rachel and Hidaka,  Shunsuke and {Ziyao Wei} and {Nullmightybofo} and Weiss,  Adam and Hereñú,  Darío and {Fabian-Robert St\"{o}ter} and Nickel,  Lorenz and Friesch,  Pius and Vollrath,  Matt and {Taewoon Kim}},
  title = {librosa/librosa: 0.9.2},
  publisher = {Zenodo},
  year = {2022},
  copyright = {Open Access}
}

@book{van1995python,
  title={Python reference manual},
  author={Van Rossum, Guido and Drake Jr, Fred L},
  year={1995},
  publisher={Centrum voor Wiskunde en Informatica Amsterdam}
}

@inproceedings{Wagner13,
    author = {Wagner, Johannes and Lingenfelser, Florian and Baur, Tobias and Damian, Ionut and Kistler, Felix and Andr{\'e}, Elisabeth},
    title = {The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time},
    booktitle = {Proceedings of the 21st ACM international conference on Multimedia},
    series = {MM '13},
    year = {2013},
    isbn = {978-1-4503-2404-5},
    location = {Barcelona, Spain},
    pages = {831--834},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/2502081.2502223},
    doi = {10.1145/2502081.2502223},
    acmid = {2502223},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {multimodal fusion, open source framework, real-time pattern recognition, social signal processing},
}

@online{pypi,
  title={Python Package Index - PyPI},
  url={https://pypi.org/},
  urldate = {2021-03-28}, 
  publisher={Python Software Foundation}
}

@incollection{Hudlicka2017,
  doi = {10.1016/b978-0-12-801851-4.00016-1},
  url = {https://doi.org/10.1016/b978-0-12-801851-4.00016-1},
  year = {2017},
  publisher = {Elsevier},
  pages = {383--436},
  author = {Eva Hudlicka},
  title = {Computational Modeling of Cognition{\textendash}Emotion Interactions: Theoretical and Practical Relevance for Behavioral Healthcare},
  booktitle = {Emotions and Affect in Human Factors and Human-Computer Interaction}
}

@article{Wu2014,
  doi = {10.1017/atsip.2014.11},
  url = {https://doi.org/10.1017/atsip.2014.11},
  year = {2014},
  publisher = {Now Publishers},
  volume = {3},
  number = {1},
  author = {Chung-Hsien Wu and Jen-Chun Lin and Wen-Li Wei},
  title = {Survey on audiovisual emotion recognition: databases,  features,  and data fusion strategies},
  journal = {{APSIPA} Transactions on Signal and Information Processing}
}

@InProceedings{10.1007/11573548_51,
    author="Jin, Xuecheng
    and Wang, Zengfu",
    editor="Tao, Jianhua
    and Tan, Tieniu
    and Picard, Rosalind W.",
    title="An Emotion Space Model for Recognition of Emotions in Spoken Chinese",
    booktitle="Affective Computing and Intelligent Interaction",
    year="2005",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="397--402",
    abstract="This paper presents a conception of emotion space modeling using psychological research for reference. Based on this conception, this paper studies the distribution of the seven emotions in spoken Chinese, including joy, anger, surprise, fear, disgust, sadness and neutral, in the two dimensional space of valence and arousal, and analyses the relationship between the dimensional ratings and the prosodic characteristics in terms of F0 maximum, minimum, range and mean. The findings show that the conception of emotion modeling is helpful to describe and distinguish emotions.",
    isbn="978-3-540-32273-3"
}

@ARTICLE{2020NumPyArray,
  author  = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Fernández del Río, Jaime and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  title   = {Array programming with {NumPy}},
  journal = {Nature},
  year    = {2020},
  volume  = {585},
  pages   = {357–362},
  doi     = {10.1038/s41586-020-2649-2}
}

@inproceedings{mckinney2010data,
  title={Data structures for statistical computing in python},
  author={McKinney, Wes and others},
  booktitle={Proceedings of the 9th Python in Science Conference},
  volume={445},
  pages={51--56},
  year={2010},
  organization={Austin, TX}
}

@article{hunter2007matplotlib,
  title={Matplotlib: A 2D graphics environment},
  author={Hunter, John D},
  journal={Computing in science \& engineering},
  volume={9},
  number={3},
  pages={90--95},
  year={2007},
  publisher={IEEE}
}

@book{gulli2017deep,
  title={Deep learning with Keras},
  author={Gulli, Antonio and Pal, Sujit},
  year={2017},
  publisher={Packt Publishing Ltd}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@misc{SileroVAD,
  author = {Silero Team},
  title = {Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/snakers4/silero-vad}},
  commit = {91f0aaecef2a8419bab6bfcada1b6f5201823071},
  email = {hello@silero.ai}
}


@software{tim_sainburg_2019_3243139,
  author       = {Tim Sainburg},
  title        = {timsainb/noisereduce: v1.0},
  month        = jun,
  year         = 2019,
  publisher    = {Zenodo},
  version      = {db94fe2},
  doi          = {10.5281/zenodo.3243139},
  url          = {https://doi.org/10.5281/zenodo.3243139}
}

@software{WebRTCVad,
  author       = {John Wiseman},
  title        = {Python interface to the WebRTC Voice Activity Detector},
  year         = 2021,
  publisher    = {GitHub},
  journal      = {GitHub repository},
  url          = {https://github.com/wiseman/py-webrtcvad},
  commit = {e283ca41df3a84b0e87fb1f5cb9b21580a286b09}
}

@article{sainburg2020finding,
  title={Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires},
  author={Sainburg, Tim and Thielk, Marvin and Gentner, Timothy Q},
  journal={PLoS computational biology},
  volume={16},
  number={10},
  pages={e1008228},
  year={2020},
  publisher={Public Library of Science}
}

@software{ayoubmalek2020,
    author = {Ayoub Malek},
    title = {pydiogment/pydiogment: 0.1.0},
    month = apr,
    year = {2020},
    version = {0.1.2},
    url = {https://github.com/SuperKogito/spafe}
}

@misc{spafe,
  doi = {10.5281/ZENODO.6824667},
  url = {https://zenodo.org/record/6824667},
  author = {Malek,  Ayoub and Borzì,  Stefano and Nielsen,  Christian Heider},
  keywords = {audio analysis,  audio features extraction,  speech processing},
  language = {en},
  title = {SuperKogito/spafe: v0.2.0},
  publisher = {Zenodo},
  year = {2022},
  copyright = {BSD 3-Clause "New" or "Revised" License}
}

@inproceedings{Degottex2014,
  doi = {10.1109/icassp.2014.6853739},
  url = {https://doi.org/10.1109/icassp.2014.6853739},
  year = {2014},
  month = may,
  publisher = {{IEEE}},
  author = {Gilles Degottex and John Kane and Thomas Drugman and Tuomo Raitio and Stefan Scherer},
  title = {{COVAREP} {\&}amp$\mathsemicolon${\#}x2014$\mathsemicolon$ A collaborative voice analysis repository for speech technologies},
  booktitle = {2014 {IEEE} International Conference on Acoustics,  Speech and Signal Processing ({ICASSP})}
}

@inproceedings{Eyben2010,
  doi = {10.1145/1873951.1874246},
  url = {https://doi.org/10.1145/1873951.1874246},
  year = {2010},
  publisher = {{ACM} Press},
  author = {Florian Eyben and Martin W\"{o}llmer and Bj\"{o}rn Schuller},
  title = {Opensmile},
  booktitle = {Proceedings of the international conference on Multimedia - {MM} {\textquotesingle}10}
}

@software{michael_waskom_2017_883859,
  author       = {Michael Waskom and
                  Olga Botvinnik and
                  Drew O'Kane and
                  Paul Hobson and
                  Saulius Lukauskas and
                  David C Gemperline and
                  Tom Augspurger and
                  Yaroslav Halchenko and
                  John B. Cole and
                  Jordi Warmenhoven and
                  Julian de Ruiter and
                  Cameron Pye and
                  Stephan Hoyer and
                  Jake Vanderplas and
                  Santi Villalba and
                  Gero Kunter and
                  Eric Quintero and
                  Pete Bachant and
                  Marcel Martin and
                  Kyle Meyer and
                  Alistair Miles and
                  Yoav Ram and
                  Tal Yarkoni and
                  Mike Lee Williams and
                  Constantine Evans and
                  Clark Fitzgerald and
                  Brian and
                  Chris Fonnesbeck and
                  Antony Lee and
                  Adel Qalieh},
  title        = {mwaskom/seaborn: v0.8.1 (September 2017)},
  month        = sep,
  year         = 2017,
  publisher    = {Zenodo},
  version      = {v0.8.1},
  doi          = {10.5281/zenodo.883859},
  url          = {https://doi.org/10.5281/zenodo.883859}
}

@article{RUSSELL1977273,
    title = {Evidence for a three-factor theory of emotions},
    journal = {Journal of Research in Personality},
    volume = {11},
    number = {3},
    pages = {273-294},
    year = {1977},
    issn = {0092-6566},
    doi = {https://doi.org/10.1016/0092-6566(77)90037-X},
    url = {https://www.sciencedirect.com/science/article/pii/009265667790037X},
    author = {James A Russell and Albert Mehrabian},
    abstract = {Two studies provided evidence that three independent and bipolar dimensions, pleasure-displeasure, degree of arousal, and dominance-submissiveness, are both necessary and sufficient to adequately define emotional states. In one study with 200 subjects, 42 verbal-report emotion scales were explored in regression analyses as functions of the three dimensions plus a measure of acquiescence bias. Multiple correlation coefficients showed that almost all of the reliable variance in the 42 scales had been accounted for. The specific definitions provided by these equations were replicated in a second study that employed 300 subjects' ratings of 151 emotion-denoting terms on semantic differential-type scales.}
}

@article{Narayanan2013,
  doi = {10.1109/jproc.2012.2236291},
  url = {https://doi.org/10.1109/jproc.2012.2236291},
  year = {2013},
  month = may,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {101},
  number = {5},
  pages = {1203--1233},
  author = {Shrikanth Narayanan and Panayiotis G. Georgiou},
  title = {Behavioral Signal Processing: Deriving Human Behavioral Informatics From Speech and Language},
  journal = {Proceedings of the {IEEE}}
}

@incollection{Daily2017,
  doi = {10.1016/b978-0-12-801851-4.00009-4},
  url = {https://doi.org/10.1016/b978-0-12-801851-4.00009-4},
  year = {2017},
  publisher = {Elsevier},
  pages = {213--231},
  author = {Shaundra B. Daily and Melva T. James and David Cherry and John J. Porter and Shelby S. Darnell and Joseph Isaac and Tania Roy},
  title = {Affective Computing: Historical Foundations,  Current Applications,  and Future Trends},
  booktitle = {Emotions and Affect in Human Factors and Human-Computer Interaction}
}

@misc{ser_strategies,
  doi = {10.48550/ARXIV.1906.05681},
  url = {https://arxiv.org/abs/1906.05681},
  author = {Tripathi,  Suraj and Kumar,  Abhay and Ramesh,  Abhiram and Singh,  Chirag and Yenigalla,  Promod},
  keywords = {Audio and Speech Processing (eess.AS),  Computation and Language (cs.CL),  Machine Learning (cs.LG),  Sound (cs.SD),  Machine Learning (stat.ML),  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Deep Learning based Emotion Recognition System Using Speech Features and Transcriptions},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@ARTICLE{8759878,
  author={Huang, Jingshan and Chen, Binqiang and Yao, Bin and He, Wangpeng},
  journal={IEEE Access}, 
  title={ECG Arrhythmia Classification Using STFT-Based Spectrogram and Convolutional Neural Network}, 
  year={2019},
  volume={7},
  number={},
  pages={92871-92880},
  doi={10.1109/ACCESS.2019.2928017}
}

@article{Lee2011,
  doi = {10.1016/j.specom.2011.06.004},
  url = {https://doi.org/10.1016/j.specom.2011.06.004},
  year = {2011},
  month = nov,
  publisher = {Elsevier {BV}},
  volume = {53},
  number = {9-10},
  pages = {1162--1171},
  author = {Chi-Chun Lee and Emily Mower and Carlos Busso and Sungbok Lee and Shrikanth Narayanan},
  title = {Emotion recognition using a hierarchical binary decision tree approach},
  journal = {Speech Communication}
}


@inproceedings{Tarantino2019,
  doi = {10.21437/interspeech.2019-2822},
  url = {https://doi.org/10.21437/interspeech.2019-2822},
  year = {2019},
  month = sep,
  publisher = {{ISCA}},
  author = {Lorenzo Tarantino and Philip N. Garner and Alexandros Lazaridis},
  title = {Self-Attention for Speech Emotion Recognition},
  booktitle = {Interspeech 2019}
}

@article{Eyben2016,
  doi = {10.1109/taffc.2015.2457417},
  url = {https://doi.org/10.1109/taffc.2015.2457417},
  year = {2016},
  month = apr,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {7},
  number = {2},
  pages = {190--202},
  author = {Florian Eyben and Klaus R. Scherer and Bjorn W. Schuller and Johan Sundberg and Elisabeth Andre and Carlos Busso and Laurence Y. Devillers and Julien Epps and Petri Laukka and Shrikanth S. Narayanan and Khiet P. Truong},
  title = {The Geneva Minimalistic Acoustic Parameter Set ({GeMAPS}) for Voice Research and Affective Computing},
  journal = {{IEEE} Transactions on Affective Computing}
}

@misc{HandCraftedSahu,
  doi = {10.48550/ARXIV.1904.06022},
  url = {https://arxiv.org/abs/1904.06022},
  author = {Sahu,  Gaurav},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Multimodal Speech Emotion Recognition and Ambiguity Resolution},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Albornoz2011,
  doi = {10.1016/j.csl.2010.10.001},
  url = {https://doi.org/10.1016/j.csl.2010.10.001},
  year = {2011},
  month = jul,
  publisher = {Elsevier {BV}},
  volume = {25},
  number = {3},
  pages = {556--570},
  author = {Enrique M. Albornoz and Diego H. Milone and Hugo L. Rufiner},
  title = {Spoken emotion recognition using hierarchical classifiers},
  journal = {Computer Speech {\&}amp$\mathsemicolon$ Language}
}

@article{Kuchibhotla2014,
  doi = {10.1007/s10772-014-9239-3},
  url = {https://doi.org/10.1007/s10772-014-9239-3},
  year = {2014},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {17},
  number = {4},
  pages = {401--408},
  author = {Swarna Kuchibhotla and H. D. Vankayalapati and R. S. Vaddi and K. R. Anne},
  title = {A comparative analysis of classifiers in emotion recognition through acoustic features},
  journal = {International Journal of Speech Technology}
}

@article{Zhou2022,
  doi = {10.1186/s12911-022-01942-2},
  url = {https://doi.org/10.1186/s12911-022-01942-2},
  year = {2022},
  month = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {22},
  number = {1},
  author = {George Zhou and Yunchan Chen and Candace Chien},
  title = {On the analysis of data augmentation methods for spectral imaged based heart sound classification using convolutional neural networks},
  journal = {{BMC} Medical Informatics and Decision Making}
}
@inproceedings{Zhao2018,
  doi = {10.1109/bds/hpsc/ids18.2018.00039},
  url = {https://doi.org/10.1109/bds/hpsc/ids18.2018.00039},
  year = {2018},
  month = may,
  publisher = {{IEEE}},
  author = {Huijuan Zhao and Ning Ye and Ruchuan Wang},
  title = {A Survey on Automatic Emotion Recognition Using Audio Big Data and Deep Learning Architectures},
  booktitle = {2018 {IEEE} 4th International Conference on Big Data Security on Cloud ({BigDataSecurity}),  {IEEE} International Conference on High Performance and Smart Computing,  ({HPSC}) and {IEEE} International Conference on Intelligent Data and Security ({IDS})}
}